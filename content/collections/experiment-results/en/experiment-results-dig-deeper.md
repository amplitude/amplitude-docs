---
id: 48014e0e-3130-40cc-ba15-946d6a16b51b
blueprint: experiment-result
title: 'Dig deeper into experimentation data with Experiment Results'
source: 'https://help.amplitude.com/hc/en-us/articles/360062072631-Dig-deeper-into-experimentation-data-with-Experiment-Results'
this_article_will_help_you:
  - 'Extend the analytic power of A/B tests you create in Amplitude Experiment'
landing: true
exclude_from_sitemap: false
updated_by: 5817a4fa-a771-417a-aa94-a0b1e7f55eae
updated_at: 1720542850
landing_blurb: 'Extend the analytic power of A/B tests you create in Amplitude Experiment'
---
With **Experiment Results**, Amplitude Analytics customers who have invested in a non-Amplitude feature flagging platform, whether third party or homegrown, can now take advantage of Amplitude’s planning, tracking, and analysis tools for Experiment—while still using the A/B tracking data generated by their own feature flagging platform.

## Availability

The Experiment Results chart is available to growth and enterprise plans. For more information, see Amplitude's [pricing page](https://amplitude.com/pricing).

## Before you begin

Prior to using Experiment Results, you’ll need to ensure you’ve **instrumented the metric events** that are relevant to your experiment. Without them, you’ll be unable to create the success metrics and goals that Experiment Results needs to compare each variant in its analysis.

Make sure you’ve instrumented the necessary **exposure events**, which represent the delivery of a variant to a user participating in the experiment.

There is also an [Experiment Results FAQ article](/docs/faq/experiment-analysis) that could provide guidance as you use this chart for the first time. 

## Analyze an A/B test using Experiment Results

To create an A/B test and see the results, follow these steps:

1. Navigate to *Create* > *Chart* > *Experiment Results*.
2. In the Metrics module, click *+ Add Metric* or *+* *Define single-use metric* to define your primary metric.
3. If adding a single-use metric, use the drop-down menu to specify the **metric type** in the *Define Metric* fly out panel:

      * Unique conversions
      * Event totals
      * Sum of property value
      * Average of property value
      * Funnel conversion
      * Formula
      * Retention

	The first four are available for individual event metric analyses, while funnel conversion allows you to define a multi-step journey that users must complete for the conversion to count. The Formula metric allows you to [define a formula](/docs/analytics/charts/experiment-results/experiment-results-use-formula-metrics) centered around a selected event or events. 

	The last option, Retention, allows you to measure the percentage of users who return to perform the selected event on a specific day (Return on nth day) after being exposed to the experiment. By default, the Retention metric doesn't support [CUPED](/docs/feature-experiment/workflow/finalize-statistical-preferences), exposure attribution settings, nor calendar day windows. Instead, the metric calculates exposure attribution settings using any exposure and the nth day value based on 24-hour window increments, for up to two months.

	{{partial:admonition type='note'}}
	Any of the above metrics can be used as a [custom metric during the design phase in Amplitude Experiment](/docs/feature-experiment/workflow/define-goals). 
	{{/partial:admonition}}

4. Next, specify the event to use for this metric. You can also filter the event using a *+ where* clause. When you’re finished, click *Done*.   

	![where_filter.png](/docs/output/img/experiment-results/where-filter-png.png)

	Optionally, click *+* *Add Metric* or *+* *Define single-use metric* in the Secondary Metrics module to add a second, subordinate metric to the analysis. You can add multiple secondary metrics as necessary.

5. Click *+ Add* *Event* in the Exposure module to define your experiment’s exposure event. The exposure event is the event users must trigger to become part of the experiment.
6. In the Variants performed by module, add your variants. All experiments require at least one variant, known as the **control**. Add a variant by clicking *+ Add Experiment Variant*.  
  
  ![add_variant.png](/docs/output/img/experiment-results/add-variant-png.png)
  
  Choose the properties and values that defines your variant and click *Apply*.

7. Click *+ Add Experiment Variant* to add more variants as necessary to reflect the experiment setup in your feature flagging system.

Amplitude calculates your statistical results on the fly and display them in the results. The results also allow you to modify your experiment's [statistical settings](/docs/feature-experiment/workflow/finalize-statistical-preferences), such as from the default Sequential test to a T-test. 

## Interpret your results

While the specifics may vary depending on the metric types you’re using, you’ll see four charts depicting your results:

* **Confidence interval of absolute performance over time**: This chart is for [sequential testing](https://help.amplitude.com/hc/en-us/articles/17767898439835) only. It can help you identify when the experiment reaches statistical significance; which occurs when the confidence interval no longer includes zero.
* [**Cumulative exposure**](/docs/feature-experiment/advanced-techniques/cumulative-exposure-change-slope): This chart details the number of users who are exposed to your experiment over time. The x-axis displays the first date of a user's exposure, and the y-axis displays a cumulative, running total of users exposed to the experiment.
* **Performance by variant**: The title of this chart is the metric you're focused on. The chart shows the number of users who did each step of a funnel, or the means of each variant if the metric isn't a funnel.
* **Mean over time** (cumulative or non-cumulative): On the x-axis, find the date the user was first exposed. On the y-axis, see the mean of the selected metric. Click the dropdown under the metric table to select a metric. Amplitude selects the recommendation metric by default for each variant. This chart is like the conversion over time chart except that it also works for non-conversion metrics. From this chart, you can see if there is any seasonality, novelty effects, or trends over time. In most cases, the mean for days near the start of the experiment is larger than the mean for days near the end of the experiment because users at the start of the experiment have had more time to do the metric. This is less of a concern if you use the exposure attribution window. You can look at the cumulative or the non-cumulative view of this chart. The cumulative view can help smooth out some daily noise and make it easier to interpret the chart.

These charts are also helpful when [learning from your end-to-end experiment](/docs/feature-experiment/overview). 

{{partial:admonition type='note'}}
By default, Amplitude selects the primary metric in experiment results. You can choose a different metric in the *Analysis* module. Click the dropdown in the metric table to see its results. 
{{/partial:admonition}}

## Group By

There are more resources on group by in experiment [here](/docs/analytics/charts/group-by) and [here](/docs/feature-experiment/workflow/experiment-learnings).