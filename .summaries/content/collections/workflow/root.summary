Amplitude Experiment is a comprehensive experimentation and feature flag management platform that enables organizations to run A/B tests, multi-armed bandit experiments, and deploy feature flags with sophisticated targeting and statistical analysis capabilities. The platform supports both traditional hypothesis-driven A/B testing and automated optimization through machine learning algorithms.

## Product Architecture and Feature Relationships

The platform operates through a structured workflow that encompasses experiment creation, configuration, audience targeting, statistical analysis, and deployment management. The core architecture consists of:

**Deployment Infrastructure**: Experiments are organized within deployments that can be configured as either client-side or server-side implementations. Each deployment receives unique access keys and integrates with Experiment SDKs or REST APIs for flag evaluation. The platform supports both local evaluation (for performance) and remote evaluation (for real-time updates) modes.

**Experiment Types**: The platform supports multiple experiment methodologies including traditional A/B/n tests for hypothesis validation and multi-armed bandit experiments that use Thompson sampling for automated traffic reallocation to optimize primary metrics without requiring statistical significance thresholds.

**Variant Management**: Experiments utilize variants with configurable names, values, payloads (JSON), and traffic distribution percentages. The system supports stratified sampling with rollout controls per segment and uniform allocation ratios for statistical validity.

**Targeting and Segmentation**: Sophisticated audience targeting operates through Rule Based User Segments that can be defined using cohorts, user properties, or custom properties. The system handles bucketing eligibility, exposure event requirements, and implements caching behavior for different property types.

## Key Nomenclature and Definitions

**Bucketing Units**: The fundamental unit for experiment assignment, typically User or Group/Company ID, ensuring consistent variant assignment and maintaining the Stable Unit Treatment Value Assumption (SUTVA).

**Evaluation Modes**: 
- **Remote Evaluation**: Real-time variant assignment with server-side processing
- **Local Evaluation**: Client-side processing for improved performance with cached configurations

**Statistical Concepts**:
- **Minimum Detectable Effect (MDE)**: The smallest effect size the experiment can reliably detect
- **CUPED**: Controlled-experiment using pre-existing data for variance reduction
- **Sequential Testing**: Continuous monitoring approach vs. traditional T-test methodology
- **Bonferroni Correction**: Multiple hypothesis testing adjustment to control false positive rates

**Event Types**:
- **Assignment Events**: When users are bucketed into variants
- **Exposure Events**: When users actually experience the variant (required for analysis)
- **Conversion Events**: Goal completion events for success measurement

**Metrics Framework**:
- **Recommendation Metrics**: Primary success indicators
- **Guardrail Metrics**: Safety metrics to prevent negative impacts
- **Secondary Metrics**: Additional insights and learning metrics

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem, leveraging user cohorts, behavioral data, and event tracking infrastructure. The platform connects with:

- **Amplitude Analytics**: For user segmentation, cohort creation, and behavioral targeting
- **SDK Ecosystem**: Multi-language SDKs for seamless integration across web, mobile, and server environments
- **REST API**: For programmatic access and custom integrations
- **CDN Infrastructure**: For global deployment and caching optimization

The platform supports migration from other experimentation tools (specifically mentioning Optimizely flag migration) and provides enterprise-grade features including mutual exclusion groups and holdout management.

## API Endpoints and Technical Implementation

The platform provides multiple integration methods:

**SDK Integration**: Multi-language SDKs that handle user context objects, variant assignment, and targeting rule evaluation. SDKs support both online and offline modes with local caching capabilities.

**REST API**: Evaluation REST API for flag evaluation and variant assignment, supporting custom user properties and real-time property evaluation.

**QA and Testing**: Test Instrumentation functionality allows designated QA testers to validate experiments before full rollout, with specific tester ID assignment capabilities.

## Analysis and Optimization Workflow

The platform provides comprehensive analysis through multiple interface cards:

- **Filter Card**: Experiment result segmentation and filtering
- **Data Quality Card**: Sample ratio mismatch (SRM) detection and data integrity validation
- **Summary Card**: High-level performance metrics and statistical significance indicators
- **Analysis Card**: Detailed statistical analysis with confidence intervals and p-value calculations
- **Diagnostics Card**: Root cause analysis and troubleshooting insights
- **Bandits Card**: Multi-armed bandit specific performance tracking

Statistical significance thresholds require 100 users plus 25 conversions for binary metrics, or 100 users for non-binary metrics. The platform supports both 95% confidence levels (default) and custom confidence level configuration.

The workflow culminates in experiment completion options including percentage rollouts, complete rollbacks, or continued optimization, with sticky bucketing settings to maintain user experience consistency.