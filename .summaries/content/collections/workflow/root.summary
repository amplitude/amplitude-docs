Amplitude Experiment is a comprehensive experimentation and feature flag platform that enables teams to run A/B tests, multi-armed bandit experiments, and manage feature rollouts. The platform provides both client-side and server-side evaluation capabilities, sophisticated audience targeting, statistical analysis tools, and comprehensive experiment lifecycle management.

## Product Architecture and Feature Relationships

The platform operates through a hierarchical structure where **deployments** serve as the foundational configuration layer. Each deployment (client-side or server-side) generates unique access keys that route flags and experiments to the appropriate execution environment. These deployments integrate with **Experiment SDKs** or **REST APIs** to handle user context and variant assignment.

**Experiments** and **feature flags** share core infrastructure but serve different purposes. Experiments focus on hypothesis testing with statistical significance, while feature flags enable controlled feature rollouts with percentage-based traffic allocation. Both utilize the same targeting engine, variant management system, and delivery mechanisms.

The **evaluation system** supports two modes: **Remote evaluation** (server-side processing with real-time targeting) and **Local evaluation** (client-side processing with cached rules). This dual-mode architecture enables optimization for different latency and complexity requirements.

## Key Nomenclature and Definitions

**Bucketing Unit**: The entity used for consistent variant assignment (User ID, Device ID, Company ID, or custom identifiers). Ensures users receive the same variant across sessions through sticky bucketing.

**Variants**: Different versions of an experiment, including the control baseline and treatment variants. Each variant can contain JSON payloads and has configurable traffic distribution percentages.

**User Segments**: Rule-based targeting criteria using cohorts, user properties, or custom properties. Segments use Boolean AND logic within rules and if/else evaluation between segments.

**Exposure Events**: Triggered when users encounter an experiment via `.variant()` SDK calls. Distinguished from assignment events, exposure events determine statistical analysis inclusion.

**Rollout Percentage**: Controls what portion of eligible users enter the experiment bucketing process, separate from variant weight distribution.

**CUPED (Controlled-experiment Using Pre-Experiment Data)**: Variance reduction technique that improves statistical power by incorporating pre-experiment user behavior.

**Sequential Testing vs T-test**: Two statistical methodologies where Sequential testing enables continuous monitoring with early stopping, while T-test requires predetermined sample sizes.

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with **Amplitude Analytics projects**, sharing user identification systems and leveraging existing user properties and cohorts for targeting. The platform supports **cohort-based targeting** with hourly synchronization from Analytics.

The system provides **QA testing capabilities** through designated tester assignment using user/device/cohort IDs, enabling pre-rollout validation. **Root Cause Analysis** and **Diagnostics cards** offer post-rollout monitoring with sample ratio mismatch detection and variant assignment tracking.

**Multi-armed bandit experiments** represent an advanced capability using **Thompson sampling** for automated traffic reallocation based on performance metrics, available on Enterprise plans with specialized optimization algorithms.

## API Endpoints and Technical Implementation

The platform exposes **Evaluation REST APIs** for server-side integration alongside comprehensive **Experiment SDKs** for various programming languages. Key technical touchpoints include:

- **`.variant()` method calls** for variant retrieval and exposure event triggering
- **Deployment access keys** for authentication and routing
- **User context objects** containing identifiers and properties for targeting
- **Bucketing salt configuration** for consistent hash-based assignment
- **CDN caching considerations** for targeting rule distribution

## Statistical Analysis and Metrics Framework

The platform supports **seven custom metric types**: unique conversions, event totals, formula metrics, funnel conversions, return on retention, sum of property values, and average of property values. Each experiment can define **recommendation metrics** (primary success measures) and **guardrail metrics** (safety monitors).

**Minimum Detectable Effect (MDE)** configuration works with the **duration estimator** to calculate required sample sizes and experiment runtime. The analysis interface provides **p-values**, **confidence intervals**, and **statistical significance determination** with configurable confidence levels.

**Bonferroni correction** addresses multiple hypothesis testing concerns, while **custom exposure windows** enable flexible conversion attribution periods. The system supports both **binary and continuous metrics** with appropriate statistical treatments for each type.