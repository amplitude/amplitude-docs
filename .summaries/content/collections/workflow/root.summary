Amplitude Experiment is a comprehensive A/B testing and feature flag platform that enables teams to run sophisticated experiments with advanced statistical analysis, targeting capabilities, and flexible deployment options across client-side and server-side environments.

## Product Architecture and Feature Relationships

The platform operates through a structured workflow beginning with **experiment creation** and **deployment configuration**. Users establish deployments (client-side or server-side) with unique access keys and integrate Experiment SDKs to enable variant assignment and targeting rules. The system supports multiple **evaluation modes** - Remote evaluation for real-time variant assignment and Local evaluation for offline scenarios.

**Experiment types** encompass traditional A/B Tests for hypothesis validation and Multi-Armed Bandit experiments utilizing Thompson sampling to automatically reallocate traffic to best-performing variants. Feature flags function as specialized experiments focused on feature rollouts rather than statistical testing.

The **audience targeting system** leverages user segments based on cohorts, user properties, or custom attributes to determine bucketing eligibility. This integrates with **variant configuration** where experiments support multiple variants with customizable traffic distribution, stratified sampling, and segment-specific rollout controls.

**Statistical analysis** incorporates configurable preferences including CUPED variance reduction, Bonferroni correction for multiple hypothesis testing, sequential testing versus T-test methodologies, and adjustable confidence levels. The platform delivers comprehensive analysis through Summary, Data Quality, Analysis, and Diagnostics dashboards displaying p-values, confidence intervals, and performance metrics.

## Key Nomenclature and Definitions

**Deployments**: Environment configurations distinguishing client-side versus server-side implementations, each with unique access keys for SDK integration

**Bucketing Unit**: The entity used for experiment assignment (User ID, Device ID, or custom identifiers like Company ID)

**Variants**: Different experiment versions including control baseline and treatment variants, each configured with names, values, payloads, and traffic allocation percentages

**Exposure Events**: Triggered when `.variant()` method is called, distinguished from assignment events and essential for proper experiment analysis

**Rollout Percentage**: Controls the percentage of users eligible for experiment bucketing prior to variant assignment

**Sticky Bucketing**: Ensures users maintain consistent variant assignment throughout experiment duration

**Minimum Detectable Effect (MDE)**: The smallest effect size the experiment is designed to detect with statistical significance

**CUPED**: Controlled-experiment Using Pre-Existing data method for variance reduction utilizing pre-exposure metrics

**Sample Ratio Mismatch (SRM)**: Diagnostic verification ensuring proper traffic distribution between variants

**Stratified Sampling**: Advanced traffic allocation enabling different rollout percentages per user segment

## Broader Product Ecosystem Integration

Amplitude Experiment integrates extensively with the Amplitude Analytics ecosystem, leveraging user cohorts, behavioral data, and event tracking for sophisticated targeting and analysis. The platform connects with **Amplitude Analytics** for user property synchronization, cohort-based targeting, and results visualization through Experiment Results charts.

The system supports **Slack integration** for experiment notifications and statistical significance alerts. **CDN caching** optimizes performance while potentially introducing delays in targeting property updates.

**Multi-user deployments** accommodate complex organizational structures, while **mutual exclusion groups** and **holdouts** prevent experiment interference. The platform facilitates **Optimizely flag migration** for teams transitioning between experimentation platforms.

## API Endpoints and Technical Implementation

The platform provides **Experiment SDKs** across multiple programming languages and a **REST API for flag evaluation** when SDK integration isn't feasible. Key technical components include:

- **`.variant()` method calls** for variant retrieval and exposure event triggering
- **User context objects** containing user_id, device_id, and user_properties for targeting
- **Deployment access keys** for authentication and environment separation
- **QA tester ID assignment** for pre-launch testing and validation

The **Test Instrumentation** feature enables pre-rollout QA with designated testers, while **Start Experiment** and **Schedule start** functionalities provide controlled experiment launches. Post-experiment, teams can select **rollout** (deploy winning variant), **rollback** (revert to control), or **continue** (extend experiment duration) options.

**Duration estimator** tools calculate required sample sizes and experiment runtime based on statistical parameters including confidence level, power, control mean, standard deviation, and minimum detectable effect, supporting both T-test and sequential testing methodologies.