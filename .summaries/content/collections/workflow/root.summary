# Amplitude Experiment Product Overview

## High-Level Overview

Amplitude Experiment is a comprehensive experimentation and feature flagging platform that enables product teams to test hypotheses, roll out features safely, and make data-driven decisions. The platform supports both A/B testing and Multi-Armed Bandit experiments, with capabilities for client-side and server-side implementations.

Key features include:
- A/B Testing with static traffic allocation
- Multi-Armed Bandit experiments with automated traffic reallocation
- Feature Flag Rollouts for controlled feature releases
- Remote (server-side) and Local (client-side) evaluation modes
- Integration with Amplitude Analytics for cohort targeting and results analysis

## Product Relationships and Features

Amplitude Experiment is structured around several core components:

1. **Deployments**: Configuration entities that connect to Analytics projects and contain deployment keys for SDK integration. They come in two types:
   - Client-side: For web and mobile applications (JavaScript, iOS, Android SDKs)
   - Server-side: For backend services (Node.js, Java, etc.)

2. **Experiments**: The central testing entities that include:
   - Variants: Different versions of an experience (including control variant)
   - Bucketing Units: Entities used for randomization (user ID, device ID)
   - Audiences: Targeting criteria for experiment eligibility
   - Goals/Metrics: Success measurements categorized as recommendation, secondary, or guardrail metrics

3. **Evaluation Modes**:
   - Remote Evaluation: Amplitude's servers determine variant assignment
   - Local Evaluation: The SDK determines variant assignment on the client

4. **Experiment Workflow** support for:
   - Creation and configuration
   - Audience definition
   - Variant setup
   - Goal definition
   - Statistical preference configuration
   - Testing and QA
   - Launch and monitoring
   - Results analysis
   - Implementation of learnings

## Key Nomenclature and Definitions

- **Variant**: A specific version of a feature being tested
- **Control Variant**: The baseline version for comparison
- **Variant Payload**: JSON data structure containing configuration for a variant
- **Bucketing**: The process of assigning users to experiment variants
- **Bucketing Salt**: Value ensuring consistent but random variant assignment
- **Exposure Event**: Event triggering a user's inclusion in an experiment
- **Minimum Detectable Effect (MDE)**: Smallest meaningful difference an experiment can detect
- **Statistical Significance**: Confidence level that observed differences aren't due to chance
- **CUPED**: Controlled-experiment Using Pre-Existing Data, a variance reduction technique
- **Bonferroni Correction**: Method to adjust for multiple hypothesis testing
- **Sequential Testing**: Statistical approach allowing continuous monitoring of results
- **Variant Jumping**: When users receive different variants across sessions
- **Sample Ratio Mismatch**: When actual user distribution differs from intended distribution

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude product suite:

1. **Amplitude Analytics**: Provides user behavior data for targeting and analysis
2. **Amplitude Cohorts**: Used for audience targeting in experiments
3. **Amplitude SDKs**: Enable implementation across platforms
4. **Amplitude Data**: Powers statistical analysis and results visualization

The platform works with various development workflows through:
- Multiple SDK implementations
- API endpoints for programmatic control
- CDN-based delivery for client-side implementations

## Technical Implementation

### SDK Integration
Amplitude Experiment requires installing the appropriate SDK:
- Client-side: JavaScript, iOS, Android
- Server-side: Node.js, Java, Python, Go, Ruby

### Key API Concepts
- **User Context**: Contains user identifiers and properties for targeting
- **Variant Assignment**: Process determining which variant a user receives
- `variant()` function: Retrieves the assigned variant for a user

### Implementation Flow
1. Initialize SDK with deployment key
2. Create user context with appropriate identifiers
3. Fetch variants for experiments
4. Implement conditional logic based on variant assignment
5. Track exposure events when users encounter the experiment

### Statistical Analysis Features
- **Duration Estimator**: Calculates required sample size and experiment runtime
- **Multiple Testing Methods**: T-test and Sequential testing
- **Confidence Levels**: Configurable (default 95%)
- **Variance Reduction**: Optional CUPED implementation
- **Multiple Comparison Correction**: Bonferroni correction for multiple metrics

### Troubleshooting Tools
- Data Quality checks for experiment integrity
- Diagnostics card for identifying implementation issues
- Sample ratio mismatch detection
- Variant jumping identification
- QA tools for pre-launch verification