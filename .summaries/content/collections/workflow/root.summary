Amplitude Experiment is a comprehensive experimentation and feature flag platform that enables teams to run A/B tests, multi-armed bandit experiments, and manage feature rollouts. The platform supports both traditional hypothesis-driven A/B testing and automated optimization through machine learning algorithms, providing statistical analysis tools and real-time traffic allocation capabilities.

## Product Architecture and Feature Relationships

The platform operates through a hierarchical structure where **experiments** and **feature flags** are deployed through **deployments** (client-side or server-side) that generate unique access keys for SDK integration. Each experiment contains multiple **variants** with configurable traffic distribution, targeting specific **user segments** defined by cohorts, user properties, or custom attributes.

The experimentation workflow follows a structured pipeline: experiment creation → audience definition → variant configuration → goal setting → statistical preference configuration → testing/QA → launch → analysis. Feature flags follow a simplified path focused on rollout management rather than statistical testing.

**Evaluation modes** form a core architectural decision, with **Remote evaluation** providing real-time targeting updates through API calls, while **Local evaluation** offers faster performance by downloading experiment configurations to client applications. The **bucketing unit** (User ID, Device ID, Company ID, etc.) determines how traffic is allocated across variants.

## Key Nomenclature and Definitions

**Variants** represent different experiment treatments, including a control baseline and treatment variations, each containing names, values (slugified for SDK consumption), and JSON payloads for complex configurations.

**Bucketing** refers to the process of assigning users to experiment variants based on the bucketing unit and allocation ratios. **Rollout percentage** controls what portion of eligible users enter the experiment, while **traffic distribution** determines how bucketed users are split between variants.

**Assignment events** track when users are bucketed into experiments, while **exposure events** (triggered by `.variant()` SDK calls) indicate actual feature interaction. The **Amplitude exposure event** is automatically generated when SDKs request variant assignments.

**User segments** define targeting criteria through cohorts (synced hourly), user properties (cached), or custom properties (real-time evaluation). **Stratified sampling** enables non-uniform allocation ratios across different user segments.

**Statistical significance** is determined through p-values and confidence intervals, with support for **Sequential testing** (continuous monitoring) or **T-tests** (fixed sample size). **CUPED** (Controlled-experiment Using Pre-Experiment Data) provides variance reduction, while **Bonferroni correction** addresses multiple hypothesis testing.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the Amplitude Analytics platform, leveraging existing user identification, event tracking, and cohort management capabilities. **User context objects** containing user_id, device_id, and custom properties flow between systems for consistent targeting and analysis.

The platform supports **multi-user deployments** for enterprise scenarios and provides **mutual exclusion groups** to prevent experiment interference. **Holdouts** enable measuring cumulative experiment impact across multiple tests.

**QA testers** can be designated using specific user IDs, device IDs, or cohort membership for pre-launch validation through **Test Instrumentation**. **Root Cause Analysis** provides post-launch diagnostics for variant assignment monitoring.

## API Endpoints and Technical Integration

The platform exposes a **REST API for flag evaluation** alongside comprehensive **Experiment SDKs** for various programming languages. The primary integration point is the `.variant()` method call that retrieves variant assignments and triggers exposure tracking.

**Deployment access keys** authenticate SDK requests, with separate keys generated for client-side and server-side deployments. The system supports both **online mode** (real-time API calls) and **offline mode** (cached variant assignments) for different performance requirements.

**CDN caching** may introduce targeting delays, while **sticky bucketing** ensures consistent user experiences across sessions. **Variant jumping** detection helps identify bucketing inconsistencies that could compromise experiment validity.

The platform includes **Slack webhook notifications** for statistical significance alerts and provides **duration estimator** tools for sample size calculations based on minimum detectable effect (MDE), power analysis, and historical conversion rates.