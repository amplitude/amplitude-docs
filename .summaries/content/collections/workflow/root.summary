Amplitude Experiment is a comprehensive experimentation and feature flag management platform that enables organizations to conduct A/B tests, multi-armed bandit experiments, and feature rollouts. The platform provides both client-side and server-side evaluation capabilities with sophisticated targeting, statistical analysis, and traffic allocation features.

## Product Architecture and Feature Relationships

The platform operates through several interconnected components that form a complete experimentation workflow:

**Core Infrastructure:**
- **Deployments**: Client-side and server-side configurations with unique access keys that determine where and how experiments execute
- **Experiment SDKs**: Code libraries that integrate with applications to handle variant assignment and exposure tracking
- **Evaluation Modes**: Remote evaluation (server-side) vs Local evaluation (client-side) determining where bucketing decisions occur
- **REST API**: Alternative to SDKs for flag evaluation and experiment management

**Experiment Lifecycle Management:**
The platform supports a complete experiment workflow from creation through analysis. Experiments begin with configuration (experiment type, evaluation mode, bucketing units), progress through audience definition and variant setup, undergo testing and QA, launch with monitoring, and conclude with statistical analysis and decision-making.

**Traffic Management:**
Sophisticated traffic allocation includes percentage rollouts, stratified sampling, rollout controls per user segment, sticky bucketing for consistent user experiences, and uniform allocation ratios with customizable weight distribution between variants.

## Key Nomenclature and Definitions

**Experiment Types:**
- **A/B Test**: Traditional hypothesis testing with control and treatment variants
- **Multi-Armed Bandit**: Automated optimization using Thompson sampling to reallocate traffic to best-performing variants
- **Feature Flags**: Binary on/off switches for feature rollouts with percentage controls

**Statistical Concepts:**
- **Minimum Detectable Effect (MDE)**: Smallest effect size the experiment can reliably detect
- **CUPED**: Controlled-experiment using pre-existing data for variance reduction
- **Bonferroni Correction**: Multiple hypothesis testing adjustment
- **Sequential Testing**: Continuous monitoring vs traditional T-test approaches
- **Statistical Significance Thresholds**: 100 users + 25 conversions for binary metrics, 100 users for non-binary metrics

**User Management:**
- **Bucketing Unit**: Entity used for variant assignment (User, Company ID, custom identifiers)
- **User Context Object**: Data structure containing user identifiers and properties for targeting
- **Exposure Events**: Triggered when `.variant()` method is called, indicating user saw experiment
- **Assignment Events**: Record of user being bucketed into experiment variant

**Targeting and Segmentation:**
- **Rule-Based User Segments**: Targeting based on cohorts, user properties, or custom properties
- **Stratified Sampling**: Different rollout percentages per user segment
- **Sticky Bucketing**: Ensures consistent variant assignment across sessions

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem:

**Analytics Integration:**
- Leverages Amplitude's user identification system (user_id, device_id)
- Utilizes existing user properties and cohorts for targeting
- Integrates with Amplitude's event tracking for exposure and conversion measurement
- Supports custom metrics including unique conversions, event totals, formula metrics, funnel conversions, retention metrics, and property value aggregations

**Data Flow:**
- User context flows from applications through SDKs to evaluation engines
- Experiment results feed back into Amplitude analytics for comprehensive analysis
- CDN caching optimizes performance for client-side evaluations
- Real-time property evaluation for dynamic targeting

## API Endpoints and Technical Implementation

**Key SDK Methods:**
- `.variant()`: Primary method for variant retrieval and exposure event triggering
- User context passing for targeting evaluation
- Offline mode support for local variant serving

**Configuration APIs:**
- Deployment creation and management endpoints
- Experiment configuration and variant setup
- User segment and targeting rule definition
- Statistical preference configuration

**Analysis and Monitoring:**
- Duration estimator for sample size calculations
- Real-time diagnostics monitoring
- Sample ratio mismatch (SRM) detection
- Root cause analysis tools

**Quality Assurance:**
- Test Instrumentation for pre-rollout QA
- QA tester ID assignment for controlled testing
- Variant verification workflows
- Experiment scheduling and automated start functionality

The platform supports enterprise-scale experimentation with features like mutual exclusion groups, holdout controls, and comprehensive troubleshooting tools for common issues including device/user ID alignment, variant assignment problems, and targeting delays.