# Amplitude Experiment Product Overview

## High-Level Overview

Amplitude Experiment is a comprehensive experimentation and feature management platform that enables teams to test hypotheses, roll out features gradually, and make data-driven decisions. The platform supports both A/B testing and feature flag management with sophisticated statistical analysis capabilities.

Key features include:
- A/B testing with control and treatment variants
- Multi-Armed Bandit experiments using Thompson sampling for automated traffic allocation
- Feature flag rollouts with targeted deployment to specific user segments
- Advanced statistical analysis tools (CUPED, Bonferroni Correction, T-tests)
- Experiment duration estimation and sample size calculation

## Product Relationships and Features

Amplitude Experiment is structured around two primary capabilities that work together:

1. **Experimentation**: Enables statistical hypothesis testing through:
   - Traditional A/B tests with fixed traffic allocation
   - Multi-Armed Bandit experiments with dynamic traffic allocation
   - Statistical rigor through built-in analysis tools

2. **Feature Management**: Provides controlled feature rollouts through:
   - Feature flags with segment targeting
   - Percentage-based rollouts
   - Gradual deployment strategies

These capabilities are supported by two evaluation modes:
- **Remote Evaluation**: Server-side variant assignment via API calls
- **Local Evaluation**: Client-side variant assignment within the application

The platform supports both client-side (JavaScript) and server-side implementations, with dedicated SDKs for each deployment type.

## Key Nomenclature and Definitions

### Experiment Components
- **Variant**: A specific version in an experiment (including control)
- **Variant Payload**: JSON configuration data for a specific variant
- **Bucketing Unit**: Entity level for variant assignment (user ID, device ID)
- **Bucketing Salt**: Value ensuring random but consistent variant assignment
- **Exposure Event**: Event triggering user inclusion in an experiment
- **Rollout Percentage**: Portion of eligible users included in an experiment

### Statistical Terms
- **Recommendation Metric**: Primary success metric for an experiment
- **Secondary/Guardrail Metrics**: Additional monitored metrics
- **Minimum Detectable Effect (MDE)**: Smallest meaningful change to detect
- **CUPED**: Controlled-experiment Using Pre-Existing Data (variance reduction)
- **Sample Ratio Mismatch**: Diagnostic issue where actual user distribution differs from expected

### Feature Flag Concepts
- **Flag Enablement**: Active/inactive status of a feature flag
- **Segment Targeting**: Directing features to specific user segments
- **Rollout Percentage**: Control over percentage of users receiving a feature

## Product Ecosystem Integration

Amplitude Experiment is tightly integrated with Amplitude Analytics, creating a cohesive data-driven decision platform:

1. **Analytics Integration**: Each experiment deployment is associated with an Analytics project, allowing:
   - Use of existing user data and cohorts for targeting
   - Seamless tracking of experiment metrics
   - Unified user identity across platforms

2. **Deployment Architecture**:
   - Experiments require a deployment configuration linked to an Analytics project
   - The deployment provides keys that SDKs use to fetch experiment configurations
   - User context ensures consistent variant assignment across sessions

3. **Data Flow**:
   - User interactions generate events in Amplitude Analytics
   - These events serve as metrics for experiment analysis
   - Experiment results feed back into product development decisions

## API and Implementation Details

### API Endpoints and SDK Integration

1. **Remote Evaluation REST API**:
   - Used for server-side evaluation with endpoint calls
   - Requires API keys for authentication

2. **Local Evaluation SDKs**:
   - Client-side evaluation within the application
   - Available for various platforms and languages

### Implementation Workflow

For experiments:
1. Create an experiment with a unique key
2. Configure evaluation mode and bucketing unit
3. Define audience targeting
4. Add variants with payloads and traffic distribution
5. Define metrics (recommendation and secondary)
6. Configure statistical preferences
7. Test implementation with QA users
8. Launch and monitor
9. Analyze results and make decisions

For feature flags:
1. Create a flag with variants
2. Define user segments
3. Configure rollout percentages
4. Activate the flag

### Diagnostic Tools

The platform provides several troubleshooting capabilities:
- Sample Ratio Mismatch Detection
- Variant Jumping Detection
- Data Quality Checks
- Root Cause Analysis tools

Common implementation issues include misaligned user IDs between Analytics and Experiment, disabled flags, variant naming inconsistencies, and event configuration errors.