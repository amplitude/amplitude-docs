# Amplitude Experiment Product Overview

Amplitude Experiment is a comprehensive experimentation and feature management platform that enables teams to test hypotheses, roll out features gradually, and make data-driven decisions. The product allows users to create, configure, and analyze A/B tests and feature flags with sophisticated statistical analysis capabilities.

## Key Features and Concepts

### Core Capabilities
- **A/B Testing**: Traditional hypothesis testing with control and treatment variants
- **Multi-Armed Bandit Experiments**: Automated traffic allocation to better-performing variants using Thompson sampling
- **Feature Flag Rollouts**: Gradual deployment of features to specific user segments
- **Statistical Analysis**: Built-in statistical tools including CUPED, Bonferroni Correction, and T-tests
- **Duration Estimation**: Calculation of required sample size and experiment runtime

### Experiment Types
1. **A/B Tests**: Traditional experiments with fixed traffic allocation that run until statistical significance
2. **Multi-Armed Bandit**: Dynamic experiments that automatically reallocate traffic to better-performing variants using Thompson sampling

### Evaluation Modes
1. **Remote Evaluation**: Server-side evaluation where variant assignment happens via API calls
2. **Local Evaluation**: Client-side evaluation where variant assignment happens directly in the application

### Deployment Types
1. **Client-side**: JavaScript-based implementations for web applications
2. **Server-side**: Backend implementations for server applications

## Product Relationships and Architecture

### Integration with Amplitude Analytics
Amplitude Experiment is tightly integrated with Amplitude Analytics, allowing experiments to leverage existing user data and cohorts. Each experiment deployment is associated with an Analytics project, enabling seamless tracking of experiment metrics.

### Deployment and SDK Structure
Experiments require a deployment configuration that connects to an Analytics project. The appropriate SDK (client-side or server-side) must be installed to implement experiments. The deployment provides keys that the SDK uses to fetch experiment configurations.

### User Context and Bucketing
Experiments use a "User context" for variant assignment and targeting. This context typically includes user identifiers (user ID, device ID, etc.) that ensure consistent variant assignment across sessions (sticky bucketing).

## Key Nomenclature and Definitions

### Experiment Components
- **Variant**: A specific version or treatment in an experiment (including the control)
- **Variant Payload**: JSON data that defines the specific configuration for a variant
- **Bucketing Unit**: The entity level at which users are assigned to variants (user ID, device ID, etc.)
- **Bucketing Salt**: A value that ensures random but consistent variant assignment
- **Exposure Event**: The event that triggers a user to be included in an experiment
- **Rollout Percentage**: The portion of eligible users who will be included in an experiment

### Statistical Terms
- **Recommendation Metric**: The primary success metric for an experiment
- **Secondary/Guardrail Metrics**: Additional metrics monitored during an experiment
- **Minimum Detectable Effect (MDE)**: The smallest meaningful change an experiment aims to detect
- **CUPED**: Controlled-experiment Using Pre-Existing Data, a variance reduction technique
- **Sample Ratio Mismatch**: A diagnostic issue where the actual distribution of users differs from the expected distribution

### Feature Flag Concepts
- **Flag Enablement**: Whether a feature flag is active or inactive
- **Segment Targeting**: Directing features to specific user segments based on properties
- **Rollout Percentage**: Controlling what percentage of eligible users receive a feature

## Implementation Details

### API and SDK Integration
Amplitude Experiment is implemented through SDKs for various platforms. The evaluation process can be performed via:

1. **Remote Evaluation REST API**: Server-side evaluation with endpoint calls
2. **Local Evaluation SDKs**: Client-side evaluation within the application

### Experiment Creation Workflow
1. Create an experiment with a unique key
2. Configure evaluation mode and bucketing unit
3. Define audience targeting
4. Add variants with appropriate payloads and traffic distribution
5. Define goals (recommendation and secondary metrics)
6. Configure statistical preferences
7. Test implementation with QA users
8. Launch and monitor the experiment
9. Analyze results and make decisions

### Feature Flag Implementation
Feature flags follow a similar workflow but focus on controlled rollouts rather than statistical testing:
1. Create a flag with variants
2. Define user segments
3. Configure rollout percentages
4. Activate the flag

## Troubleshooting and Diagnostics

The platform provides several diagnostic tools to identify issues:
- **Sample Ratio Mismatch Detection**: Identifies imbalances in variant assignment
- **Variant Jumping Detection**: Identifies users changing variants during an experiment
- **Data Quality Checks**: Verifies experiment data integrity
- **Root Cause Analysis**: Tools to diagnose implementation issues

Common issues include misaligned user IDs between Amplitude Analytics and Experiment, disabled flags, variant naming inconsistencies, and event configuration errors.

## Analysis and Decision Making

After running an experiment, users can:
1. **Roll Out Winning Variant**: Implement the successful variant for all users
2. **Roll Back**: Revert to the control variant
3. **Continue**: Maintain the experiment for further data collection

The platform provides detailed analysis cards showing statistical significance, confidence intervals, and metric performance by variant to guide these decisions.