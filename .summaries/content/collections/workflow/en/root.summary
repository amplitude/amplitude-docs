## Amplitude Experiment Product Overview

Amplitude Experiment is a comprehensive experimentation and feature flag platform that enables teams to run A/B tests, multi-armed bandit experiments, and manage feature rollouts. The platform supports both traditional hypothesis-driven testing and automated optimization through machine learning algorithms, providing statistical analysis tools and real-time traffic allocation capabilities.

## Product Architecture and Feature Relationships

The platform operates through several interconnected components:

**Experiment Types and Evaluation Modes:**
- **A/B Tests**: Traditional hypothesis-driven experiments with control and treatment variants
- **Multi-Armed Bandit Experiments**: Automated optimization using Thompson sampling that reallocates traffic to best-performing variants based on primary optimization metrics
- **Feature Flags**: Binary on/off switches for feature rollouts with percentage-based traffic control

**Deployment and Evaluation Infrastructure:**
- **Client-side vs Server-side Deployments**: Each generates unique access keys for SDK integration
- **Remote vs Local Evaluation Modes**: Remote evaluation provides real-time configuration updates, while local evaluation offers faster response times with cached configurations
- **Experiment SDKs and REST API**: Multiple integration options for flag evaluation and variant assignment

**User Targeting and Segmentation:**
- **Bucketing Units**: Configurable units (User ID, Device ID, Company ID, City) that determine experiment assignment consistency
- **User Segments**: Rule-based targeting using cohorts, user properties, or custom properties with Boolean AND logic within segments and if/else evaluation between segments
- **Sticky Bucketing**: Ensures users remain in assigned variants throughout experiment duration

## Key Nomenclature and Definitions

**Core Experiment Concepts:**
- **Variants**: Different versions of an experiment including control (baseline) and treatment variants, each with names, values, and JSON payloads
- **Rollout Percentage**: Controls what percentage of eligible users enter the experiment bucketing process
- **Traffic Distribution**: Percentage allocation of users between variants within the experiment
- **Exposure Event**: Triggered when `.variant()` method is called, indicating user saw the experiment
- **Assignment Event**: Records when a user is bucketed into an experiment variant

**Statistical and Analysis Terms:**
- **Minimum Detectable Effect (MDE)**: Smallest effect size the experiment can reliably detect
- **CUPED (Controlled-experiment Using Pre-Experiment Data)**: Variance reduction technique using historical user data
- **Sequential Testing vs T-test**: Two statistical approaches for determining significance
- **Bonferroni Correction**: Adjustment for multiple hypothesis testing to control false positive rates
- **Sample Ratio Mismatch (SRM)**: Diagnostic indicator of potential experiment setup issues

**Advanced Features:**
- **Stratified Sampling**: Non-uniform allocation ratios across different user segments
- **Variant Jumping**: Undesired behavior where users switch between variants
- **Root Cause Analysis**: Post-rollout diagnostic tool for monitoring variant assignment issues

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem:

**Data Integration:**
- Leverages Amplitude's user cohorts for targeting with hourly synchronization
- Utilizes user properties and custom properties from Amplitude's user profiles
- Connects with Amplitude's event tracking for exposure and conversion measurement

**Analysis and Reporting:**
- **Custom Metrics**: Seven types including unique conversions, event totals, formula metrics, funnel conversions, return on retention, sum/average of property values
- **Recommendation vs Guardrail Metrics**: Primary optimization targets versus safety metrics
- **Statistical Notifications**: Slack webhook integration for significance alerts
- **Confidence Intervals**: Provided for binary metrics with configurable confidence levels

**Quality Assurance and Monitoring:**
- **Test Instrumentation**: Pre-rollout QA system with designated tester assignment using user/device/cohort IDs
- **Diagnostics Card**: Real-time monitoring of experiment health and user enrollment
- **Duration Estimator**: Tool for calculating required sample sizes and experiment runtime

## API Endpoints and Technical Implementation

**Core SDK Methods:**
- `.variant()`: Primary method for retrieving experiment variants and triggering exposure events
- User context object configuration for variant assignment and targeting rule evaluation

**Deployment Configuration:**
- Deployment access keys for authentication
- Bucketing salt configuration for consistent user assignment
- CDN caching considerations for targeting delays

**Integration Patterns:**
- Multi-user deployment support
- Offline mode variant delivery
- REST API flag evaluation for server-side implementations
- Mutual exclusion groups and holdout management for Enterprise plans

The platform supports migration from other experimentation tools like Optimizely and provides comprehensive troubleshooting documentation for common issues including device/user ID alignment, variant assignment problems, and user bucketing inconsistencies.