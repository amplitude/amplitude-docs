# Amplitude Experiment Product Overview

Amplitude Experiment is a comprehensive experimentation and feature flagging platform that enables product teams to test hypotheses, roll out features safely, and make data-driven decisions. The platform supports both A/B testing and Multi-Armed Bandit experiments, allowing teams to optimize their product experiences based on user behavior and metrics.

## Key Features and Concepts

### Experiment Types
- **A/B Testing**: Traditional hypothesis testing with control and treatment variants
- **Multi-Armed Bandit**: Automated traffic allocation to better-performing variants using Thompson sampling
- **Feature Flag Rollouts**: Controlled feature releases without full experimentation

### Core Components
1. **Deployments**: Server-side or client-side configurations that connect to an Analytics project
2. **Evaluation Modes**:
   - Remote Evaluation: Variant assignment happens on Amplitude's servers
   - Local Evaluation: Variant assignment happens within the client application
3. **Variants**: Different versions of a feature or experience being tested
4. **Bucketing**: The process of assigning users to experiment variants
5. **Goals/Metrics**: Success measurements for experiments (recommendation, secondary, guardrail)
6. **Audience Targeting**: Rules that determine which users are eligible for an experiment

### Experiment Workflow
1. Create an experiment with a unique key
2. Configure delivery settings (evaluation mode, bucketing unit)
3. Define variants with custom payloads and traffic distribution
4. Set audience targeting criteria
5. Define experiment goals and metrics
6. Test implementation with QA users
7. Launch the experiment
8. Analyze results and make decisions

## Product Relationships and Architecture

### Integration with Amplitude Analytics
Amplitude Experiment is tightly integrated with Amplitude Analytics, allowing experiments to:
- Use Analytics cohorts for targeting
- Send experiment data to Analytics for analysis
- Leverage user properties and events from Analytics

### Deployment Types
1. **Client-side Deployments**: 
   - Used for front-end experiments
   - Requires client-side SDK implementation
   - Suitable for UI changes and user experience tests

2. **Server-side Deployments**:
   - Used for back-end experiments
   - Requires server-side SDK implementation
   - Suitable for algorithm tests, pricing changes, and infrastructure experiments

### SDK Ecosystem
Amplitude Experiment provides SDKs for various platforms that handle:
- Fetching experiment configurations
- Assigning users to variants
- Tracking exposure events
- Managing feature flags

## Key Nomenclature and Definitions

- **Experiment Key**: Unique identifier for an experiment
- **Bucketing Unit**: The entity used for variant assignment (user ID, device ID, etc.)
- **Bucketing Salt**: Random string that ensures consistent but unique bucketing across experiments
- **Variant**: A specific version or treatment in an experiment
- **Control Variant**: The baseline version against which other variants are compared
- **Payload**: JSON configuration attached to variants that defines the experience
- **Exposure Event**: The event that triggers a user to be included in an experiment
- **Assignment Event**: The event that records which variant a user was assigned to
- **Rollout Percentage**: The portion of eligible users who will be included in an experiment
- **Minimum Detectable Effect (MDE)**: The smallest meaningful difference an experiment can detect
- **CUPED**: Controlled-experiment Using Pre-Existing Data, a variance reduction technique
- **Stratified Sampling**: Ensuring proportional representation across user segments
- **Sticky Bucketing**: Ensuring users consistently see the same variant
- **Variant Jumping**: When users inconsistently receive different variants

## Statistical Analysis Features

- **Duration Estimator**: Calculates required sample size and experiment runtime
- **Sequential Testing**: Continuously evaluates results as data accumulates
- **T-tests**: Traditional hypothesis testing with fixed sample sizes
- **Confidence Levels**: Typically set at 95%, customizable
- **Bonferroni Correction**: Adjusts for multiple hypothesis testing
- **Thompson Sampling**: Algorithm used in Multi-Armed Bandit experiments

## API and Implementation

### Evaluation REST API
Used for remote evaluation to determine variant assignment:
```
POST https://api.lab.amplitude.com/v1/vardata
```

### SDK Implementation Examples
While specific code examples aren't provided in the documentation summaries, the platform supports multiple SDKs with methods for:
- Fetching and initializing experiments
- Getting variant assignments
- Tracking exposures
- Managing feature flags

### QA Testing
The platform supports adding test users with specific IDs to verify experiment implementation before launch.

## Use Cases

1. **A/B Testing Product Features**: Test different UI designs, workflows, or functionality
2. **Gradual Feature Rollouts**: Safely release new features to increasing percentages of users
3. **Personalization Experiments**: Test different experiences for different user segments
4. **Algorithm Optimization**: Test different recommendation or pricing algorithms
5. **Multi-Armed Bandit Optimization**: Automatically optimize for conversion or engagement

Amplitude Experiment provides a complete toolkit for product experimentation, from hypothesis formation to implementation, analysis, and decision-making, helping teams build better products through data-driven development.