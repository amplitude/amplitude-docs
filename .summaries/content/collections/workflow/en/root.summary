## Amplitude Experiment: Comprehensive Experimentation and Feature Flag Platform

Amplitude Experiment is a comprehensive experimentation and feature flag management platform that enables organizations to run A/B tests, multi-armed bandit experiments, and deploy feature flags with sophisticated targeting and statistical analysis capabilities. The platform supports both client-side and server-side implementations through SDKs and REST APIs, providing flexible deployment options for web and feature experiments.

## Product Architecture and Feature Relationships

The platform operates through a hierarchical structure where **deployments** serve as the foundational configuration layer, containing access keys and evaluation modes (Remote vs Local). Within deployments, organizations create **experiments** (A/B tests or multi-armed bandit) and **feature flags** that utilize shared infrastructure for user targeting, variant assignment, and traffic distribution.

**Experiments** encompass multiple components working in concert: variant configuration with traffic distribution controls, audience targeting through user segments, goal definition with recommendation and guardrail metrics, and statistical analysis with customizable preferences. The platform supports both traditional hypothesis-driven A/B testing and automated multi-armed bandit experiments that use Thompson sampling for dynamic traffic reallocation.

**Feature flags** share the same targeting and delivery infrastructure but focus on feature rollouts rather than statistical testing, utilizing rollout percentages and weight distribution for controlled feature deployment.

The **evaluation system** operates in two modes: Remote evaluation queries Amplitude's servers in real-time, while Local evaluation downloads flag configurations for offline processing. Both modes support the same targeting rules and user context handling.

## Key Nomenclature and Definitions

**Bucketing Unit**: The entity used for consistent variant assignment (User ID, Device ID, or custom identifiers), ensuring users receive the same variant across sessions through sticky bucketing.

**Variants**: Different versions of an experiment or feature flag, including control baselines and treatment variants, each with configurable names, values, payloads (JSON), and traffic allocation percentages.

**User Segments**: Targeting rules based on cohorts, user properties, or custom properties that determine bucketing eligibility and exposure event triggering.

**Exposure Events**: Analytics events triggered when users encounter experiment variants, primarily through `.variant()` SDK calls, used for statistical analysis and sample ratio mismatch detection.

**Rollout Percentage vs Weight Distribution**: Rollout percentage controls what portion of eligible users enter the experiment, while weight distribution allocates traffic between variants within that rollout.

**Evaluation Modes**: Remote evaluation provides real-time server queries with up-to-date targeting, while Local evaluation offers faster performance through cached configurations with periodic updates.

**Statistical Preferences**: Configurable analysis settings including CUPED (variance reduction), Bonferroni correction (multiple hypothesis testing), Sequential vs T-test methodologies, and confidence levels.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude Analytics ecosystem, leveraging user cohorts, behavioral data, and event tracking for sophisticated targeting and analysis. The platform utilizes Amplitude's user identification system (user_id, device_id) and property framework for consistent cross-product experiences.

**Multi-Armed Bandit** experiments represent an advanced capability available on Enterprise plans, using Thompson sampling algorithms to automatically optimize traffic allocation based on primary metrics, differing from traditional A/B tests by eliminating the need for statistical significance testing.

**Quality Assurance** workflows include Test Instrumentation for pre-rollout validation, designated tester targeting, and post-rollout diagnostics with Root Cause Analysis for monitoring variant assignment and user enrollment patterns.

## API Endpoints and Technical Implementation

The platform provides multiple integration methods:

- **Experiment SDKs** for client-side and server-side implementation with user context handling
- **Evaluation REST API** for flag evaluation without SDK integration
- **`.variant()` method calls** that trigger exposure events and return variant assignments
- **Deployment access keys** for authentication and configuration management

**Command-line equivalent operations** include experiment lifecycle management through Start Experiment and Complete Experiment controls, rollout percentage adjustments, and experiment scheduling capabilities.

The platform supports advanced features like **stratified sampling** with per-segment rollout controls, **mutual exclusion groups** for experiment isolation, and **holdouts** for measuring overall program impact. Statistical analysis includes duration estimation tools, minimum detectable effect (MDE) configuration, and comprehensive diagnostics for troubleshooting variant assignment issues and user bucketing inconsistencies.