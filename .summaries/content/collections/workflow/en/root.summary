## Amplitude Experiment: A/B Testing and Feature Flag Platform

Amplitude Experiment is a comprehensive experimentation platform that enables teams to run A/B tests, multi-armed bandit experiments, and deploy feature flags with sophisticated targeting and statistical analysis capabilities. The platform supports both client-side and server-side implementations through SDKs and REST APIs, providing flexible deployment options for web and feature experiments.

## Product Architecture and Feature Relationships

The platform operates through a structured workflow that begins with **experiment creation** and **deployment configuration**. Users create deployments (client-side or server-side) with unique access keys and install Experiment SDKs to enable variant assignment and targeting rules. The system supports multiple **evaluation modes** - Remote evaluation for real-time variant assignment and Local evaluation for offline scenarios.

**Experiment types** include traditional A/B Tests for hypothesis validation and Multi-Armed Bandit experiments that use Thompson sampling to automatically reallocate traffic to best-performing variants. Feature flags operate as a specialized experiment type focused on feature rollouts rather than statistical testing.

The **audience targeting system** leverages user segments based on cohorts, user properties, or custom properties to determine bucketing eligibility. This integrates with **variant configuration** where experiments can have multiple variants with customizable traffic distribution, stratified sampling, and rollout controls per segment.

**Statistical analysis** is powered by configurable preferences including CUPED variance reduction, Bonferroni correction for multiple hypothesis testing, sequential testing vs T-test methodologies, and confidence level settings. The platform provides comprehensive analysis through Summary, Data Quality, Analysis, and Diagnostics cards that display p-values, confidence intervals, and performance metrics.

## Key Nomenclature and Definitions

**Deployments**: Environment configurations (client-side vs server-side) with unique access keys for SDK integration

**Bucketing Unit**: The entity used for experiment assignment (User ID, Device ID, or custom identifiers like Company ID)

**Variants**: Different versions of an experiment including control baseline and treatment variants, each with names, values, payloads, and traffic allocation percentages

**Exposure Events**: Triggered when `.variant()` method is called, distinguishing from assignment events and enabling proper experiment analysis

**Rollout Percentage**: Controls what percentage of users are eligible for experiment bucketing before variant assignment

**Sticky Bucketing**: Ensures users remain in the same variant throughout the experiment duration

**Minimum Detectable Effect (MDE)**: The smallest effect size the experiment is designed to detect with statistical significance

**CUPED**: Controlled-experiment Using Pre-Existing data method for variance reduction using pre-exposure metrics

**Sample Ratio Mismatch (SRM)**: Diagnostic check ensuring proper traffic distribution between variants

**Stratified Sampling**: Advanced traffic allocation allowing different rollout percentages per user segment

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude Analytics ecosystem, leveraging user cohorts, behavioral data, and event tracking for sophisticated targeting and analysis. The platform connects with **Amplitude Analytics** for user property synchronization, cohort-based targeting, and results visualization through Experiment Results charts.

The system supports **Slack integration** for experiment notifications and statistical significance alerts. **CDN caching** is implemented for performance optimization, though it can introduce delays in targeting property updates.

**Multi-user deployments** enable complex organizational structures, while **mutual exclusion groups** and **holdouts** prevent experiment interference. The platform accommodates **Optimizely flag migration** for teams transitioning between experimentation platforms.

## API Endpoints and Technical Implementation

The platform provides **Experiment SDKs** for multiple programming languages and a **REST API for flag evaluation** when SDK integration isn't feasible. Key technical components include:

- **`.variant()` method calls** for variant retrieval and exposure event triggering
- **User context objects** containing user_id, device_id, and user_properties for targeting
- **Deployment access keys** for authentication and environment separation
- **QA tester ID assignment** for pre-launch testing and validation

The **Test Instrumentation** feature enables pre-rollout QA with designated testers, while **Start Experiment** and **Schedule start** functionalities provide controlled experiment launches. Post-experiment, teams can choose between **rollout** (deploy winning variant), **rollback** (revert to control), or **continue** (extend experiment duration) options.

**Duration estimator** tools calculate required sample sizes and experiment runtime based on statistical parameters including confidence level, power, control mean, standard deviation, and minimum detectable effect, supporting both T-test and sequential testing methodologies.