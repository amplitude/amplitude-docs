# Amplitude Experiment Product Overview

Amplitude Experiment is a comprehensive experimentation and feature flagging platform that enables product teams to test hypotheses, roll out features safely, and make data-driven decisions. The platform supports both A/B testing and Multi-Armed Bandit experiments, allowing teams to optimize their product experiences based on user behavior and performance metrics.

## Key Features and Concepts

### Experiment Types
- **A/B Testing**: Traditional hypothesis testing with static traffic allocation
- **Multi-Armed Bandit**: Advanced experimentation that automatically reallocates traffic to better-performing variants using Thompson sampling
- **Feature Flag Rollouts**: Controlled feature releases without formal experimentation

### Core Components
- **Deployments**: Server-side or client-side configurations that connect to an Analytics project
- **Variants**: Different versions of an experience being tested (including a control variant)
- **Evaluation Modes**:
  - Remote: Evaluation happens on Amplitude's servers
  - Local: Evaluation happens within the client application
- **Bucketing Units**: Determines how users are assigned to variants (user ID, device ID, etc.)
- **Metrics**: Measurements used to evaluate experiment performance
- **Audience Targeting**: Rules that determine which users are eligible for an experiment

### Statistical Analysis
- **Duration Estimator**: Calculates required sample size and experiment runtime
- **Statistical Preferences**: Configurable settings including CUPED, Bonferroni Correction, and test types
- **Results Analysis**: Tools for interpreting statistical significance, p-values, and confidence intervals

## Product Relationships and Architecture

Amplitude Experiment integrates closely with Amplitude Analytics, using the same user identification system to ensure consistent tracking and analysis. The workflow consists of:

1. **Configuration**: Creating deployments linked to Analytics projects
2. **Experiment Creation**: Setting up variants, goals, and targeting
3. **Implementation**: Installing SDKs and instrumenting code
4. **Testing**: QA procedures to verify correct implementation
5. **Activation**: Launching the experiment to users
6. **Analysis**: Monitoring results and making decisions

The platform uses a client-server architecture where:
- Server-side deployments evaluate variants on Amplitude's infrastructure
- Client-side deployments use SDKs to evaluate locally on user devices
- Both approaches use the same underlying bucketing algorithm to ensure consistent user experiences

## Key Nomenclature and Definitions

- **Experiment Key**: Unique identifier for an experiment
- **Deployment Keys**: Credentials that authenticate SDK requests
- **Variants**: Different versions of a feature or experience being tested
- **Variant Payload**: JSON data structure containing configuration for a variant
- **Bucketing**: The process of assigning users to experiment variants
- **Exposure Event**: The event that triggers a user to be included in an experiment
- **Recommendation Metric**: The primary success metric for an experiment
- **Guardrail/Secondary Metrics**: Additional metrics monitored to ensure changes don't harm the user experience
- **Minimum Detectable Effect (MDE)**: The smallest meaningful change an experiment aims to detect
- **Statistical Significance**: The likelihood that observed differences between variants are not due to chance
- **CUPED**: Controlled-experiment Using Pre-Existing Data, a variance reduction technique
- **Sticky Bucketing**: Ensuring users consistently see the same variant

## Integration with Amplitude Ecosystem

Amplitude Experiment is tightly integrated with other Amplitude products:

1. **Amplitude Analytics**: Provides user data for targeting and metrics for analysis
2. **Amplitude Cohorts**: Can be used to define experiment audiences
3. **Amplitude SDKs**: Used to implement experiments and track user interactions

The platform leverages Amplitude's user identification system, allowing experiments to target users based on their behavior and properties tracked in Analytics.

## API Endpoints and SDK Usage

### REST API
- Evaluation API: Used for server-side variant assignment

### SDK Implementation
- Client-side SDKs available for Web, iOS, Android, and React Native
- Server-side SDKs for Node.js, Java, Python, Go, and Ruby

### Common SDK Methods
```javascript
// Initialize the SDK
amplitude.experiment.initialize('<DEPLOYMENT_KEY>');

// Fetch variants for a user
const variants = await amplitude.experiment.fetch({
  user_id: 'user@example.com'
});

// Check if a user is in a specific variant
const variant = amplitude.experiment.variant('<EXPERIMENT_KEY>');
```

## Workflow Process

1. **Create an experiment**: Define key, type, and basic configuration
2. **Configure delivery**: Set evaluation mode and bucketing unit
3. **Add variants**: Create variants with custom names, values, and traffic distribution
4. **Define audience**: Target all users or specific segments
5. **Define goals**: Set recommendation and secondary metrics
6. **Finalize statistical preferences**: Configure analysis settings
7. **Test the experiment**: Verify implementation with QA testers
8. **Launch and monitor**: Activate the experiment and analyze results
9. **Take action**: Roll out winning variant, roll back, or continue testing

This comprehensive platform enables teams to make data-driven decisions through rigorous experimentation while providing the safety mechanisms of feature flags for controlled rollouts.