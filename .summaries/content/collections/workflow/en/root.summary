## Amplitude Experiment: Comprehensive Experimentation and Feature Flag Platform

Amplitude Experiment is a comprehensive experimentation and feature flag platform that enables organizations to conduct A/B tests, multi-armed bandit experiments, and feature rollouts. The platform provides both client-side and server-side evaluation capabilities with sophisticated targeting, statistical analysis, and automated optimization features.

## Product Architecture and Feature Relationships

The platform operates through several interconnected components that form a complete experimentation workflow:

**Core Infrastructure:**
- **Deployments**: Client-side and server-side configurations with unique access keys that determine where and how experiments execute
- **Evaluation Modes**: Remote evaluation (server-side processing) vs Local evaluation (client-side processing) with different performance and targeting characteristics
- **SDKs and REST API**: Multiple integration options for flag evaluation and variant assignment across different technology stacks

**Experiment Types:**
- **A/B Tests**: Traditional hypothesis-driven experiments with statistical significance testing
- **Multi-Armed Bandit Experiments**: Automated optimization using Thompson sampling that reallocates traffic to best-performing variants
- **Feature Flags**: Simple on/off toggles or multi-variant flags for feature rollouts with percentage-based traffic allocation

**Targeting and Segmentation:**
- **User Segments**: Rule-based targeting using cohorts, user properties, or custom properties
- **Bucketing Units**: Configurable units (User ID, Device ID, Company ID) that determine experiment assignment consistency
- **Stratified Sampling**: Advanced traffic distribution with rollout controls per segment

## Key Nomenclature and Definitions

**Experiment Configuration:**
- **Variants**: Different versions of an experiment including control baseline, with customizable names, values, payloads (JSON), and traffic distribution percentages
- **Bucketing**: The process of assigning users to experiment variants based on bucketing units and targeting rules
- **Rollout Percentage**: Controls what portion of eligible users are included in the experiment
- **Sticky Bucketing**: Ensures users remain in the same variant across sessions

**Measurement and Analysis:**
- **Exposure Event**: Triggered by `.variant()` SDK calls, indicating when a user actually sees an experiment variant
- **Assignment Event**: Records when a user is bucketed into an experiment, separate from exposure
- **Primary Metric**: The main success metric for optimization in multi-armed bandit experiments
- **Minimum Detectable Effect (MDE)**: The smallest effect size the experiment is designed to detect

**Statistical Framework:**
- **CUPED**: Controlled-experiment using pre-existing data for variance reduction
- **Bonferroni Correction**: Multiple hypothesis testing adjustment to control false positive rates
- **Sequential Testing vs T-test**: Different statistical approaches for ongoing vs fixed-duration analysis
- **Sample Ratio Mismatch (SRM)**: Diagnostic check for unexpected traffic distribution between variants

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem:

**Data Integration:**
- Leverages Amplitude's user identification system (user_id, device_id) for consistent cross-platform tracking
- Utilizes existing user properties and cohorts from Amplitude Analytics for sophisticated targeting
- Shares event data with Amplitude Analytics for comprehensive funnel and retention analysis

**Measurement Capabilities:**
- **Seven Custom Metric Types**: Unique conversions, event totals, formula metrics, funnel conversions, return on retention, sum of property values, average of property values
- **Recommendation vs Guardrail Metrics**: Primary optimization targets vs protective monitoring metrics
- **Duration Estimator**: Statistical power calculations for sample size and experiment runtime planning

**Analysis and Reporting:**
- **Multi-card Analysis Interface**: Filter, Data Quality, Summary, Analysis, and Diagnostics cards for comprehensive result interpretation
- **Statistical Significance Testing**: P-values, confidence intervals, and automated notifications
- **Segment Analysis**: Group-by functionality for heterogeneous treatment effects
- **Diagnostics Monitoring**: Variant jumping detection, exposure tracking, and root cause analysis

## API Endpoints and Technical Implementation

**Core SDK Methods:**
- `.variant()`: Primary method for retrieving experiment variants and triggering exposure events
- User context passing through SDK initialization and REST API calls

**Configuration Endpoints:**
- Evaluation REST API for server-side flag evaluation
- Deployment-specific access keys for authentication
- Multi-user deployment support for complex organizational structures

**Testing and QA:**
- **Test Instrumentation**: Pre-rollout QA with designated tester IDs
- **QA Tester Assignment**: Specific user targeting for experiment validation
- **Variant Verification**: Confirmation of proper variant delivery before launch

**Operational Controls:**
- **Start Experiment**: Manual and scheduled experiment activation
- **Experiment Completion Options**: Rollout (promote winning variant), rollback (revert to control), or continue (maintain current state)
- **Percentage Rollouts**: Gradual feature deployment with configurable traffic allocation

The platform supports enterprise-scale experimentation with features like mutual exclusion groups, holdout controls, and integration capabilities including Slack webhook notifications for automated result sharing.