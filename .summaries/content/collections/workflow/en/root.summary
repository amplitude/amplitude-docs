# Amplitude Experiment Product Overview

Amplitude Experiment is a comprehensive experimentation and feature flagging platform that enables product teams to test hypotheses, roll out features safely, and make data-driven decisions. The platform supports both A/B testing and Multi-Armed Bandit experiments, allowing teams to optimize their product experiences based on user behavior and performance metrics.

## Key Features and Concepts

### Experiment Types
- **A/B Testing**: Traditional hypothesis testing with static traffic allocation
- **Multi-Armed Bandit**: Advanced experimentation that automatically reallocates traffic to better-performing variants using Thompson sampling
- **Feature Flag Rollouts**: Controlled feature releases without formal experimentation

### Core Components
- **Deployments**: Server-side or client-side configurations that connect to Analytics projects
- **Variants**: Different versions of an experience being tested (including a control variant)
- **Evaluation Modes**:
  - Remote: Variant assignment happens on Amplitude's servers
  - Local: Variant assignment happens within the client application
- **Bucketing Units**: User identifiers used for consistent variant assignment (e.g., user ID, device ID)
- **Metrics**: Measurements used to evaluate experiment performance
- **Audience Targeting**: Rules that determine which users are eligible for an experiment

### Statistical Analysis
- **Duration Estimator**: Calculates required sample size and experiment runtime
- **Statistical Preferences**: Configurable settings including CUPED, Bonferroni Correction, and test types
- **Results Analysis**: Tools for interpreting statistical significance, p-values, and confidence intervals

## Product Relationships and Architecture

Amplitude Experiment integrates closely with Amplitude Analytics, using the same user identification system to ensure consistent tracking and analysis. The workflow follows a structured process:

1. **Creation and Configuration**: Set up experiments with specific keys, evaluation modes, and bucketing units
2. **Variant Definition**: Create multiple variants with custom payloads (including JSON for dynamic experiences)
3. **Audience Targeting**: Define eligible users through cohorts or property-based rules
4. **Goal Setting**: Configure recommendation metrics and secondary metrics to measure success
5. **Testing and Launch**: QA with test users before full deployment
6. **Analysis**: Evaluate results through statistical analysis
7. **Completion Actions**: Roll out winning variants, roll back changes, or continue experiments

## Key Nomenclature and Definitions

- **Experiment Key**: Unique identifier for an experiment
- **Deployment Keys**: Credentials that connect client applications to Amplitude Experiment
- **Bucketing Salt**: Value that ensures consistent but unique user assignment across experiments
- **Exposure Event**: The trigger that determines when a user is included in an experiment
- **Variant()**: Function used to retrieve variant assignments
- **Minimum Detectable Effect (MDE)**: Smallest meaningful difference an experiment can detect
- **Statistical Significance**: Confidence that observed differences aren't due to random chance
- **Sample Ratio Mismatch**: Discrepancy between expected and actual variant distribution
- **Variant Jumping**: When users move between variants during an experiment
- **CUPED**: Controlled-experiment Using Pre-Existing Data, a variance reduction technique
- **Bonferroni Correction**: Statistical method to address multiple hypothesis testing

## Product Ecosystem Integration

Amplitude Experiment functions as part of the broader Amplitude product suite:

1. **Amplitude Analytics**: Provides user data for targeting and metrics for measuring experiment outcomes
2. **Amplitude SDKs**: Client and server libraries that implement experiment logic in applications
3. **Evaluation REST API**: Allows server-side applications to request variant assignments

The platform supports both technical and non-technical users through:
- SDK implementations for developers
- Visual interfaces for configuring experiments
- Analytics dashboards for interpreting results

## API and Implementation Details

### SDK Integration
Amplitude Experiment requires installing the appropriate SDK for client-side or server-side deployments:
- Client-side SDKs: JavaScript, React Native, iOS, Android
- Server-side SDKs: Node.js, Java, Python, Go, Ruby

### Key API Endpoints
- **Evaluation API**: Used for remote evaluation to determine variant assignments
- **Exposure API**: Tracks when users are exposed to experiment variants

### Implementation Methods
- **Remote Evaluation**: `amplitude.experiment.variant('experiment-key')`
- **Local Evaluation**: Performed within the SDK using downloaded flag configurations
- **QA Testing**: Special mode that allows forcing specific variant assignments for testing

The platform ensures consistent user experiences through sticky bucketing while providing flexibility for gradual rollouts and targeted experiments based on user properties and behaviors.