## Product Overview

Amplitude's Source Catalog is a comprehensive data integration ecosystem comprising over 80 connectors that enable organizations to centralize customer behavior analytics by ingesting data from diverse platforms including marketing automation tools, attribution systems, customer engagement platforms, subscription management services, data warehouses, and analytics tools. The catalog serves as the foundational data ingestion layer for Amplitude's behavioral analytics platform, transforming disparate data sources into unified customer journey insights.

## Product Architecture and Feature Relationships

The Source Catalog operates through four primary integration architectures that define data flow patterns:

**Raw Events Integration** forms the core ingestion mechanism, streaming behavioral data directly into Amplitude as structured events while preserving original event structure and mapping critical fields (user_id, device_id, event_properties) to Amplitude's schema.

**Event Streaming Integration** enables real-time bidirectional data synchronization, allowing Amplitude to both receive behavioral events and export cohorts back to source platforms for activation.

**Cohorts Integration** facilitates behavioral segment export to external platforms for targeted marketing campaigns and personalization workflows.

**Attribution Integration** processes marketing attribution data through Amplitude's Attribution API with specific requirements for advertising ID matching (IDFA/IDFV/ADID) and attribution window handling (typically 72 hours).

The catalog's integrations cluster into distinct functional categories that work synergistically:

- **Data Warehouse Integrations** (Snowflake, BigQuery, Databricks, Amazon S3) provide enterprise-grade data import capabilities with sophisticated sync strategies
- **Marketing Attribution Platforms** (Facebook Ads, Google Ads, AppsFlyer, Adjust, Branch) enable comprehensive campaign performance tracking
- **Customer Engagement Tools** (Braze, HubSpot, Intercom, OneSignal) support omnichannel communication analytics
- **Subscription Management Systems** (RevenueCat, Adapty, Stripe) facilitate revenue and monetization tracking
- **Product Experience Platforms** (Chameleon, Userflow, Sprig) capture user onboarding and feedback data

## Key Nomenclature and Definitions

**User ID Mapping**: Critical process of aligning user identifiers across platforms through amplitudeUserId, customerUserId, or email-based matching to maintain consistent user identity.

**Device ID Synchronization**: Ensures consistent device tracking across platforms, particularly crucial for mobile attribution using advertising identifiers (IDFA/IDFV/ADID).

**Event Properties Transformation**: Platform-specific data structure mapping to Amplitude's schema, often requiring JSON transformations and field restructuring.

**Insert ID Deduplication**: Prevents duplicate event ingestion through unique identifier management across data sources.

**Schema Enforcement**: Ensures data consistency and validation across integrated platforms, particularly relevant for warehouse integrations.

**Change Data Capture (CDC)**: Advanced synchronization method used in warehouse integrations (Snowflake, Databricks) for efficient delta sync operations.

**Mirror Sync, Append Only Sync, Timestamp Sync, Full Sync**: Different synchronization strategies for warehouse integrations based on data update patterns and requirements.

**Attribution Window**: Time period (commonly 72 hours) during which attribution platforms can match user actions to marketing touchpoints.

**Cohort Export**: Process of sending Amplitude's behavioral segments to external platforms for targeted activation.

## Broader Product Ecosystem Integration

The Source Catalog integrates seamlessly with Amplitude's comprehensive analytics ecosystem, supporting Portfolio-level configurations that enable multi-product analytics across organizational boundaries. The catalog feeds data into Amplitude's core behavioral analytics capabilities including funnel analysis, retention tracking, advanced segmentation, and cohort creation.

The integrations enable sophisticated cross-platform analysis by combining:
- Quantitative behavioral data from product interactions
- Qualitative feedback from survey platforms (Sprig, Survicate, Qualtrics)
- Attribution insights from marketing platforms
- Revenue metrics from subscription and payment systems
- Engagement data from communication platforms

This unified data foundation supports Customer Acquisition Cost (CAC) and Return on Ad Spend (ROAS) calculations, comprehensive customer journey analysis, and behavioral cohort activation across marketing channels.

## API Endpoints and Technical Implementation

The catalog utilizes several core API endpoints:

- **Amplitude Attribution API**: Processes attribution data ingestion with advertising ID matching
- **HTTP V2 API**: Handles standard event ingestion for real-time data streaming
- **Batch Upload API**: Manages high-volume data imports from warehouse sources
- **User Privacy API**: Supports data compliance and user deletion requirements

Authentication mechanisms vary by integration type:
- API key configuration for standard integrations
- OAuth flows for marketing and engagement platforms
- Service account permissions for cloud warehouse integrations (BigQuery Job User, BigQuery Data Viewer, Storage Admin)
- Personal access tokens for Databricks Unity Catalog access
- Webhook authentication tokens for real-time event streaming

The system supports regional data center configurations (US and EU) with specific endpoint routing for data residency compliance.

Technical implementation includes sophisticated data transformation capabilities:
- VARIANT JSON data type handling in Snowflake
- STRUCT and RECORD type processing in BigQuery
- JSON_EXTRACT_SCALAR transformations for nested data structures
- OBJECT_CONSTRUCT functions for event property mapping
- EPOCH_MILLISECOND extraction for timestamp normalization

File ingestion supports multiple formats (JSON, CSV, Parquet) with organizational requirements including date prefix structure (YYYY/MM/DD/HH) and file size constraints (1MB-1GB) for optimal processing performance.