## Amplitude Source Catalog Product Overview

Amplitude's Source Catalog is a comprehensive data integration ecosystem comprising over 70 pre-built connectors that enable businesses to centralize customer behavior analytics from diverse platforms and tools. The catalog serves as the primary data ingestion layer for Amplitude's analytics platform, supporting integrations across attribution platforms, marketing automation tools, data warehouses, customer engagement platforms, and specialized analytics services.

## Product Architecture and Integration Relationships

The Source Catalog operates through four distinct integration patterns that define how data flows into Amplitude's analytics pipeline:

**Raw Events Integration** forms the foundation of most integrations, streaming event data directly from external platforms while preserving original structure and enriching with Amplitude's standard properties (`user_id`, `device_id`, `event_type`, `event_properties`). This pattern supports platforms like Segment, mParticle, and various marketing automation tools.

**Attribution API Integration** provides specialized mobile attribution capabilities for platforms like AppsFlyer, Adjust, Branch, and Kochava. These integrations require Advertising ID matching (IDFA/IDFV/ADID) and implement a 72-hour attribution window for accurate user matching and campaign attribution.

**Cohorts Integration** enables bidirectional data flow, not only importing behavioral data but also exporting Amplitude-generated user cohorts to external platforms for targeted campaigns and personalization across tools like Braze, Iterable, and Facebook Ads.

**Warehouse Integrations** provide enterprise-grade connections to data warehouses (Snowflake, BigQuery, Databricks) with multiple synchronization strategies including Full Sync, Mirror Sync, Append Only, and Change Data Capture (CDC) for real-time data synchronization.

## Key Nomenclature and Definitions

**Daily Ad Metrics Events**: Standardized event format for advertising platforms containing properties like `ad_metrics.impressions`, `ad_metrics.clicks`, and `ad_metrics.cost` for CAC/ROAS calculations across Facebook Ads, Google Ads, LinkedIn Ads, TikTok Ads, and X Ads.

**Attribution Window**: The 72-hour timeframe used by mobile attribution integrations to match user events with attribution data, critical for accurate campaign performance measurement.

**Mirror Sync**: A warehouse integration strategy that maintains an exact replica of Amplitude data in external warehouses, supporting real-time analytics and custom reporting.

**Change Data Capture (CDC)**: Real-time data synchronization method that captures and streams only changed data records, minimizing bandwidth and processing overhead for warehouse integrations.

**Identity Resolution**: The process of mapping user identities across platforms using various methods including `user_id` matching, email-based mapping, `device_id` synchronization, and custom identifier fields like `amplitudeUserId`.

**Insert ID Deduplication**: Event-level deduplication mechanism that prevents duplicate data ingestion using unique identifiers, essential for data accuracy across high-volume integrations.

## Broader Product Ecosystem Integration

The Source Catalog positions Amplitude as the central analytics hub in modern data stacks, creating a unified customer journey analytics platform. The catalog integrates with three primary ecosystem layers:

**Marketing and Growth Stack**: Connects attribution platforms (AppsFlyer, Adjust, Branch), advertising platforms (Facebook, Google, LinkedIn, TikTok), and marketing automation tools (Braze, Iterable, SendGrid, Mailchimp) to provide comprehensive campaign performance and customer engagement analytics.

**Product and Experience Layer**: Integrates with product experience tools (Chameleon, Userflow, Userguiding, Bento), subscription management platforms (RevenueCat, Adapty, Stripe, Nami), and customer feedback tools to track product adoption, monetization, and user satisfaction metrics.

**Data Infrastructure Layer**: Connects with Customer Data Platforms (Segment, mParticle, Tealium, RudderStack) and enterprise data warehouses (Snowflake, BigQuery, Databricks) to support both data ingestion and export workflows, enabling advanced analytics and machine learning use cases.

## Technical Implementation Specifications

**Authentication Methods**: The catalog supports multiple authentication patterns including OAuth for platforms like HubSpot and LinkedIn Ads, API keys for most standard integrations, service accounts for Google Cloud services, and webhook configurations for real-time event streaming from Stripe and SendGrid.

**API Endpoints**: Key integration endpoints include:
- `api.amplitude.com/sendgrid` for SendGrid webhook integration
- `api.amplitude.com/iterable` for Iterable campaign data
- Attribution API endpoints for mobile attribution platforms
- HTTP V2 API for warehouse and server-side integrations

**Data Transformation Capabilities**: Warehouse integrations support SQL transformations for complex data types including STRUCT columns in BigQuery, VARIANT types in Snowflake, and JSON property extraction using functions like `JSON_EXTRACT_SCALAR`.

**Event Naming Conventions**: Platform-specific event prefixes (e.g., "[Intercom]", "[Mailchimp]") distinguish source systems while maintaining consistent property naming for cross-platform analysis and unified reporting.

**Enterprise Warehouse Features**: 
- **Snowflake**: Change Data Capture (CDC), key pair authentication, specific requirements for `DATA_RETENTION_TIME_IN_DAYS` and `ABORT_DETACHED_QUERY` settings
- **BigQuery**: GCS bucket configuration, service account permissions (BigQuery Job User, Storage Admin), streaming table exports with catch-up importing
- **Databricks**: Change Data Feed (CDF) for delta sync operations, all-purpose compute cluster requirements, personal access tokens and service principal authentication

The Source Catalog supports various data formats including JSON, CSV, and Parquet files, with specific requirements for chronological ordering, insert_id deduplication, and event volume management to ensure data quality and system performance across the entire analytics pipeline.