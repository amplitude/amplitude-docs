## Amplitude Source Catalog

Amplitude's Source Catalog is a comprehensive data integration ecosystem comprising over 60 source connectors that enable organizations to centralize behavioral analytics by ingesting data from diverse platforms across marketing, attribution, customer data, warehousing, and analytics tools. The catalog serves as the foundational data ingestion layer for Amplitude's behavioral analytics platform, supporting multiple integration patterns and data flow architectures.

## Integration Architecture and Data Flow Relationships

The source catalog operates through four primary integration patterns that define how data flows between external systems and Amplitude:

**Raw Events Integration** forms the core data ingestion pattern, streaming granular behavioral event data directly into Amplitude's analytics engine. This pattern preserves timestamped user interactions with associated properties and is utilized by platforms like Adobe Analytics, Braze, HubSpot, and mobile attribution networks.

**Event Streaming Integration** enables bidirectional real-time data synchronization, primarily used by marketing automation platforms (Iterable, MoEngage) and customer engagement tools to maintain synchronized user actions and campaign responses across systems.

**Cohorts Integration** supports behavioral audience sharing with bidirectional capabilities, allowing Amplitude's user segments to be exported while simultaneously importing external segmentation data from platforms like Braze, OneSignal, and Segment.

**Attribution Integration** provides specialized mobile marketing attribution through Amplitude's Attribution API, supporting platforms like Adjust, AppsFlyer, Branch, and Singular with 72-hour retention windows and advertising ID matching capabilities.

## Key Nomenclature and Definitions

**Mobile Measurement Platforms (MMPs)**: Attribution platforms that track mobile app install attribution and post-install events, including AppsFlyer, Adjust, Kochava, and Singular.

**Self-Reporting Networks (SRNs)**: Advertising networks that report their own attribution data rather than using third-party measurement, requiring special handling in attribution integrations.

**Change Data Capture (CDC)**: Database replication method used in warehouse integrations to capture and sync only changed records, supported in Mirror Sync configurations.

**Advertising ID Matching**: Identity resolution using platform-specific identifiers including IDFA (iOS), IDFV (iOS Vendor), and ADID (Android) for cross-platform user tracking.

**Reverse ETL**: Data movement pattern where warehouse data is synced back to operational systems, supported through Census and Hightouch integrations.

**Daily Ad Metrics Events**: Standardized event format for importing campaign-level advertising data including spend, impressions, and click metrics from platforms like Google Ads and Facebook Ads.

**UTM Parameter Mapping**: Standardized campaign tracking parameter extraction and mapping for attribution analysis across marketing channels.

**User Identity Resolution**: Cross-platform user identification using various methods including user_id matching, email-based mapping, device_id synchronization, and custom identifiers like amplitudeDeviceId and customerUserId.

## Product Ecosystem Integration

The Source Catalog integrates with Amplitude's broader product ecosystem through several key touchpoints:

**Amplitude Attribution API** serves as the specialized endpoint for mobile attribution data ingestion, handling attribution events with specific retention and identity matching requirements.

**Amplitude Analytics Platform** receives all raw events and streaming data, where behavioral analysis, funnel analysis, and user journey mapping occur using the ingested source data.

**Amplitude Audiences** (Cohorts) enables bidirectional audience sharing, where behavioral segments created in Amplitude can be activated in destination platforms while importing external segmentation data.

**Amplitude Experiment** integrates with experimentation platforms like Split, Optimizely, and Taplytics to import feature flag impressions and A/B test variation data for comprehensive experiment analysis.

The catalog supports modern data stack architectures where cloud data warehouses (Snowflake, BigQuery, Databricks) serve as the single source of truth, with Amplitude acting as the specialized behavioral analytics layer that both receives data from and contributes insights back to the warehouse ecosystem.

## API Endpoints and Technical Implementation

**Amplitude Attribution API**: Specialized endpoint for mobile attribution data with 72-hour attribution windows and advertising ID matching capabilities.

**Webhook-based Event Streaming**: Real-time event ingestion through webhook endpoints supporting platforms like Stripe (payment_intent.succeeded, charge.succeeded, invoice.payment_succeeded), SendGrid, and Mailchimp.

**OAuth Authentication Flows**: Supported for platforms like HubSpot, Intercom, and Google services, enabling secure, user-authorized data access.

**Service Account Configurations**: Enterprise-grade authentication for Google Cloud services, Snowflake, and other warehouse integrations using JWT tokens and service account credentials.

**Regional Compliance Endpoints**: EU-specific data residency endpoints for GDPR compliance across multiple integrations.

**SQL-based Data Transformations**: Warehouse integrations utilize platform-specific SQL functions including `OBJECT_CONSTRUCT` for Snowflake and `JSON_EXTRACT_SCALAR` for BigQuery for data transformation during ingestion.

**Sync Strategy Configurations**: Multiple data synchronization patterns including Full Sync, Timestamp-based imports, Append Only Sync, and Mirror Sync with CDC functionality for warehouse integrations.

The Source Catalog represents a mature, enterprise-ready integration ecosystem that enables organizations to create a unified behavioral analytics foundation by connecting diverse data sources through standardized, scalable, and reliable integration patterns, supporting both real-time streaming and batch processing use cases across the modern data stack.