## Amplitude Source Catalog

The Amplitude Source Catalog is an enterprise data integration platform that enables organizations to synchronize data between cloud data warehouses and Amplitude's analytics platform. The catalog provides comprehensive data warehouse connectors for major platforms including Databricks and Snowflake, supporting bidirectional data flow for events, user properties, group properties, and user profiles.

## Product Architecture and Feature Relationships

The source catalog operates as a unified integration layer with standardized capabilities across supported data warehouse platforms:

**Core Data Import Framework**: All integrations share a common architecture for importing four primary data types - events for behavioral tracking, user properties for individual user attributes, group properties for organizational or cohort-level data, and complete user profiles for comprehensive user management.

**Synchronization Engine**: The platform implements multiple sync strategies to accommodate different data update patterns and business requirements. Full Sync provides complete data refreshes, Timestamp-based syncing enables incremental updates using temporal markers, Append Only Sync supports write-only scenarios for immutable data, and Mirror Sync maintains exact replicas with Change Data Capture capabilities for real-time synchronization.

**Authentication and Security Layer**: Enterprise-grade security is implemented through multiple authentication methods including personal access tokens (PAT), service principal authentication for Databricks environments, and key pair authentication for Snowflake, ensuring secure data access within organizational security frameworks.

**Data Transformation Pipeline**: Built-in SQL query mapping capabilities allow custom data transformation during the import process, enabling organizations to reshape and enrich data as it flows from warehouse to analytics platform.

## Key Nomenclature and Technical Definitions

**Change Data Feed (CDF)**: Databricks-specific table-level change tracking mechanism that captures data modifications at the row level, enabling efficient delta synchronization by processing only changed records rather than performing full table scans.

**Change Data Capture (CDC)**: Snowflake's systematic approach to identifying and capturing data modifications in real-time, supporting continuous synchronization between warehouse and analytics platforms through automated change detection.

**Mirror Sync vs Append Only Sync**: Mirror Sync maintains bidirectional data consistency with support for updates, deletions, and insertions, creating an exact replica of source data. Append Only Sync operates in a unidirectional mode, exclusively adding new records without modifying existing data, optimized for immutable data scenarios.

**Delta Sync**: Incremental synchronization methodology that identifies and transfers only data changes since the last synchronization checkpoint, optimizing network bandwidth and processing resources while maintaining data consistency.

**Unity Catalog**: Databricks' centralized governance framework requiring specific Data Reader permissions and cluster access mode configurations for secure table access and metadata management.

**Insert ID**: Unique event identifier used for deduplication logic, preventing duplicate event ingestion during synchronization processes and ensuring data integrity across multiple sync operations.

**EPOCH_MILLISECOND**: Standardized timestamp format used across integrations for precise event timing, ensuring consistent temporal data representation regardless of source warehouse timestamp formats.

## Enterprise Ecosystem Integration

The source catalog functions as a critical component within Amplitude's broader enterprise analytics ecosystem, providing upstream data ingestion capabilities that connect to downstream analytics and compliance services:

**Privacy and Compliance Integration**: Deep integration with Amplitude's User Privacy API ensures that user deletion requests and privacy controls are respected across all synchronized data, maintaining compliance with GDPR, CCPA, and other data protection regulations.

**Regional Data Governance**: Support for regional IP allowlists and geographic data residency requirements enables global organizations to maintain data sovereignty while leveraging centralized analytics capabilities.

**Analytics Pipeline Integration**: Synchronized data flows directly into Amplitude's behavioral analytics engine, supporting advanced user journey analysis, cohort segmentation, and predictive analytics capabilities.

**Enrichment Services Control**: Configurable enrichment settings allow organizations to disable automatic data enhancement when raw data fidelity is required, providing granular control over data processing pipelines.

## Technical Implementation Specifications

**Databricks Integration Technical Requirements**:
- All-purpose compute clusters (excludes serverless and job clusters)
- JDBC connection protocol for data access
- SQL-based transformation capabilities during import
- Table versioning support for change tracking
- Configurable sync frequency based on event volume requirements
- Unity Catalog compatibility with Data Reader permissions

**Snowflake Integration Technical Specifications**:
- VARIANT JSON data type support for flexible schema handling
- OBJECT_CONSTRUCT function implementation for complex data mapping
- TIMESTAMP_NTZ format support for timezone-neutral temporal data
- ABORT_DETACHED_QUERY configuration for connection management
- DATA_RETENTION_TIME_IN_DAYS enforcement for historical data access
- 12-hour query timeout with 1 billion events batch processing limit
- Storage integration with external stages for large-scale data transfers

**Universal Technical Features**:
- SQL query mapping for custom data transformation during sync
- Regional IP allowlist configuration for network security compliance
- Event volume impact analysis for performance optimization
- Configurable authentication methods supporting enterprise security policies
- Batch processing capabilities with intelligent retry mechanisms
- Real-time sync monitoring and error handling

The Amplitude Source Catalog represents a comprehensive enterprise data integration solution, designed to bridge the gap between organizational data warehouses and advanced analytics platforms while maintaining the security, compliance, and performance standards required by large-scale enterprise deployments.