Amplitude's Source Catalog is a comprehensive data integration ecosystem that enables organizations to centralize behavioral analytics by ingesting data from over 60 diverse platforms and tools. The catalog spans marketing automation, attribution tracking, customer engagement, subscription management, data warehouses, and experimentation platforms, creating a unified view of customer behavior across all touchpoints.

## Integration Architecture and Data Flow Patterns

The source catalog operates on four primary integration types that define data flow into Amplitude's analytics engine:

**Raw-Events Integration** represents the most common pattern, directly ingesting event data from external platforms. This includes subscription lifecycle events from RevenueCat and Adapty, survey responses from Sprig and Survicate, and user behavior data from platforms like Convizit and Tribe.

**Event-Streaming Integration** enables real-time bidirectional data flow, allowing platforms like Braze, Iterable, and OneSignal to both send behavioral events to Amplitude and receive behavioral cohorts for targeted campaign activation.

**Cohorts Integration** facilitates audience segmentation by sharing Amplitude's behavioral cohorts with marketing platforms like Mailchimp, Leanplum, and Insider for personalized customer experiences and campaign targeting.

**Attribution Integration** leverages Amplitude's Attribution API to process mobile attribution data from platforms like Adjust, AppsFlyer, Branch, and Kochava, typically operating with 72-hour attribution windows and advertising ID (ADID/IDFA) matching requirements.

## Data Warehouse and ETL Ecosystem

The catalog includes sophisticated warehouse integrations supporting multiple import strategies and data synchronization methods:

**Snowflake Integration** offers four distinct sync methods: Full Sync for complete data replacement, Timestamp-based incremental updates, Append Only for immutable event streams, and Mirror Sync for real-time replication. The integration includes Change Data Capture (CDC) functionality for efficient delta processing.

**BigQuery Integration** handles Google Analytics 4 (GA4) data imports with specific considerations for 72-hour daily export delays and STRUCT data type transformations. The integration supports complex nested data structures common in GA4 event schemas.

**Databricks Integration** utilizes Change Data Feed (CDF) for delta sync operations, requiring all-purpose compute clusters and supporting Unity Catalog configurations for enterprise data governance.

Cloud storage integrations for **Amazon S3** and **Google Cloud Storage** support both Mirror Sync and Append Only strategies with converter configurations, IAM role setup, and chronological file ordering requirements. These integrations handle JSON, CSV, and Parquet formats with file size requirements between 1MB-1GB for optimal processing.

## Marketing Attribution and Advertising Ecosystem

The catalog extensively covers marketing attribution through mobile measurement platforms (MMPs) including AppsFlyer, Adjust, and Kochava, all utilizing Amplitude's Attribution API with ADID/IDFA matching and standardized 72-hour attribution windows.

**Facebook Ads** and **Google Ads** integrations import campaign-level metrics as "Daily Ad Metrics" events with structured properties like `ad_metrics.impressions`, `ad_metrics.clicks`, and UTM parameter mapping for customer acquisition cost (CAC) and return on ad spend (ROAS) calculations.

Email marketing platforms including **HubSpot**, **SendGrid**, **Mailchimp**, and **Braze** stream engagement events (opens, clicks, deliveries, bounces) while supporting bidirectional cohort sharing for behavioral targeting and campaign personalization.

## Customer Engagement and Product Experience

User onboarding and product experience tools form a significant integration category, with platforms like **Chameleon**, **Userflow**, **Userguiding**, and **Bento** streaming product tour events, step completion tracking, and guide engagement metrics. These integrations typically require user_id matching and support both raw-events ingestion and cohorts export functionality.

Survey and feedback platforms including **Qualtrics**, **Sprig**, and **Survicate** enable qualitative feedback analysis by streaming survey responses with automatic email-based user ID mapping and support for CSAT, NPS, and custom survey data structures.

## Subscription and Revenue Analytics

Subscription management platforms including **RevenueCat**, **Adapty**, **Apphud**, and **Nami** stream in-app purchase events and subscription lifecycle data, enabling behavioral cohort analysis of subscription customers and revenue attribution modeling.

**Stripe Integration** imports revenue events via webhooks with user ID mapping priority logic and currency normalization for global revenue tracking and customer lifetime value calculations.

## Tag Management and Customer Data Platform Integrations

**Google Tag Manager** offers multiple implementation templates: a client-side template using Browser SDK 2.0 with autocapture capabilities, a server-side template using HTTP V2 API for enhanced data privacy, and legacy template support with migration guidance.

**Segment Integration** provides bidirectional data flow with both Actions (modern) and Classic destinations, supporting client-side bundled integration and server-side sources for comprehensive data orchestration.

Customer Data Platforms including **Tealium**, **mParticle**, **RudderStack**, and **MetaRouter** enable comprehensive data orchestration with Universal Data Object (UDO) configurations and cross-platform SDK support for unified customer profiles.

## Key Technical Specifications and Implementation Patterns

**Authentication Methods** vary by platform: OAuth 2.0 for platforms like Intercom and Mailchimp, API key authentication for most integrations, service account permissions for warehouse integrations, and webhook configurations for real-time data streaming.

**User ID Mapping** represents a critical requirement across integrations, with platforms supporting various mapping strategies including email fallback (Intercom), amplitudeDeviceId mapping (Adapty), and unique_arg parameters (SendGrid) for cross-platform user identification.

**Data Delivery Intervals** range from real-time streaming (attribution platforms) to 30-minute intervals (Convizit) to hourly syncing (HubSpot), with some platforms offering configurable sync cadences based on data volume and latency requirements.

**Regional Data Residency** includes EU data residency support for platforms like Google Tag Manager and SendGrid, with specific endpoint configurations (api.eu.amplitude.com) for European data centers and GDPR compliance.

## API Endpoints and SDK Integration Patterns

The catalog leverages several key API endpoints:
- **Attribution API**: Used by mobile attribution platforms for real-time attribution data ingestion
- **HTTP V2 API**: Standard endpoint for event ingestion and server-side implementations
- **Lookup Table API**: For CSV data import and property enrichment workflows
- **User Privacy API**: Integrated with warehouse sources for data compliance and user deletion requests

SDK integrations include JavaScript SDK, Browser SDK 2.0, React Native SDK, and platform-specific mobile SDKs, with implementation patterns like `amplitude.getInstance()` method calls and `window.amplitude` browser integrations for client-side tracking.

## Product Ecosystem Integration

Within Amplitude's broader product ecosystem, the Source Catalog serves as the foundational data ingestion layer that feeds into Amplitude Analytics for behavioral analysis, Amplitude Experiment for feature flagging and experimentation, and Amplitude Audiences for customer segmentation and activation. The bidirectional nature of many integrations enables closed-loop marketing where behavioral insights drive campaign targeting and optimization across the entire customer journey.

This comprehensive integration ecosystem enables organizations to implement a composable customer data architecture while maintaining flexibility in data flow patterns and supporting both technical and non-technical implementation approaches across diverse technology stacks.