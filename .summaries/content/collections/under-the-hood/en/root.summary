# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test new features and measure their impact on user behavior. The platform provides robust capabilities for randomized controlled trials, with sophisticated mechanisms for user assignment, event tracking, and statistical analysis.

Key features include:
- Deterministic user randomization for consistent experiment assignment
- Sequential statistical testing for faster, more flexible experiment analysis
- Local and remote evaluation options for flag and experiment delivery
- Flag dependencies for complex experiment scenarios
- Comprehensive event tracking integration with Amplitude Analytics
- Performance optimization through CDN caching and client-side strategies

Primary use cases include A/B testing, feature rollouts, mutual exclusion testing, and holdout experiments to measure the combined impact of multiple features.

## Product and Feature Relationships

Amplitude Experiment consists of several interconnected components:

1. **Evaluation System**: Offers two modes:
   - **Remote Evaluation**: Server-side evaluation with CDN caching (35.9ms average for cache hits, 194.21ms for misses)
   - **Local Evaluation**: Client-side evaluation with sub-millisecond performance across Node.js, Go, and JVM SDKs

2. **Randomization Engine**: Uses a two-stage deterministic process:
   - First determines if a user is included in an experiment
   - Then assigns variants to included users based on variant weights

3. **Event Tracking System**: Captures two primary event types:
   - **Assignment Events**: Record when a user is assigned to an experiment variant
   - **Exposure Events**: Record when a user actually sees the experiment

4. **Analysis System**: Calculates experiment results using:
   - Sequential testing methodology (mSPRT)
   - Various metric calculations (unique conversions, event totals, property values, funnels)

5. **Flag Dependencies**: Manages relationships between flags:
   - **Prerequisites**: Control evaluation order for complex feature releases
   - **Mutual Exclusion Groups**: Ensure users only participate in one experiment at a time
   - **Holdout Groups**: Withhold traffic to measure combined experiment impact

6. **Performance Infrastructure**: Built on:
   - Fastly CDN for caching
   - AWS services (Application Load Balancer, Relational Databases, DynamoDB)
   - Client SDK caching mechanisms

## Key Nomenclature and Definitions

- **Flag**: A feature toggle that can be turned on/off or used for experimentation
- **Variant**: A specific version or treatment in an experiment
- **Bucketing Key**: The unique identifier used to assign users to variants (typically Amplitude ID)
- **Bucketing Salt**: A string used to ensure randomization is unique per experiment
- **Assignment Event**: `[Experiment] Assignment` event recording when a user is assigned to a variant
- **Exposure Event**: `[Experiment] Exposure` event recording when a user actually sees an experiment
- **mSPRT**: Mixture Sequential Probability Ratio Test, the statistical method used for experiment analysis
- **Flag Dependencies**: Relationships between flags that control evaluation order
- **Cache Hit/Miss**: Whether a flag evaluation request is served from cache or requires full evaluation

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude product ecosystem:

1. **Amplitude Analytics**: 
   - Experiment automatically sends assignment and exposure events to Analytics
   - These events set user properties (`[Experiment] flag_key = variant`) for segmentation
   - Analytics data powers experiment analysis and behavioral cohorts

2. **Identity Resolution System**:
   - Uses Amplitude's identity resolution to maintain consistent user identification
   - Enables accurate experiment assignment across devices and sessions

3. **Behavioral Cohorts**:
   - Leverages Amplitude's behavioral cohorts for targeted experiment deployment
   - Allows dynamic targeting based on user behavior patterns

## API and Implementation Details

### Event Tracking APIs

- Assignment events are automatically tracked with properties:
  - `flag_key`: Identifier for the experiment
  - `variant`: The assigned variant
  - `deployment`: Deployment environment
  - `inserted_id`: Unique identifier for the event
  - `timestamp`: When the assignment occurred

- Exposure events include the same properties plus:
  - `exposure_id`: Unique identifier for the exposure
  - `exposure_time`: When the exposure occurred

### Evaluation APIs

- Remote Evaluation:
  ```
  GET /sdk/v2/vardata
  ```
  Returns flag evaluation results for a user

- Local Evaluation SDKs:
  - Node.js: `amplitude.evaluate(flagKey, user, options)`
  - Go: `client.Evaluate(flagKey, user, options)`
  - JVM: `amplitude.evaluate(flagKey, user, options)`

### Implementation Best Practices

- Use local defaults to handle evaluation failures
- Implement client-side caching to reduce evaluation requests
- Set appropriate cache TTLs based on flag update frequency
- Consider regional performance implications for global deployments
- Use flag dependencies to create complex experiment hierarchies

The platform architecture is designed to scale efficiently, with performance optimizations at every level from CDN caching to client-side implementation strategies, ensuring reliable experimentation capabilities even at high traffic volumes.