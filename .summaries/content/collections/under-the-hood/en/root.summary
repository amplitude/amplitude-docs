Amplitude Experiment is a feature flagging and A/B testing platform that provides both remote and local evaluation capabilities for running controlled experiments and managing feature rollouts. The platform is built on a high-performance architecture utilizing Fastly CDN and AWS services including Application Load Balancer and DynamoDB for experiment storage.

## Core Features and Capabilities

The platform supports comprehensive experimentation through deterministic randomization using murmur3 hash algorithms, advanced statistical analysis via mixture sequential probability ratio test (mSPRT), and sophisticated flag dependency management. Key evaluation modes include remote evaluation with CDN caching and local evaluation with pre-fetched configurations, both optimized for different performance requirements across geographic regions.

## Product Relationships and Architecture

Amplitude Experiment integrates tightly with Amplitude Analytics through automatic event tracking systems. The platform generates assignment events (`[Experiment] Assignment`) and exposure events (`$exposure`/`[Experiment] Exposure`) that feed into experiment analysis workflows. The system maintains user properties in the format `[Experiment] <flag_key>` and supports both server-side and client-side experiment implementations.

The performance architecture leverages Fastly CDN for global distribution with 60-minute TTL caching, AWS Application Load Balancer for traffic routing, and DynamoDB for experiment configuration storage. An identity resolution system manages user metadata and behavioral cohorts, which are computed hourly for dynamic targeting capabilities.

## Key Nomenclature and Definitions

**Evaluation Methods:**
- Remote evaluation: Real-time flag evaluation via `fetch()` API calls with CDN caching
- Local evaluation: Client-side evaluation using `evaluate()` with pre-fetched configurations

**Randomization Components:**
- Bucketing salt: Unique identifier combined with amplitude_id for deterministic assignment
- Two-stage bucketing: First determines experiment inclusion via mod 100 against percentage rollout, then assigns variants through floor division with weighted ranges 0-42949672

**Event Types:**
- Assignment events: Track when users are assigned to experiment variants
- Exposure events: Record actual user exposure to experimental treatments
- Monthly Tracked Users (MTU): Billing metric based on event volume

**Statistical Framework:**
- mSPRT (mixture sequential probability ratio test): Enables early experiment termination without predetermined sample sizes
- Sequential testing: Allows valid statistical inference at any viewing time, contrasting with traditional T-tests

**Flag Dependencies:**
- Prerequisites: Flags that must evaluate before dependent flags
- Mutual exclusion groups: Ensure users participate in only one experiment within a group
- Holdout groups: Reserve user segments for control analysis

## Ecosystem Integration

Amplitude Experiment operates within the broader Amplitude ecosystem by automatically integrating with Amplitude Analytics for event tracking and analysis. The platform supports multiple SDK implementations across Node.js, Ruby, JVM, Go, Python, and PHP environments, each with specific version requirements for local evaluation support.

The system processes experiment data through chronological event analysis, calculating metrics including unique conversions, event totals, property value aggregations, and funnel conversions using formulas based on exposed users (E), metric events (M), and totals (T).

## API Endpoints and Implementation

Key API methods include:
- `fetch()`: Remote evaluation with automatic assignment tracking
- `evaluate()`: Local evaluation for pre-fetched configurations  
- `variant()`: Retrieves assigned variant for specific flags

The platform supports client-side caching strategies, local defaults implementation for performance optimization, and exposure tracking providers for SDK integrations. Cache invalidation occurs automatically on deployment updates, with considerations for dynamic targeting based on user properties and behavioral cohorts that may experience cache staleness.

Performance benchmarks show geographic latency variations across regions like us-west-2 and eu-central-1, with cache hit/miss ratios affecting overall system performance. The architecture guarantees high availability while maintaining deterministic user experiences through consistent randomization algorithms.