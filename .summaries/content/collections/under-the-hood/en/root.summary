# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test new features and measure their impact. The platform supports both remote and local evaluation modes, providing flexible implementation options for different use cases. Key features include deterministic randomization, sequential statistical testing, flag dependencies, and comprehensive event tracking for experiment analysis.

The platform is designed for high performance and reliability, utilizing AWS infrastructure and Fastly CDN to deliver fast evaluation responses globally. It integrates deeply with Amplitude Analytics, allowing teams to leverage user behavioral data for targeting and analysis.

## Product Relationships and Architecture

### Evaluation Modes
1. **Remote Evaluation**: Server-based evaluation where flag rules are processed on Amplitude's servers
   - Utilizes Fastly CDN for caching responses (35.9ms average for cache hits vs. 194.21ms for misses)
   - Supports dynamic targeting and behavioral cohorts
   - Recommended for client-side implementations

2. **Local Evaluation**: Client-side evaluation where flag rules are processed locally
   - Sub-millisecond performance across Node.js, Go, and JVM SDKs
   - Requires downloading flag configurations
   - Ideal for server-side implementations where latency is critical

### Event Tracking System
The platform uses two primary event types to enable experiment analysis:
1. **Assignment Events**: Record when a user is assigned to an experiment variant
   - Can be tracked automatically or manually depending on implementation
   - Set experiment user properties for analysis

2. **Exposure Events**: Record when a user is actually exposed to an experiment
   - Critical for accurate analysis (distinguishes assignment from actual exposure)
   - Can be transformed from assignment events in certain implementations

### Flag Dependencies
Three key dependency implementations:
1. **Flag Prerequisites**: Define conditions where one flag depends on another flag's state
2. **Mutual Exclusion Groups**: Ensure users are only included in one experiment at a time
3. **Holdout Groups**: Withhold traffic from multiple experiments to measure combined impact

### Statistical Analysis
Amplitude uses sequential testing (mSPRT - mixture Sequential Probability Ratio Test) for statistical inference, which:
- Allows valid statistical conclusions at any point during an experiment
- Enables faster experimentation and early termination
- Doesn't require predetermined sample sizes unlike traditional T-tests

## Key Nomenclature and Definitions

- **Bucketing Key**: The unique identifier used to assign users to variants (typically Amplitude ID)
- **Bucketing Salt**: A string used to ensure different experiments produce different assignments
- **Deterministic Randomization**: Process ensuring users consistently receive the same variant
- **Exposure Transformation**: Converting assignment events to exposure events automatically
- **Flag Dependencies**: Relationships between flags that control evaluation order
- **Local Defaults**: Fallback values used when flag evaluation fails
- **mSPRT**: Mixture Sequential Probability Ratio Test, the statistical method used for experiment analysis
- **Mutual Exclusion Groups**: Configuration ensuring users only participate in one experiment at a time
- **Remote Evaluation**: Server-side processing of flag rules
- **Local Evaluation**: Client-side processing of flag rules
- **Variant**: A specific version or treatment in an experiment

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude product ecosystem:

1. **Amplitude Analytics**: 
   - Provides user behavioral data for experiment targeting
   - Receives experiment assignment and exposure events
   - Powers experiment analysis dashboards

2. **Amplitude SDKs**:
   - Available for multiple platforms (Node.js, Go, JVM, client-side JavaScript)
   - Support both remote and local evaluation modes
   - Handle event tracking and flag evaluation

3. **Amplitude Cohorts**:
   - Used for targeting experiments to specific user segments
   - Accessible in real-time for remote evaluation

## Technical Implementation Details

### Randomization Algorithm
```
// Inclusion determination
mod(murmur3_x86_32("bucketingSalt/id"), 100) < inclusionPercent

// Variant assignment
variantIndex = floor(murmur3_x86_32("bucketingSalt/id") / MAX_HASH * 100)
```

### Performance Metrics
- Remote evaluation with cache hit: ~35.9ms globally
- Remote evaluation with cache miss: ~194.21ms globally
- Local evaluation: sub-millisecond across all SDKs

### Experiment Analysis Calculations
For different metric types:
- **Unique Conversions**: (Users who triggered metric event) รท (Users exposed to variant)
- **Event Totals**: (Total metric events) รท (Users exposed to variant)
- **Property Values**: (Sum of property values) รท (Users who triggered metric event)
- **Funnel Conversions**: (Users who completed funnel) รท (Users who started funnel)

### Caching Mechanisms
- CDN caching with configurable TTL (time-to-live)
- Cache invalidation on flag updates
- Client-side caching for improved performance
- Local storage options for offline capabilities