# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test new features and measure their impact. The platform supports both remote and local evaluation modes, providing flexible implementation options for different use cases. Key features include deterministic randomization, sequential statistical testing, flag dependencies, and comprehensive event tracking for experiment analysis.

The platform is designed for performance and scalability, leveraging AWS services and Fastly CDN to deliver fast evaluation responses globally. Amplitude Experiment integrates with the broader Amplitude analytics ecosystem, allowing teams to use behavioral data for targeting and to analyze experiment results with sophisticated statistical methods.

## Product Relationships and Architecture

Amplitude Experiment consists of several interconnected components:

1. **Evaluation Modes**:
   - **Remote Evaluation**: Server-side evaluation where flag rules are processed on Amplitude's servers
   - **Local Evaluation**: Client-side evaluation where flag rules are downloaded and processed locally

2. **Event Tracking System**:
   - **Assignment Events**: Record when users are assigned to experiment variants
   - **Exposure Events**: Record when users are actually exposed to experiment treatments

3. **Analysis Components**:
   - **Experiment Analysis Charts**: Visualize experiment results using various metrics
   - **Sequential Testing Engine**: Provides statistical inference without predetermined sample sizes

4. **Infrastructure**:
   - **Fastly CDN**: Caches evaluation results for improved performance
   - **AWS Services**: Including Application Load Balancer, Relational Databases, and DynamoDB
   - **Identity Resolution System**: Manages user identification across touchpoints

5. **Flag Management**:
   - **Flag Dependencies**: Control relationships between flags (prerequisites, mutual exclusion, holdouts)
   - **Targeting Rules**: Determine which users see which variants

## Key Nomenclature and Definitions

- **Bucketing Key**: The unique identifier used to determine variant assignment, typically the Amplitude ID
- **Bucketing Salt**: A string used to ensure randomization is unique to each experiment
- **Deterministic Randomization**: A two-stage process using murmur3 hash functions to consistently assign users to variants
- **Sequential Testing (mSPRT)**: Statistical method that allows valid inference at any point during an experiment
- **Flag Dependencies**: Relationships between flags that control evaluation order:
  - **Prerequisites**: Flags that must be evaluated before dependent flags
  - **Mutual Exclusion Groups**: Ensure users are only included in one experiment at a time
  - **Holdout Groups**: Withhold traffic to measure combined experiment impact
- **Cache Hit/Miss**: Whether an evaluation request is served from cache (hit) or requires full processing (miss)
- **Exposure Transformation**: Process of converting assignment events to exposure events
- **Event Property Inheritance**: How experiment metadata is attached to tracked events

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude ecosystem:

1. **Amplitude Analytics**: Experiment results are analyzed using Amplitude's analytics capabilities, with experiment user properties attached to events for segmentation and analysis.

2. **Behavioral Cohorts**: Experiments can target users based on behavioral cohorts defined in Amplitude Analytics.

3. **Identity Resolution**: Leverages Amplitude's identity resolution system to maintain consistent user identification.

4. **SDK Integrations**: Various SDKs (Node.js, Go, JVM, client-side) integrate with the platform for different implementation scenarios.

## API and Implementation Details

### Evaluation Endpoints

- Remote evaluation is performed via API endpoints with responses cached in Fastly CDN
- Performance metrics:
  - Remote evaluation: 35.9ms average for cache hits, 194.21ms for cache misses globally
  - Local evaluation: Sub-millisecond performance (Node.js: 0.025ms, Go: 0.009ms, JVM: 0.012ms)

### Randomization Implementation

```
// Inclusion determination
mod(murmur3_x86_32("bucketingSalt/id"), 100) < rolloutPercentage

// Variant assignment
variantIndex = floor(murmur3_x86_32("bucketingSalt/id") / MAX_HASH * 100)
```

### Event Tracking Implementation

- Assignment events: `[Experiment] Assignment` (automatically tracked by SDKs)
- Exposure events: `[Experiment] Exposure` (must be explicitly tracked in some cases)
- User properties set on events:
  - `[Experiment] <experiment-key>` (variant name)
  - `[Experiment] <experiment-key> Variant` (variant value)

### Analysis Calculation Formulas

- Unique conversions: `(users with metric event AND exposure) / (users with exposure)`
- Event totals: `(sum of events from exposed users) / (count of exposed users)`
- Property value sum: `(sum of property values from exposed users) / (count of exposed users with property)`
- Funnel conversion: `(users completing funnel AND exposed) / (users starting funnel AND exposed)`