Amplitude Experiment is a comprehensive A/B testing and feature flagging platform that provides sophisticated experimentation capabilities with advanced statistical methods, high-performance infrastructure, and detailed analytics. The platform enables organizations to run controlled experiments, manage feature flags, and analyze results using cutting-edge statistical techniques.

## Core Features and Capabilities

The platform operates through two primary evaluation modes: **remote evaluation** for server-side experiments and **local evaluation** for client-side implementations. Amplitude Experiment employs deterministic randomization using the murmur3 hash algorithm, ensuring consistent user assignment across sessions while supporting complex experimental designs including flag dependencies, mutual exclusion groups, and holdout groups.

A key differentiator is the implementation of **mixture sequential probability ratio test (mSPRT)** for statistical inference, allowing experiments to be terminated early without predetermined sample sizes while maintaining statistical validity. This contrasts with traditional T-test approaches and enables more efficient experimentation workflows.

## Product Architecture and Infrastructure

The platform is built on a robust, globally distributed infrastructure leveraging **Fastly CDN** for edge caching and **AWS services** including Application Load Balancer (ALB), RDS, and DynamoDB. This architecture supports identity resolution, user metadata storage, and behavioral cohorts processing with high availability across multiple geographic regions (us-west-2, eu-central-1).

Performance optimization is achieved through multiple layers including client-side variant caching, local defaults implementation, and CDN caching with 60-minute TTL. The system automatically handles cache invalidation upon deployment changes and provides sub-100ms latency for cached requests globally.

## Key Nomenclature and Definitions

**Assignment Events**: Automatically triggered by `fetch()` or `evaluate()` calls, tracking when users are assigned to experiment variants
**Exposure Events**: Triggered by `variant()` method access, indicating actual feature interaction
**Bucketing Salt**: Used in conjunction with amplitude_id for deterministic hash-based user assignment
**Flag Dependencies**: Define evaluation order relationships enabling prerequisites, mutual exclusion, and holdout configurations
**Experiment User Properties**: Stored in format `[Experiment] <flag_key>` for analysis and segmentation
**mSPRT**: Mixture sequential probability ratio test enabling valid statistical inference at any viewing time

## Randomization and Statistical Methods

User assignment follows a two-stage deterministic process using murmur3_x86_32 hash of "bucketingSalt/amplitude_id". The first stage determines experiment inclusion via mod 100 against percentage rollout, while the second stage assigns variants through floor division with weighted ranges 0-42949672. This ensures consistent, reproducible assignments while supporting complex traffic allocation scenarios.

The sequential testing implementation supports both binary and continuous metrics, leveraging Central Limit Theorem for statistical inference while handling outliers appropriately. This enables early experiment termination with maintained statistical rigor.

## Analytics and Measurement

Experiment Analysis charts provide comprehensive metrics including unique conversions, event totals, property value calculations, and funnel conversions. The calculation engine processes exposure events (E), metric events (M), event totals (T), property sums (S), and property averages (A) to generate meaningful insights.

Funnel analysis tracks chronological event progression with FM (funnel users) and FT (funnel totals) metrics, enabling complex conversion path analysis. All calculations maintain user-level granularity while supporting aggregated reporting.

## API and SDK Integration

The platform provides multiple SDK options including Node.js, Go, and JVM implementations supporting both remote and local evaluation modes. Key methods include:
- `fetch()` and `evaluate()` for variant assignment
- `variant()` for accessing assigned variants and triggering exposure tracking
- Automatic event tracking with configurable exposure tracking providers

Event deduplication is handled through insert_id mechanisms, and the system integrates with Monthly Tracked Users (MTU) billing models while managing event volume efficiently.

## Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem, utilizing behavioral cohorts computed hourly and supporting dynamic targeting based on user properties. The platform handles complex dependency chains between flags and experiments while maintaining performance through intelligent caching strategies.

The system supports both server-side and client-side experiment implementations, with automatic user property assignment for downstream analysis and segmentation within the Amplitude platform.