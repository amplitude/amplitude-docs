## Amplitude Experiment

Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that provides both remote and local evaluation capabilities for running controlled experiments. The platform combines deterministic user assignment, sophisticated statistical analysis, and high-performance infrastructure to enable data-driven product decisions through experimentation.

## Product Architecture and Feature Relationships

The core of Amplitude Experiment operates on a multi-layered architecture that handles user assignment, event tracking, and statistical analysis:

**Evaluation Modes**: The platform supports both remote and local evaluation modes. Remote evaluation provides real-time flag resolution through API calls, while local evaluation enables client-side caching and offline capabilities through SDKs for Node.js, Go, and JVM environments.

**User Assignment System**: At the foundation is a deterministic randomization algorithm using murmur3 hash of "bucketingSalt/amplitude_id" that performs two-stage user assignment - first determining experiment inclusion via mod 100 against percentage rollout, then variant assignment through floor division with weighted ranges 0-42949672.

**Event Tracking Pipeline**: The system automatically generates assignment events (triggered by `fetch()` or `evaluate()` calls) and exposure events (triggered by `variant()` method access), creating experiment user properties in the format `[Experiment] <flag_key>` for downstream analysis.

**Flag Dependencies**: Advanced experiment design is supported through flag dependencies that define evaluation order relationships, enabling flag prerequisites, mutual exclusion groups, and holdout groups with specific variant handling.

## Key Nomenclature and Definitions

**Assignment Events**: Automatically tracked events generated when `fetch()` or `evaluate()` methods are called, indicating a user has been assigned to an experiment variant.

**Exposure Events**: Events triggered when the `variant()` method is accessed, representing actual user exposure to experimental treatments. These use the `$exposure` event type.

**Bucketing Salt**: A unique identifier combined with amplitude_id in the murmur3 hash function to ensure deterministic but randomized user assignment across experiments.

**mSPRT (Mixture Sequential Probability Ratio Test)**: The statistical methodology used for experiment analysis, allowing early experiment termination without predetermined sample sizes while maintaining statistical validity.

**Flag Dependencies**: Relationships between flags that control evaluation order, supporting prerequisites, mutual exclusion, and holdout scenarios.

**Dynamic Targeting**: Real-time user segmentation based on user properties and behavioral cohorts, with considerations for caching behavior.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem:

**Analytics Integration**: Experiment events flow into Amplitude Analytics for comprehensive user journey analysis, with experiment user properties enabling segmentation and cohort analysis.

**Behavioral Cohorts**: The system leverages Amplitude's behavioral cohort computation (updated hourly) for sophisticated user targeting based on historical actions.

**Identity Resolution**: Built-in identity resolution system manages user metadata and cross-device tracking for consistent experiment experiences.

**Monthly Tracked Users (MTU)**: Experiment exposure events contribute to Amplitude's MTU billing model, with exposure tracking providers and insert_id deduplication managing event volume.

## Infrastructure and Performance

The platform runs on a high-availability infrastructure built on:

- **Fastly CDN**: Provides global edge caching with 60-minute TTL for experiment configurations
- **AWS Services**: Application Load Balancer (ALB), RDS for relational data, and DynamoDB for experiment storage
- **Geographic Distribution**: Primary data centers in us-west-2 and eu-central-1 with optimized latency across regions

**Performance Characteristics**:
- Cache hit latencies typically under 100ms globally
- Cache invalidation occurs automatically on deployment changes
- Local evaluation SDKs provide sub-millisecond performance through client-side caching

## API Methods and Technical Implementation

**Core SDK Methods**:
- `fetch()`: Retrieves experiment assignments and triggers assignment events
- `evaluate()`: Performs local evaluation and triggers assignment events  
- `variant()`: Accesses assigned variants and triggers exposure events

**Analysis Calculations**: The platform uses specific formulas for experiment analysis charts:
- Unique conversions: Count of users with both exposure and metric events
- Event totals: Sum of metric events for exposed users
- Property values: Sum and average calculations of metric event properties
- Funnel conversions: Chronological event processing with FM (funnel users) and FT (funnel totals) metrics

The system emphasizes performance optimization through local defaults implementation, client-side variant caching, and network call optimization while maintaining statistical rigor through sequential testing methodologies that contrast with traditional T-test approaches.