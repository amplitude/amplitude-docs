Amplitude Experiment is a feature flagging and A/B testing platform that provides sophisticated experimentation capabilities through deterministic randomization, statistical analysis, and high-performance evaluation infrastructure. The platform supports both remote and local evaluation modes with comprehensive event tracking for experiment analysis.

## Core Architecture and Evaluation Modes

Amplitude Experiment operates through two primary evaluation modes:

**Remote Evaluation** uses `fetch()` calls to retrieve flag variants from Amplitude's servers, leveraging Fastly CDN for global distribution with 60-minute cache TTL. The infrastructure runs on AWS services including Application Load Balancer, RDS, and DynamoDB for experiment storage and user metadata.

**Local Evaluation** employs `evaluate()` calls using downloaded flag configurations, enabling client-side variant caching and reduced network dependencies. Local evaluation SDKs support advanced features like flag dependencies and are available for Node.js, Go, and JVM platforms.

## Randomization and User Assignment

The platform implements a deterministic randomization algorithm using murmur3_x86_32 hash of "bucketingSalt/amplitude_id" for consistent user assignment. This operates through a two-stage process:

1. **Experiment Inclusion**: Users are bucketed via mod 100 against percentage rollout
2. **Variant Assignment**: Floor division assigns users to weighted ranges (0-42949672) for variant allocation

This deterministic approach ensures users receive consistent experiences across sessions while supporting percentage rollouts and weighted variant distribution.

## Event Tracking and Analysis

Amplitude Experiment tracks user interactions through two event types:

**Assignment Events** (`[Experiment] Assignment`) are automatically generated during remote evaluation `fetch()` calls, capturing when users are assigned to experiment variants.

**Exposure Events** (`$exposure` or `[Experiment] Exposure`) track when users actually see experiment variants, typically fired during local evaluation `evaluate()` calls or through SDK integrations.

Experiment user properties follow the format `[Experiment] <flag_key>` and support inheritance and deduplication through insert_id mechanisms.

## Statistical Analysis and Sequential Testing

The platform employs mixture sequential probability ratio test (mSPRT) for statistical inference, enabling early experiment termination without predetermined sample sizes. This contrasts with traditional T-tests by providing valid results at any viewing time.

Experiment Analysis charts calculate metrics using specific formulas:
- **Unique Conversions**: Count of users who triggered metric events after exposure
- **Event Totals**: Sum of all metric events from exposed users  
- **Property Values**: Sum and average calculations of metric event properties
- **Funnel Conversions**: Chronological event processing with FM (funnel users) and FT (funnel totals) metrics

## Flag Dependencies and Advanced Features

Flag dependencies define evaluation order relationships enabling:
- **Flag Prerequisites**: Conditional flag activation based on other flags
- **Mutual Exclusion Groups**: Preventing users from entering multiple experiments
- **Holdout Groups**: Reserving user traffic with specific variant allocation

These features require local evaluation SDKs and support slot allocation mechanisms for traffic management.

## Performance and Caching

The platform delivers sub-100ms latency across geographic regions (us-west-2, eu-central-1) through Fastly CDN caching. Cache invalidation occurs automatically on deployment changes, with special considerations for dynamic targeting using user properties and behavioral cohorts.

Behavioral cohorts are computed hourly and cached appropriately, while user metadata stores support identity resolution across the platform. Performance optimization recommendations include implementing local defaults and client-side variant caching to minimize network calls.

## Key API Methods and Integration Points

- `fetch()`: Remote evaluation with automatic assignment tracking
- `evaluate()`: Local evaluation with exposure event generation  
- `variant()`: Variant retrieval method
- Exposure tracking providers for SDK integrations
- Event volume billing considerations for assignment vs exposure events

The platform integrates with Amplitude's broader analytics ecosystem, supporting user property inheritance, identity resolution, and comprehensive experiment analysis workflows.