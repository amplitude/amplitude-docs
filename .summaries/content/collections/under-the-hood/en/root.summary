# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test hypotheses, roll out features safely, and make data-driven decisions. The platform combines robust randomization techniques, statistical analysis, and performance optimization to deliver reliable experimentation capabilities.

Key features include:
- Deterministic user randomization for consistent experiment assignment
- Sequential statistical testing for faster decision-making
- Flag dependencies for complex experiment scenarios
- Comprehensive event tracking for accurate analysis
- High-performance evaluation through CDN caching and local evaluation options
- Detailed experiment analysis with various metric calculations

Primary use cases include A/B testing, feature rollouts, mutual exclusion testing, and holdout experiments to measure the combined impact of multiple experiments.

## Product and Feature Relationships

Amplitude Experiment consists of several interconnected components:

1. **Evaluation System**: Offers two modes:
   - **Remote Evaluation**: Server-side evaluation with CDN caching for performance
   - **Local Evaluation**: Client-side evaluation for sub-millisecond performance

2. **Randomization Engine**: Uses deterministic two-stage randomization:
   - First determines if a user is included in an experiment
   - Then assigns variants to included users based on weights

3. **Event Tracking System**: Captures two primary event types:
   - **Assignment Events**: Record when a user is assigned to an experiment variant
   - **Exposure Events**: Record when a user actually sees the experiment

4. **Analysis Framework**: Calculates experiment results using:
   - Unique conversions, event totals, property values, and funnel conversions
   - Sequential testing methodology (mSPRT) for statistical inference

5. **Flag Dependencies System**: Manages relationships between flags:
   - **Prerequisites**: Controls feature release dependencies
   - **Mutual Exclusion Groups**: Ensures users only participate in one experiment
   - **Holdout Groups**: Measures combined experiment impact

## Key Nomenclature and Definitions

- **Bucketing Key**: The unique identifier used for randomization, typically the Amplitude ID
- **Bucketing Salt**: A value that ensures different experiments have independent randomization
- **Variant**: A specific version or treatment in an experiment
- **Exposure**: When a user actually encounters an experiment variant
- **Assignment**: When a user is allocated to an experiment variant
- **mSPRT (mixture Sequential Probability Ratio Test)**: The statistical method used for sequential testing
- **Flag Dependencies**: Relationships between flags that control evaluation order
- **Cache Hit/Miss**: Whether a request is served from cache (hit) or requires backend processing (miss)
- **Fallback Variant**: The default variant served when a user doesn't qualify for an experiment
- **Behavioral Cohorts**: User segments based on past behavior used for targeting

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude ecosystem:

1. **Amplitude Analytics**: Experiment events flow into Amplitude Analytics for analysis
   - Assignment and exposure events are automatically tracked
   - Experiment user properties are set for segmentation

2. **Infrastructure**: Leverages AWS and Fastly CDN
   - Uses AWS Application Load Balancer, Relational Databases, and DynamoDB
   - Fastly CDN provides global caching for performance optimization

3. **Identity Resolution**: Connects with Amplitude's identity system
   - Uses Amplitude ID as the primary bucketing key
   - Supports consistent user identification across platforms

4. **Behavioral Cohorts**: Accesses Amplitude's behavioral cohort system
   - Enables targeting users based on past behaviors
   - Integrates with dynamic targeting capabilities

## Technical Implementation Details

### Randomization Algorithm
```
// Inclusion determination
included = mod(murmur3_x86_32("bucketingSalt/id"), 100) < rolloutPercentage

// Variant assignment (if included)
variantValue = floor(murmur3_x86_32("bucketingSalt/id"), 100)
```

### Performance Metrics
- **Remote Evaluation**:
  - Global average: 35.9ms (cache hit), 194.21ms (cache miss)
  - Cache TTL: Typically 5 minutes

- **Local Evaluation**:
  - Node.js SDK: 0.05ms
  - Go SDK: 0.03ms
  - JVM SDK: 0.08ms

### Event Tracking
- **Assignment Event**: `[Experiment] Assignment`
- **Exposure Event**: `[Experiment] Exposure`

Both events include experiment metadata and variant information, with automatic tracking available in supported SDKs.

### Flag Dependencies Implementation
```json
{
  "prerequisites": [
    {
      "flag": "parent-flag",
      "variants": ["on"]
    }
  ]
}
```

This system requires SDK version compatibility for proper implementation, with specific version requirements for each dependency type.