Amplitude Experiment is a feature flagging and A/B testing platform that provides deterministic experimentation capabilities with sophisticated statistical analysis, performance optimization, and comprehensive event tracking. The platform enables organizations to run controlled experiments through feature flags with advanced targeting, randomization, and real-time performance monitoring.

## Product Architecture and Feature Relationships

Amplitude Experiment operates through two primary evaluation modes that form the core of its architecture:

**Remote Evaluation** uses the `fetch()` method to retrieve flag configurations from Amplitude's cloud infrastructure, built on Fastly CDN with AWS services (Application Load Balancer, RDS, DynamoDB). This mode provides real-time flag updates but introduces network latency.

**Local Evaluation** employs the `evaluate()` method to process flags client-side using cached configurations, offering sub-millisecond performance with 60-minute cache TTL. Local evaluation requires specific SDK versions and supports advanced features like flag dependencies.

The **Flag Dependencies System** creates hierarchical relationships between flags, enabling prerequisites, mutual exclusion groups, and holdout groups. Dependencies define evaluation order and variant allocation logic, allowing complex experiment designs with traffic slot management.

**Event Tracking** operates through two complementary systems: assignment events (`[Experiment] Assignment`) generated during flag evaluation, and exposure events (`$exposure`/`[Experiment] Exposure`) triggered when users encounter experiment variants. The platform automatically tracks assignments via `fetch()` and `evaluate()` calls, while exposure tracking requires integration with the `variant()` method.

## Key Nomenclature and Definitions

**Assignment Events** (`[Experiment] Assignment`): Automatically generated when users are bucketed into experiment variants, used for traffic allocation analysis.

**Exposure Events** (`$exposure`/`[Experiment] Exposure`): Triggered when users actually experience experiment variants, used for statistical analysis and conversion measurement.

**Experiment User Properties** (`[Experiment] <flag_key>`): User-level properties storing variant assignments, enabling cohort analysis and user segmentation.

**Bucketing Salt**: Unique identifier combined with `amplitude_id` in the murmur3 hash algorithm for deterministic user assignment.

**Flag Dependencies**: Hierarchical relationships defining evaluation order, including prerequisites (required flag states), mutual exclusion groups (preventing simultaneous enrollment), and holdout groups (control populations).

**mSPRT (Mixture Sequential Probability Ratio Test)**: Statistical method enabling early experiment termination through continuous hypothesis testing of null vs alternative hypotheses.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the Amplitude Analytics platform through shared user identification (`amplitude_id`) and event tracking infrastructure. The system leverages Amplitude's behavioral cohorts, computed hourly, for dynamic user targeting and segmentation.

The platform's identity resolution system connects user metadata stores with experiment assignment logic, enabling sophisticated targeting based on user properties and behavioral patterns. Event volume billing considerations influence tracking strategy, with automatic deduplication via `insert_id` to prevent duplicate charges.

Geographic distribution spans multiple data centers (us-west-2, eu-central-1) with Fastly CDN routing for optimized global performance, supporting latency requirements across different regions.

## API Endpoints and Technical Implementation

**Core SDK Methods:**
- `fetch()`: Remote evaluation with automatic assignment tracking
- `evaluate()`: Local evaluation for high-performance scenarios  
- `variant()`: Exposure event triggering for statistical analysis

**Randomization Algorithm**: Two-stage deterministic assignment using murmur3_x86_32 hash of "bucketingSalt/amplitude_id", first determining experiment inclusion via mod 100 against percentage rollout, then variant assignment through floor division with weighted ranges 0-42949672.

**Performance Benchmarks**: Remote evaluation latency varies by region (sub-100ms typical), while local evaluation achieves sub-millisecond performance. Cache invalidation occurs automatically on deployment changes, with behavioral cohorts updating hourly.

**Statistical Analysis**: Experiment Analysis charts calculate unique conversions, event totals, property values, and funnel conversions using exposure events (E), metric events (M), and user counting logic with variables T, S, A for comprehensive experiment measurement.

The platform supports multiple SDK implementations (Node.js, Go, JVM) with consistent performance characteristics and feature parity across client-side and server-side experiment scenarios.