Amplitude Experiment is a comprehensive A/B testing and feature flagging platform that provides sophisticated experimentation capabilities with advanced statistical methods, high-performance infrastructure, and detailed analytics. The platform enables organizations to run controlled experiments, manage feature flags, and analyze results using cutting-edge statistical techniques.

## Core Features and Capabilities

The platform operates through two primary evaluation modes: **remote evaluation** for server-side experiments and **local evaluation** for client-side implementations. Amplitude Experiment employs deterministic randomization using the murmur3 hash algorithm, ensuring consistent user assignment across sessions while supporting complex experimental designs including flag dependencies, mutual exclusion groups, and holdout groups.

A key differentiator is the implementation of **mixture sequential probability ratio test (mSPRT)** for statistical inference, allowing experiments to be terminated early without predetermined sample sizes while maintaining statistical validity. This contrasts with traditional T-test approaches and enables more efficient experimentation workflows.

## Product Architecture and Infrastructure

The platform is built on a robust, globally distributed infrastructure leveraging **Fastly CDN** for edge caching and **AWS services** including Application Load Balancer (ALB), RDS, and DynamoDB. This architecture supports identity resolution, user metadata storage, and behavioral cohorts processing, with cache TTL of 60 minutes and automatic invalidation on deployment changes.

Performance is optimized through local defaults implementation and client-side variant caching, with latency benchmarks available across geographic regions (us-west-2, eu-central-1). The system supports high availability and scales to handle enterprise-level traffic loads.

## Key Nomenclature and Definitions

**Assignment Events**: Triggered by `fetch()` or `evaluate()` calls, tracking when users are assigned to experiment variants
**Exposure Events**: Triggered by `variant()` method access, indicating actual feature interaction
**Bucketing Salt**: Used in the murmur3 hash algorithm for deterministic user assignment
**Flag Dependencies**: Define evaluation order relationships between flags, enabling prerequisites and mutual exclusion
**Holdout Groups**: Control groups that receive no treatment, used for measuring overall experiment impact
**mSPRT**: Mixture sequential probability ratio test for continuous statistical monitoring
**Dynamic Targeting**: Real-time user segmentation based on properties and behavioral cohorts

## Statistical Analysis and Metrics

Experiment Analysis charts calculate various metrics including:
- **Unique Conversions**: User-level conversion tracking with deduplication
- **Event Totals**: Aggregate event counting across experiment variants  
- **Property Value Calculations**: Sum and average computations for numeric properties
- **Funnel Conversions**: Multi-step conversion tracking with chronological event processing

The system uses specific input variables (E, M, T, S, A) and supports both binary and continuous metrics, with outlier handling through Central Limit Theorem applications.

## API Integration and Implementation

Key methods include:
- `fetch()`: Remote evaluation API call for server-side experiments
- `evaluate()`: Local evaluation method for client-side implementations
- `variant()`: Variant access method that triggers exposure tracking

The platform supports multiple SDK implementations (Node.js, Go, JVM) with local storage capabilities and network call optimization.

## Event Tracking and Analytics

The system automatically tracks experiment interactions through:
- **[Experiment] Assignment** events with flag_key and variant properties
- **[Experiment] Exposure** events with experiment_key metadata
- **Experiment user properties** in format `[Experiment] <flag_key>`

Event deduplication uses insert_id mechanisms, and the system integrates with Monthly Tracked Users (MTU) billing models. Exposure tracking providers can be configured for custom analytics implementations.

## Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics platform, leveraging behavioral cohorts that are computed hourly and user properties for dynamic targeting. The system supports complex experimental designs through flag dependencies, enabling sophisticated traffic allocation strategies and prerequisite-based feature rollouts.

The platform handles both feature flagging for operational control and statistical experimentation for product optimization, providing a unified interface for managing the complete experimentation lifecycle from randomization through statistical analysis.