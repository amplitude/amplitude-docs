Amplitude Experiment is a feature flagging and A/B testing platform that provides deterministic experimentation capabilities with robust statistical analysis, high-performance delivery infrastructure, and comprehensive event tracking systems.

## Core Features and Capabilities

The platform operates through two primary evaluation modes: **remote evaluation** using `fetch()` calls and **local evaluation** using `evaluate()` methods. Remote evaluation leverages Fastly CDN caching with 60-minute TTL for optimal performance, while local evaluation enables client-side processing with pre-fetched flag configurations. The system supports advanced experimentation features including flag dependencies, mutual exclusion groups, holdout groups, and sequential testing methodologies.

## Product Relationships and Architecture

Amplitude Experiment integrates tightly with Amplitude's analytics platform through automated event tracking. The system generates `[Experiment] Assignment` events during remote evaluation and `$exposure`/`[Experiment] Exposure` events during local evaluation. User properties are automatically formatted as `[Experiment] <flag_key>` to enable downstream analysis.

The infrastructure spans multiple AWS services including Application Load Balancers and DynamoDB for experiment storage, with Fastly CDN providing global distribution. Identity resolution systems and user metadata stores support dynamic targeting based on user properties and behavioral cohorts, which are computed hourly.

## Key Nomenclature and Definitions

**Flag Dependencies**: Define evaluation order relationships between flags, enabling prerequisites, mutual exclusion groups, and holdout functionality. Dependencies control variant allocation and traffic percentage distribution across experiment slots.

**Bucketing Process**: Uses deterministic randomization via murmur3 hash of "bucketingSalt/amplitude_id" for two-stage assignment. First stage determines experiment inclusion using mod 100 against percentage rollout, second stage assigns variants through floor division with weighted ranges 0-42949672.

**Sequential Testing**: Implements mixture sequential probability ratio test (mSPRT) for statistical inference, allowing valid results at any viewing time without predetermined sample sizes and enabling early experiment termination.

**Exposure Events**: Track when users are exposed to experiment variants, distinguished from assignment events which track initial user bucketing. These events feed into Experiment Analysis charts for conversion calculations.

## Broader Product Ecosystem Integration

Amplitude Experiment functions as the experimentation layer within Amplitude's product analytics ecosystem. Experiment user properties automatically populate in Amplitude Analytics for segmentation and analysis. The platform supports behavioral cohorts computed from user activity data, enabling sophisticated targeting strategies.

The system integrates with multiple SDK environments (Node.js, Go, JVM) with specific minimum versions required for local evaluation of flag dependencies. Geographic performance optimization spans regions including us-west-2 and eu-central-1 with sub-100ms latency targets.

## API Endpoints and Implementation Methods

Primary API methods include:
- `fetch()` for remote evaluation with automatic assignment tracking
- `evaluate()` for local evaluation with exposure event generation  
- `variant()` for retrieving specific variant assignments

The platform supports cache invalidation through deployment updates and provides insert_id deduplication for event tracking. Client-side caching and local defaults implementation are recommended for optimal performance, with behavioral cohorts requiring hourly computation cycles for dynamic targeting scenarios.

Experiment Analysis charts calculate metrics using exposure events (E), metric events (M, T), property values (S, A), and funnel events (FM, FT) through chronological event processing for user-based experiment analysis.