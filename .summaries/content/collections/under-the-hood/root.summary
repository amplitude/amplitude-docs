# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test hypotheses, safely roll out features, and make data-driven decisions. The platform combines deterministic randomization, statistical analysis, and performance optimization to deliver reliable experimentation capabilities.

Key features include deterministic user randomization, sequential statistical testing, flag dependencies, comprehensive event tracking, high-performance evaluation through CDN caching and local evaluation, and detailed experiment analysis with various metric calculations.

The platform supports A/B testing, feature rollouts, mutual exclusion testing, and holdout experiments to measure the combined impact of multiple experiments.

## Product and Feature Relationships

Amplitude Experiment consists of five interconnected components:

1. **Evaluation System** operates in two modes:
   - Remote Evaluation: Server-side evaluation with CDN caching
   - Local Evaluation: Client-side evaluation for sub-millisecond performance

2. **Randomization Engine** uses deterministic two-stage randomization:
   - First determines user inclusion in an experiment
   - Then assigns variants to included users based on weights

3. **Event Tracking System** captures:
   - Assignment Events: When a user is assigned to an experiment variant
   - Exposure Events: When a user actually sees the experiment

4. **Analysis Framework** calculates results using:
   - Unique conversions, event totals, property values, and funnel conversions
   - Sequential testing methodology (mSPRT) for statistical inference

5. **Flag Dependencies System** manages relationships between flags:
   - Prerequisites: Controls feature release dependencies
   - Mutual Exclusion Groups: Ensures users participate in only one experiment
   - Holdout Groups: Measures combined experiment impact

## Key Nomenclature and Definitions

- **Bucketing Key**: Unique identifier used for randomization (typically Amplitude ID)
- **Bucketing Salt**: Value ensuring independent randomization across experiments
- **Variant**: Specific version or treatment in an experiment
- **Exposure**: When a user encounters an experiment variant
- **Assignment**: When a user is allocated to an experiment variant
- **mSPRT**: Mixture Sequential Probability Ratio Test - statistical method for sequential testing
- **Flag Dependencies**: Relationships between flags controlling evaluation order
- **Cache Hit/Miss**: Whether a request is served from cache or requires backend processing
- **Fallback Variant**: Default variant served when a user doesn't qualify for an experiment
- **Behavioral Cohorts**: User segments based on past behavior used for targeting

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude ecosystem through:

1. **Amplitude Analytics**: Experiment events flow into Analytics for analysis
   - Assignment and exposure events are automatically tracked
   - Experiment user properties are set for segmentation

2. **Infrastructure**: Leverages AWS and Fastly CDN
   - Uses AWS Application Load Balancer, Relational Databases, and DynamoDB
   - Fastly CDN provides global caching for performance optimization

3. **Identity Resolution**: Connects with Amplitude's identity system
   - Uses Amplitude ID as the primary bucketing key
   - Supports consistent user identification across platforms

4. **Behavioral Cohorts**: Accesses Amplitude's behavioral cohort system
   - Enables targeting users based on past behaviors
   - Integrates with dynamic targeting capabilities

## Technical Implementation Details

### Randomization Algorithm
```
// Inclusion determination
included = mod(murmur3_x86_32("bucketingSalt/id"), 100) < rolloutPercentage

// Variant assignment (if included)
variantValue = floor(murmur3_x86_32("bucketingSalt/id"), 100)
```

### Performance Metrics
- **Remote Evaluation**:
  - Global average: 35.9ms (cache hit), 194.21ms (cache miss)
  - Cache TTL: Typically 5 minutes

- **Local Evaluation**:
  - Node.js SDK: 0.05ms
  - Go SDK: 0.03ms
  - JVM SDK: 0.08ms

### Event Tracking
- **Assignment Event**: `[Experiment] Assignment`
- **Exposure Event**: `[Experiment] Exposure`

Both events include experiment metadata and variant information, with automatic tracking available in supported SDKs.

### Flag Dependencies Implementation
```json
{
  "prerequisites": [
    {
      "flag": "parent-flag",
      "variants": ["on"]
    }
  ]
}
```

This system requires SDK version compatibility for proper implementation, with specific version requirements for each dependency type.