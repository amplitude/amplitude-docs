Amplitude Experiment is a comprehensive feature flagging and A/B testing platform designed for enterprise-scale experimentation and feature management. The platform provides sophisticated statistical analysis capabilities, high-performance global delivery, and flexible evaluation modes to support both real-time feature rollouts and controlled experiments.

## Core Product Architecture

The system operates on a distributed cloud architecture leveraging Fastly CDN for global content delivery, AWS Application Load Balancer for intelligent traffic routing, and DynamoDB for experiment configuration persistence. This infrastructure delivers sub-100ms latency globally with cache hit rates exceeding 95%, supporting both remote and local evaluation paradigms.

**Remote Evaluation** utilizes CDN caching with 60-minute TTL for server-side flag resolution, while **Local Evaluation** enables client-side processing with pre-fetched configurations, achieving microsecond-level evaluation performance. The platform maintains geographic routing optimization across us-west-2 and eu-central-1 regions.

## Feature Relationships and Dependencies

Amplitude Experiment supports complex feature relationships through three primary dependency mechanisms:

- **Flag Prerequisites**: Establish hierarchical evaluation chains where dependent flags require parent flag activation
- **Mutual Exclusion Groups**: Prevent users from simultaneous assignment to conflicting experiments, ensuring clean traffic allocation
- **Holdout Groups**: Reserve specific traffic percentages for control analysis and baseline comparisons

These dependency types enable sophisticated experiment slot management and traffic allocation strategies while maintaining statistical validity through proper holdout analysis.

## Key Nomenclature and Definitions

**Bucketing Salt**: Unique identifier used in hash generation for deterministic user assignment
**Assignment Events**: `[Experiment] Assignment` events tracking when users are allocated to experiment variants
**Exposure Events**: `$exposure` or `[Experiment] Exposure` events recording actual user interaction with experiments
**Behavioral Cohorts**: User segments computed hourly based on behavioral patterns and properties
**Identity Resolution**: System maintaining user metadata stores for dynamic targeting capabilities
**mSPRT**: Mixture sequential probability ratio test enabling continuous statistical inference

## Statistical Analysis Framework

The platform employs advanced statistical methodologies centered on sequential testing principles. The system uses mixture sequential probability ratio test (mSPRT) for continuous statistical inference, enabling early stopping capabilities that provide valid results at any viewing time without predetermined sample sizes.

Analysis processing handles multiple event types chronologically:
- **Exposure events (E)**: User experiment interactions
- **Metric events (M, T)**: Conversion and performance measurements  
- **Property values (S, A)**: User attribute data for segmentation
- **Funnel events (FM, FT)**: Multi-step conversion tracking

The framework supports both binary and continuous metrics using Central Limit Theorem principles for multi-metric analysis capabilities.

## Randomization and Assignment System

User assignment utilizes a sophisticated deterministic randomization system based on murmur3 hash algorithms. The two-stage assignment process involves:

1. **Hash Generation**: Creates murmur3_x86_32 hash of "bucketingSalt/amplitude_id"
2. **Inclusion Determination**: First stage uses mod 100 against percentage rollout for experiment inclusion
3. **Variant Assignment**: Second stage employs floor division with weighted ranges (0-42949672) for variant allocation

This deterministic approach ensures consistent user experiences across sessions while maintaining statistical validity and reproducible experiment results.

## SDK Integration and API Methods

The platform provides comprehensive SDK support across multiple platforms (Node.js, Go, JVM) with specific compatibility matrices for feature support. Primary API methods include:

- `fetch()`: Remote evaluation with automatic assignment event tracking
- `evaluate()`: Local evaluation for single flag assessment without exposure tracking
- `variant()`: Local evaluation with automatic exposure event generation

SDK implementations support client-side caching, local defaults, and automatic event tracking with insert_id deduplication to prevent duplicate exposure recording.

## Caching and Performance Optimization

The CDN caching system operates with sophisticated cache invalidation mechanisms triggered by deployment updates and configuration changes. Cache keys incorporate user information to ensure personalized experiment delivery while maintaining optimal performance through high cache hit rates.

Performance optimization features include:
- Client-side SDK caching with configurable TTL
- Local defaults implementation for offline scenarios
- Behavioral cohorts with hourly computation cycles
- Dynamic targeting based on real-time user properties

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude analytics ecosystem, automatically formatting experiment user properties as `[Experiment] <flag_key>` for seamless analysis integration. The platform supports automatic exposure tracking that feeds directly into Amplitude's analytics pipeline, enabling comprehensive experiment performance analysis and user journey tracking.

The system maintains identity resolution capabilities that leverage Amplitude's user identification system, ensuring consistent user experiences across the entire product ecosystem while supporting sophisticated targeting and segmentation strategies.