# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test new features and measure their impact. The platform provides both remote evaluation (server-side) and local evaluation (client-side) modes to accommodate different implementation needs. Core capabilities include deterministic randomization for consistent user assignment, sequential statistical testing for flexible experiment analysis, flag dependencies for complex experiment coordination, and comprehensive event tracking to measure experiment outcomes.

The platform is built for enterprise-grade performance and reliability, utilizing AWS infrastructure and Fastly CDN to deliver fast evaluation responses globally. Amplitude Experiment is deeply integrated with the Amplitude analytics ecosystem, allowing teams to leverage behavioral data for targeting and to analyze experiment results using sophisticated statistical methods.

## Product Relationships and Architecture

Amplitude Experiment consists of several key components that work together:

### Evaluation Systems
- **Remote Evaluation**: Server-side evaluation where flag rules are processed on Amplitude's servers, ideal for server-rendered applications or when client performance is a concern
- **Local Evaluation**: Client-side evaluation where flag rules are downloaded and processed locally, providing sub-millisecond performance and offline capabilities

### Event Tracking Framework
- **Assignment Events**: Automatically tracked when users are assigned to experiment variants
- **Exposure Events**: Recorded when users are actually exposed to experiment treatments
- **Event Property Inheritance**: Experiment metadata is attached to all subsequent events for analysis

### Analysis Engine
- **Sequential Testing Engine**: Implements modified Sequential Probability Ratio Test (mSPRT) to provide valid statistical inference at any point during an experiment
- **Experiment Analysis Charts**: Visualize experiment results across various metrics and segments

### Flag Management System
- **Flag Configuration**: Controls for variant distribution, targeting rules, and rollout percentages
- **Flag Dependencies**: Manages relationships between flags:
  - Prerequisites: Flags that must be evaluated before dependent flags
  - Mutual Exclusion Groups: Ensure users only participate in one experiment at a time
  - Holdout Groups: Withhold traffic to measure combined experiment impact

### Infrastructure Components
- **Fastly CDN**: Caches evaluation results for improved performance
- **AWS Services**: Including Application Load Balancer, RDS, and DynamoDB
- **Identity Resolution System**: Ensures consistent user identification across touchpoints

## Key Nomenclature and Definitions

- **Bucketing Key**: The unique identifier (typically Amplitude ID) used to determine variant assignment
- **Bucketing Salt**: A string used to ensure randomization is unique to each experiment
- **Deterministic Randomization**: Two-stage process using murmur3 hash functions to consistently assign users to variants
- **Sequential Testing (mSPRT)**: Statistical method allowing valid inference at any point during an experiment without requiring predetermined sample sizes
- **Flag Dependencies**:
  - **Prerequisites**: Flags that must be evaluated before dependent flags
  - **Mutual Exclusion Groups**: Ensure users are only included in one experiment at a time
  - **Holdout Groups**: Withhold traffic to measure combined experiment impact
- **Cache Hit/Miss**: Whether an evaluation request is served from cache (hit) or requires full processing (miss)
- **Exposure Transformation**: Process of converting assignment events to exposure events
- **Variant**: A specific treatment or version in an experiment
- **Rollout Percentage**: The percentage of users who will be included in an experiment

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude ecosystem in several ways:

1. **Amplitude Analytics Integration**: Experiment results are analyzed using Amplitude's analytics capabilities. Experiment metadata is attached to events as user properties, enabling segmentation and analysis.

2. **Behavioral Targeting**: Experiments can target users based on behavioral cohorts defined in Amplitude Analytics, allowing for precise audience selection.

3. **Identity Resolution**: The platform leverages Amplitude's identity resolution system to maintain consistent user identification across devices and sessions.

4. **SDK Ecosystem**: Various SDKs (Node.js, Go, JVM, client-side JavaScript) integrate with the platform for different implementation scenarios, with performance optimized for each environment.

5. **Data Pipelines**: Experiment data flows into the broader Amplitude data ecosystem for comprehensive analysis and reporting.

## API and Implementation Details

### Evaluation Endpoints
- Remote evaluation is performed via API endpoints with responses cached in Fastly CDN
- Performance metrics:
  - Remote evaluation: 35.9ms average for cache hits, 194.21ms for cache misses globally
  - Local evaluation: Sub-millisecond performance (Node.js: 0.025ms, Go: 0.009ms, JVM: 0.012ms)

### Randomization Implementation
```
// Inclusion determination
mod(murmur3_x86_32("bucketingSalt/id"), 100) < rolloutPercentage

// Variant assignment
variantIndex = floor(murmur3_x86_32("bucketingSalt/id") / MAX_HASH * 100)
```

### Event Tracking Implementation
- Assignment events: `[Experiment] Assignment` (automatically tracked by SDKs)
- Exposure events: `[Experiment] Exposure` (must be explicitly tracked in some cases)
- User properties set on events:
  - `[Experiment] <experiment-key>` (variant name)
  - `[Experiment] <experiment-key> Variant` (variant value)

### Analysis Calculation Formulas
- Unique conversions: `(users with metric event AND exposure) / (users with exposure)`
- Event totals: `(sum of events from exposed users) / (count of exposed users)`
- Property value sum: `(sum of property values from exposed users) / (count of exposed users with property)`
- Funnel conversion: `(users completing funnel AND exposed) / (users starting funnel AND exposed)`