Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that enables sophisticated experimentation through deterministic user assignment, statistical analysis, and high-performance evaluation infrastructure. The platform serves as a critical component for product teams conducting controlled experiments and feature rollouts at scale.

## Core Product Architecture

Amplitude Experiment operates through a dual-mode evaluation system designed for flexibility and performance:

**Remote Evaluation Mode** utilizes `fetch()` API calls to retrieve flag variants from Amplitude's cloud infrastructure. This mode leverages Fastly CDN for global content distribution with 60-minute cache TTL, backed by AWS services including Application Load Balancer, RDS, and DynamoDB for experiment configuration storage and user metadata management.

**Local Evaluation Mode** employs `evaluate()` API calls using pre-downloaded flag configurations, enabling client-side processing with reduced network dependencies. This mode supports advanced capabilities like flag dependencies and is available through specialized SDKs for Node.js, Go, and JVM platforms.

## User Assignment and Randomization Engine

The platform implements a sophisticated deterministic randomization system using murmur3_x86_32 hash algorithms applied to "bucketingSalt/amplitude_id" combinations. This ensures consistent user experiences across sessions through a two-stage assignment process:

1. **Experiment Inclusion Stage**: Users are bucketed using modulo 100 operations against percentage rollout configurations
2. **Variant Assignment Stage**: Floor division algorithms assign users to weighted ranges (0-42949672) for precise variant allocation

This deterministic approach enables reliable traffic splitting while supporting dynamic percentage rollouts and complex variant weighting schemes.

## Event Tracking and Analytics Framework

Amplitude Experiment captures user interactions through a dual-event tracking system:

**Assignment Events** (`[Experiment] Assignment`) are automatically generated during remote evaluation `fetch()` calls, documenting when users receive experiment variant assignments from the server.

**Exposure Events** (`$exposure` or `[Experiment] Exposure`) track actual user interactions with experiment variants, typically triggered during local evaluation `evaluate()` calls or through SDK exposure tracking providers.

User properties follow the standardized format `[Experiment] <flag_key>` and support inheritance mechanisms and deduplication through insert_id systems for accurate experiment attribution.

## Statistical Analysis and Sequential Testing

The platform employs mixture sequential probability ratio test (mSPRT) methodology for statistical inference, enabling valid experiment conclusions at any analysis point without predetermined sample size requirements. This advanced approach contrasts with traditional T-test methodologies by supporting early experiment termination based on statistical significance.

Experiment Analysis charts calculate key metrics through specific formulas:
- **Unique Conversions**: Distinct user counts triggering metric events post-exposure
- **Event Totals**: Aggregate sum of metric events from exposed user populations
- **Property Values**: Sum and average calculations of metric event properties
- **Funnel Conversions**: Chronological event sequence processing with FM (funnel users) and FT (funnel totals) metrics

## Advanced Feature Management

**Flag Dependencies** enable sophisticated experiment orchestration through:
- **Flag Prerequisites**: Conditional activation based on other flag states
- **Mutual Exclusion Groups**: Traffic isolation preventing cross-experiment contamination
- **Holdout Groups**: Reserved user traffic allocation with specific variant distributions

These capabilities require local evaluation SDKs and support slot allocation mechanisms for precise traffic management across complex experiment portfolios.

## Performance and Global Infrastructure

The platform delivers sub-100ms latency across multiple geographic regions (us-west-2, eu-central-1) through Fastly CDN caching infrastructure. Cache invalidation occurs automatically during deployment changes, with specialized handling for dynamic targeting using user properties and behavioral cohorts.

Behavioral cohorts undergo hourly computation cycles with appropriate caching strategies, while user metadata stores facilitate identity resolution across the broader Amplitude ecosystem. Performance optimization includes local defaults implementation and client-side variant caching to minimize network overhead.

## Key API Methods and Integration Points

Core API methods include:
- `fetch()`: Remote evaluation with automatic assignment event tracking
- `evaluate()`: Local evaluation with exposure event generation capabilities
- `variant()`: Direct variant retrieval for flag states

The platform integrates deeply with Amplitude's analytics ecosystem through exposure tracking providers, user property inheritance systems, and comprehensive experiment analysis workflows. Event volume considerations affect billing models, particularly regarding assignment versus exposure event generation patterns.

## Product Ecosystem Integration

Amplitude Experiment functions as a central component within Amplitude's broader product analytics platform, supporting identity resolution, user property inheritance, and seamless data flow between experimentation and analytics workflows. The platform enables product teams to conduct sophisticated experiments while maintaining data consistency across the entire Amplitude ecosystem for comprehensive user behavior analysis and product optimization.