Amplitude Experiment is a feature flagging and A/B testing platform that delivers deterministic experimentation capabilities through high-performance infrastructure, advanced statistical analysis, and comprehensive event tracking systems. The platform enables organizations to conduct controlled experiments with sophisticated targeting, real-time evaluation, and robust analytics integration.

## Core Features and Capabilities

The platform operates through two primary evaluation modes: **remote evaluation** using `fetch()` calls and **local evaluation** using `evaluate()` methods. Remote evaluation leverages Fastly CDN caching with 60-minute TTL for optimal performance, while local evaluation enables client-side processing with pre-fetched flag configurations. 

Advanced experimentation features include flag dependencies for complex experiment relationships, mutual exclusion groups to prevent user overlap, holdout groups for incrementality testing, and sequential testing methodologies that enable early experiment termination without predetermined sample sizes. The system implements mixture sequential probability ratio test (mSPRT) for statistical inference, allowing valid results at any viewing time.

## Product Relationships and Architecture

Amplitude Experiment integrates seamlessly with Amplitude's analytics platform through automated event tracking. The system generates `[Experiment] Assignment` events during remote evaluation and `$exposure`/`[Experiment] Exposure` events during local evaluation. User properties are automatically formatted as `[Experiment] <flag_key>` to enable downstream analysis in Amplitude Analytics.

The infrastructure spans multiple AWS services including Application Load Balancers and DynamoDB for experiment storage, with Fastly CDN providing global distribution across regions including us-west-2 and eu-central-1 with sub-100ms latency targets. Identity resolution systems and user metadata stores support dynamic targeting based on user properties and behavioral cohorts, which are computed hourly.

## Key Nomenclature and Definitions

**Flag Dependencies** define evaluation order relationships between flags, enabling prerequisites, mutual exclusion groups, and holdout functionality. Dependencies control variant allocation and traffic percentage distribution across experiment slots.

**Bucketing Process** uses deterministic randomization via murmur3 hash of "bucketingSalt/amplitude_id" for two-stage assignment. The first stage determines experiment inclusion using mod 100 against percentage rollout, while the second stage assigns variants through floor division with weighted ranges 0-42949672.

**Sequential Testing** implements mSPRT for continuous statistical monitoring, allowing experiments to conclude early when statistical significance is achieved without waiting for predetermined sample sizes.

**Exposure Events** track when users are exposed to experiment variants, distinguished from assignment events which track initial user bucketing. These events feed into Experiment Analysis charts for conversion calculations and are critical for accurate experiment measurement.

## Broader Product Ecosystem Integration

Amplitude Experiment functions as the experimentation layer within Amplitude's comprehensive product analytics ecosystem. Experiment user properties automatically populate in Amplitude Analytics for segmentation and analysis, while behavioral cohorts computed from user activity data enable sophisticated targeting strategies.

The platform supports multiple SDK environments including Node.js, Go, and JVM with specific minimum versions required for local evaluation of flag dependencies. Integration with Amplitude's broader data infrastructure enables real-time user property updates and cohort membership calculations.

## API Endpoints and Implementation Methods

Primary API methods include:
- `fetch()` for remote evaluation with automatic assignment tracking
- `evaluate()` for local evaluation with exposure event generation
- `variant()` for retrieving specific variant assignments

The platform supports cache invalidation through deployment updates and provides insert_id deduplication for event tracking. Client-side caching and local defaults implementation are recommended for optimal performance, with behavioral cohorts requiring hourly computation cycles for dynamic targeting scenarios.

Experiment Analysis charts calculate metrics using exposure events (E), metric events (M, T), property values (S, A), and funnel events (FM, FT) through chronological event processing for user-based experiment analysis. The system maintains geographic performance optimization with robust failover mechanisms and comprehensive monitoring across the global CDN infrastructure.