Amplitude Experiment is a feature flagging and A/B testing platform that delivers deterministic experimentation capabilities with advanced statistical analysis, performance optimization, and comprehensive event tracking. The platform enables organizations to conduct controlled experiments through feature flags with sophisticated targeting, randomization algorithms, and real-time performance monitoring across global infrastructure.

## Product Architecture and Feature Relationships

The platform operates through two fundamental evaluation modes that define its core architecture:

**Remote Evaluation** leverages the `fetch()` method to retrieve flag configurations from Amplitude's cloud infrastructure, built on Fastly CDN with AWS services including Application Load Balancer, RDS, and DynamoDB. This mode provides real-time flag updates with network latency trade-offs.

**Local Evaluation** utilizes the `evaluate()` method for client-side flag processing using cached configurations, delivering sub-millisecond performance with 60-minute cache TTL. This mode requires specific SDK versions and enables advanced capabilities like flag dependencies.

The **Flag Dependencies System** creates hierarchical relationships between flags, supporting prerequisites, mutual exclusion groups, and holdout groups. Dependencies control evaluation order and variant allocation logic, enabling complex experiment designs with sophisticated traffic slot management.

**Event Tracking Architecture** operates through dual systems: assignment events (`[Experiment] Assignment`) generated during flag evaluation, and exposure events (`$exposure`/`[Experiment] Exposure`) triggered when users encounter experiment variants. The platform automatically tracks assignments via `fetch()` and `evaluate()` calls, while exposure tracking requires integration with the `variant()` method for statistical analysis.

## Key Nomenclature and Definitions

**Assignment Events** (`[Experiment] Assignment`): Automatically generated events when users are bucketed into experiment variants, primarily used for traffic allocation analysis and debugging.

**Exposure Events** (`$exposure`/`[Experiment] Exposure`): Events triggered when users actually experience experiment variants, forming the foundation for statistical analysis and conversion measurement.

**Experiment User Properties** (`[Experiment] <flag_key>`): User-level properties storing variant assignments, enabling cohort analysis, user segmentation, and cross-experiment insights.

**Bucketing Salt**: Unique identifier combined with `amplitude_id` in the murmur3 hash algorithm to ensure deterministic and consistent user assignment across sessions.

**Flag Dependencies**: Hierarchical relationships defining evaluation order and constraints, including prerequisites (required flag states), mutual exclusion groups (preventing simultaneous enrollment), and holdout groups (control populations).

**mSPRT (Mixture Sequential Probability Ratio Test)**: Advanced statistical method enabling early experiment termination through continuous hypothesis testing, comparing null versus alternative hypotheses for faster decision-making.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates seamlessly with the Amplitude Analytics platform through shared user identification systems (`amplitude_id`) and unified event tracking infrastructure. The platform leverages Amplitude's behavioral cohorts, computed hourly, for dynamic user targeting and sophisticated segmentation strategies.

The identity resolution system connects user metadata stores with experiment assignment logic, enabling complex targeting based on user properties, behavioral patterns, and real-time analytics data. Event volume billing considerations influence tracking strategy implementation, with automatic deduplication via `insert_id` to prevent duplicate charges and optimize cost management.

Geographic distribution spans multiple data centers (us-west-2, eu-central-1) with Fastly CDN routing for optimized global performance, supporting varying latency requirements across different regions and user bases.

## API Endpoints and Technical Implementation

**Core SDK Methods:**
- `fetch()`: Remote evaluation with automatic assignment event tracking
- `evaluate()`: Local evaluation optimized for high-performance scenarios
- `variant()`: Exposure event triggering essential for statistical analysis

**Randomization Algorithm**: Two-stage deterministic assignment using murmur3_x86_32 hash of "bucketingSalt/amplitude_id". First stage determines experiment inclusion via modulo 100 against percentage rollout, followed by variant assignment through floor division with weighted ranges 0-42949672.

**Performance Benchmarks**: Remote evaluation latency varies by geographic region (typically sub-100ms), while local evaluation consistently achieves sub-millisecond performance. Cache invalidation occurs automatically on deployment changes, with behavioral cohorts updating on hourly intervals.

**Statistical Analysis Framework**: Experiment Analysis charts calculate unique conversions, event totals, property values, and funnel conversions using exposure events (E), metric events (M), and user counting logic with variables T, S, A for comprehensive experiment measurement and insights.

The platform supports multiple SDK implementations (Node.js, Go, JVM) with consistent performance characteristics and feature parity across both client-side and server-side experiment scenarios, ensuring flexibility in implementation approaches while maintaining statistical rigor.