# Amplitude Experiment: Under the Hood

## Product Overview

Amplitude Experiment is a feature flagging and experimentation platform that enables product teams to test new features and measure their impact. The platform provides two evaluation modes: remote evaluation (server-based) and local evaluation (client-side), allowing teams to choose the implementation that best fits their performance and functionality requirements.

Key features include deterministic randomization to ensure consistent user experiences, sequential statistical testing for faster experiment analysis, flag dependencies for complex experiment scenarios, and comprehensive event tracking that integrates with Amplitude Analytics. The platform is built on AWS infrastructure with Fastly CDN integration to deliver high-performance flag evaluations globally.

## Product Relationships and Features

### Evaluation Modes

1. **Remote Evaluation**
   - Server-based evaluation where flag rules are processed on Amplitude's servers
   - Leverages Fastly CDN for response caching (35.9ms average for cache hits vs. 194.21ms for misses)
   - Supports dynamic targeting and behavioral cohorts
   - Recommended for client-side implementations where latency is less critical

2. **Local Evaluation**
   - Client-side evaluation where flag rules are processed within the application
   - Delivers sub-millisecond performance across Node.js, Go, and JVM SDKs
   - Requires downloading flag configurations to the client
   - Ideal for server-side implementations where minimal latency is critical

### Flag Dependencies System

Amplitude Experiment supports three types of flag dependencies:

1. **Flag Prerequisites**: Allows flags to depend on the state of other flags
2. **Mutual Exclusion Groups**: Ensures users only participate in one experiment at a time
3. **Holdout Groups**: Withholds traffic from multiple experiments to measure combined impact

### Event Tracking System

The platform uses two primary event types for experiment analysis:

1. **Assignment Events**: Record when a user is assigned to an experiment variant
   - Can be tracked automatically or manually depending on implementation
   - Set experiment user properties for downstream analysis

2. **Exposure Events**: Record when a user is actually exposed to an experiment
   - Critical for accurate analysis as they distinguish assignment from actual exposure
   - Can be transformed from assignment events in certain implementations

### Statistical Analysis

Amplitude uses sequential testing methodology (mSPRT - mixture Sequential Probability Ratio Test) which:
- Allows valid statistical conclusions at any point during an experiment
- Enables faster experimentation and early stopping when results are clear
- Doesn't require predetermined sample sizes unlike traditional T-tests

## Key Nomenclature and Definitions

- **Bucketing Key**: The unique identifier used to assign users to variants (typically Amplitude ID)
- **Bucketing Salt**: A string used to ensure different experiments produce different assignments
- **Deterministic Randomization**: Process ensuring users consistently receive the same variant
- **Exposure Transformation**: Converting assignment events to exposure events automatically
- **Flag Dependencies**: Relationships between flags that control evaluation order
- **Local Defaults**: Fallback values used when flag evaluation fails
- **mSPRT**: Mixture Sequential Probability Ratio Test, the statistical method used for experiment analysis
- **Mutual Exclusion Groups**: Configuration ensuring users only participate in one experiment at a time
- **Remote Evaluation**: Server-side processing of flag rules
- **Local Evaluation**: Client-side processing of flag rules
- **Variant**: A specific version or treatment in an experiment

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude product ecosystem:

1. **Amplitude Analytics**
   - Provides user behavioral data for experiment targeting
   - Receives and processes experiment assignment and exposure events
   - Powers experiment analysis dashboards and reporting

2. **Amplitude SDKs**
   - Available for multiple platforms (Node.js, Go, JVM, client-side JavaScript)
   - Support both remote and local evaluation modes
   - Handle event tracking and flag evaluation

3. **Amplitude Cohorts**
   - Used for targeting experiments to specific user segments
   - Accessible in real-time for remote evaluation

## Technical Implementation Details

### Randomization Algorithm
```
// Inclusion determination
mod(murmur3_x86_32("bucketingSalt/id"), 100) < inclusionPercent

// Variant assignment
variantIndex = floor(murmur3_x86_32("bucketingSalt/id") / MAX_HASH * 100)
```

### Performance Metrics
- Remote evaluation with cache hit: ~35.9ms globally
- Remote evaluation with cache miss: ~194.21ms globally
- Local evaluation: sub-millisecond across all SDKs

### Experiment Analysis Calculations
- **Unique Conversions**: (Users who triggered metric event) รท (Users exposed to variant)
- **Event Totals**: (Total metric events) รท (Users exposed to variant)
- **Property Values**: (Sum of property values) รท (Users who triggered metric event)
- **Funnel Conversions**: (Users who completed funnel) รท (Users who started funnel)

### Caching Mechanisms
- CDN caching with configurable TTL (time-to-live)
- Cache invalidation on flag updates
- Client-side caching for improved performance
- Local storage options for offline capabilities