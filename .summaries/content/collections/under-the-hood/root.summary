Amplitude Experiment is a sophisticated A/B testing and feature flagging platform that combines advanced statistical methods with high-performance infrastructure to enable comprehensive experimentation capabilities. The platform serves as a critical component of the broader Amplitude analytics ecosystem, providing organizations with tools to run controlled experiments, manage feature flags, and analyze results using cutting-edge statistical techniques.

## Core Architecture and Evaluation Modes

The platform operates through two distinct evaluation paradigms: **remote evaluation** for server-side experiments and **local evaluation** for client-side implementations. This dual-mode architecture enables flexible deployment strategies while maintaining consistent user experiences across different application contexts.

The underlying randomization system employs deterministic assignment using the murmur3_x86_32 hash algorithm, processing a combination of bucketing salt and amplitude_id to ensure consistent user assignment across sessions. This two-stage process first determines experiment inclusion through mod 100 operations against percentage rollout, then assigns specific variants using floor division with weighted ranges (0-42949672).

## Advanced Statistical Framework

A key differentiator is the implementation of **mixture sequential probability ratio test (mSPRT)** for statistical inference, enabling experiments to be terminated early without predetermined sample sizes while maintaining statistical validity. This approach contrasts with traditional T-test methodologies and supports both binary and continuous metrics through Central Limit Theorem applications.

The sequential testing framework handles outliers appropriately and enables valid statistical inference at any viewing time, significantly improving experimentation efficiency compared to fixed-horizon testing approaches.

## Infrastructure and Performance Optimization

The platform leverages a globally distributed infrastructure built on **Fastly CDN** for edge caching and **AWS services** including Application Load Balancer (ALB), RDS, and DynamoDB. This architecture spans multiple geographic regions (us-west-2, eu-central-1) and supports identity resolution, user metadata storage, and behavioral cohorts processing with high availability.

Performance optimization occurs through multiple layers:
- Client-side variant caching with automatic invalidation
- Local defaults implementation for offline scenarios
- CDN caching with 60-minute TTL
- Sub-100ms latency for cached requests globally
- Automatic cache invalidation upon deployment changes

## Key Nomenclature and Operational Concepts

**Assignment Events** are automatically triggered by `fetch()` or `evaluate()` API calls, tracking when users are assigned to experiment variants, while **Exposure Events** are triggered by `variant()` method access, indicating actual feature interaction and user engagement.

**Flag Dependencies** define evaluation order relationships that enable sophisticated experimental designs including prerequisites, mutual exclusion groups, and holdout configurations. These dependencies support complex traffic allocation scenarios while maintaining deterministic assignment behavior.

**Experiment User Properties** are stored in the format `[Experiment] <flag_key>` and enable downstream analysis and segmentation within the broader Amplitude platform ecosystem.

**Bucketing Salt** works in conjunction with amplitude_id to ensure deterministic hash-based user assignment, providing consistency across sessions and platforms.

## Analytics and Measurement Framework

The Experiment Analysis system processes multiple metric types including unique conversions, event totals, property value calculations, and funnel conversions. The calculation engine handles exposure events (E), metric events (M), event totals (T), property sums (S), and property averages (A) to generate comprehensive insights.

Funnel analysis capabilities track chronological event progression using FM (funnel users) and FT (funnel totals) metrics, enabling complex conversion path analysis while maintaining user-level granularity for detailed behavioral understanding.

## SDK Integration and API Endpoints

The platform provides multiple SDK implementations including Node.js, Go, and JVM variants supporting both remote and local evaluation modes. Core API methods include:

- `fetch()` and `evaluate()` for variant assignment and user bucketing
- `variant()` for accessing assigned variants and triggering exposure tracking
- Automatic event tracking with configurable exposure tracking providers

Event deduplication is managed through insert_id mechanisms, and the system integrates with Monthly Tracked Users (MTU) billing models while efficiently managing event volume.

## Ecosystem Integration and Behavioral Targeting

Amplitude Experiment integrates deeply with the broader Amplitude analytics platform, utilizing behavioral cohorts computed hourly and supporting dynamic targeting based on user properties and historical behavior patterns. The platform handles complex dependency chains between flags and experiments while maintaining performance through intelligent caching strategies.

The system automatically assigns experiment user properties for downstream analysis within the Amplitude ecosystem, enabling sophisticated segmentation and behavioral analysis workflows. This integration supports both server-side and client-side experiment implementations while maintaining data consistency across the entire analytics pipeline.