Amplitude Experiment is a comprehensive behavioral experimentation and feature flag platform that enables teams to run A/B tests and manage progressive feature delivery through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and feature flagging for controlled rollouts without metrics tracking.

## Core Product Architecture

Amplitude Experiment operates on a hierarchical data model consisting of organizations, projects, deployments, flags/experiments, variants, and users. The platform offers two primary evaluation modes:

**Remote Evaluation** provides advanced targeting capabilities by fetching variants from Amplitude's evaluation servers, supporting Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership targeting, and user enrichment using historical analytics data. This mode enables sticky bucketing and comprehensive user targeting but requires network requests.

**Local Evaluation** runs evaluation logic directly within SDKs, achieving sub-millisecond performance by eliminating network requests. However, it has limited targeting capabilities compared to remote evaluation, lacking Amplitude ID resolution, user enrichment, and sticky bucketing features.

## Key Features and Capabilities

The platform supports comprehensive **A/B testing workflows** including hypothesis formation, metric selection (primary success metrics, secondary metrics, and guardrail metrics), variant creation with JSON payloads, and statistical analysis using either sequential testing or T-test models. Experiments can be bucketed by user, organization, or custom identifiers like company_id through the Accounts add-on.

**Feature flags** enable progressive feature delivery without metrics tracking, allowing teams to control feature rollouts through variant assignment and payload-based configuration. Both client-side and server-side deployments are supported with distinct deployment keys and evaluation modes.

**Cohort targeting** integrates with Amplitude Analytics through hourly synchronization, supporting both static and dynamic user cohorts. Remote evaluation syncs to the Amplitude Experiment destination, while local evaluation requires the Experiment Local Evaluation destination and server-side SDK support with user IDs.

## Statistical Analysis and Measurement

The **Analysis view** provides comprehensive statistical measurements including relative performance lift calculations, confidence intervals, statistical significance (p-values), and absolute value conversions. The platform supports advanced statistical techniques like CUPED variance reduction, sequential testing, and Bonferroni correction for multiple comparison adjustments.

**Dimensional Analysis** enables filtering of QA users and internal traffic from experiment results through the "Exclude testers" functionality, allowing teams to analyze clean user segments and maintain statistical significance.

## Implementation and Evaluation Process

The evaluation implementation follows a systematic process: pre-targeting steps (activation checks, flag dependencies, individual inclusions, sticky bucketing), targeting segment evaluation, and consistent bucketing using murmur3 hashing with bucketing salt and allocation percentages. This ensures consistent variant assignment and prevents sample ratio mismatch (SRM) issues.

**Exposure tracking** captures user interactions with feature flag variants through the Analytics REST API v2.0 or automatic tracking via client-side Experiment SDKs, enabling comprehensive exposure analysis and experiment evaluation.

## Key Nomenclature and Definitions

- **Deployment Keys**: Authentication tokens for client-side and server-side deployments
- **Bucketing Units**: The entity level for experiment assignment (user_id, device_id, group properties)
- **Variants**: Experiment treatments with value and payload properties in kebab-case formatting
- **Allocation**: Percentage of users exposed to experiments
- **Payload Variables**: JSON configuration data passed with variant assignments
- **Assignment Events**: Server-side tracking of variant assignments
- **Exposure Events**: Client-side tracking of user interactions with variants
- **Mutual Exclusion Groups**: Preventing users from participating in conflicting experiments
- **Holdout Groups**: Control populations excluded from experiments
- **SUTVA**: Stable Unit Treatment Value Assumption for experiment validity

## API Endpoints and Integration

The platform provides REST API endpoints for experiment management, with the Management API offering Enterprise customers the ability to bypass project-level permissions. SDK initialization is supported across multiple platforms (TypeScript/JavaScript, Kotlin, Objective-C, Swift, Python) with configuration options for tracking, session management, and event handling.

Key API methods include `experiment.start()` for initialization, `experiment.variant()` for variant retrieval, and `client.track()` for event tracking with BaseEvent class structure requiring user_id/device_id and supporting event_properties objects.

## Permissions and Governance

The platform implements **project-level permissions** independent from Analytics permissions, featuring role-based access control (Viewer, Member, Manager, Admin) and flag-level access controls for restricting experiment editing capabilities. **Notifications** can be configured through Slack channels and webhooks with alert scoping by project, deployment, or tags for flag lifecycle events.

## Broader Ecosystem Integration

Amplitude Experiment integrates deeply with the Amplitude Analytics ecosystem, leveraging historical user data for enrichment, cohort membership, and ID resolution. The platform supports the Accounts add-on for organization-level bucketing and maintains consistency with Amplitude's broader data model and user identification systems.