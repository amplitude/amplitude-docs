# Amplitude Experiment: Product Overview

## High-Level Overview

Amplitude Experiment is a behavioral experimentation platform that enables product teams to conduct A/B testing, implement progressive feature delivery, and create dynamic in-product experiences through feature flags. The platform supports two primary workflows: experimentation (with hypotheses, metrics, variants, and statistical analysis) and feature flagging (controlled feature rollouts without metrics analysis).

The platform offers two evaluation modes to meet different performance needs:
- **Remote evaluation**: Server-based evaluation with advanced targeting capabilities
- **Local evaluation**: In-SDK evaluation for sub-millisecond performance in latency-sensitive applications

## Product Relationships and Architecture

Amplitude Experiment's data model consists of a hierarchical structure:

1. **Organizations**: Top-level containers that house projects
2. **Projects**: Containers for flags, experiments, and deployments
3. **Deployments**: Environments (e.g., development, production) with unique keys
4. **Flags**: Feature toggles that can be turned on/off or used for experiments
5. **Experiments**: Structured tests with variants, metrics, and analysis
6. **Variants**: Different versions of a feature being tested
7. **Users**: Identified by User ID, Device ID, or other properties

The platform integrates with the broader Amplitude ecosystem:
- **Analytics Integration**: Tracks exposure events to the Analytics platform
- **Cohort Integration**: Uses Amplitude cohorts for targeting experiment audiences
- **User Properties**: Leverages user properties from Amplitude for targeting and analysis
- **Permissions System**: Offers project-level permissions that can be managed separately from Analytics permissions

## Key Nomenclature and Definitions

- **p-value**: Statistical measure indicating the probability of observing results by chance
- **Confidence interval**: Range of values likely to contain the true effect size
- **CUPED**: Controlled-experiment Using Pre-Experiment Data, a variance reduction technique
- **Allocation**: Percentage of users included in an experiment
- **Variants**: Different versions of a feature being tested (control vs. treatment)
- **Success metrics**: Primary metrics used to determine experiment success
- **Guardrail metrics**: Secondary metrics monitored to ensure no negative impacts
- **Sequential testing**: Statistical approach that allows for early stopping
- **Type 1 error**: False positive (incorrectly rejecting null hypothesis)
- **Type 2 error**: False negative (incorrectly accepting null hypothesis)
- **Minimum detectable effect**: Smallest effect size that can be reliably detected
- **Exposure event**: Analytics event tracking when a user is exposed to a variant
- **Cohort Targeting**: Targeting users based on behavioral segments
- **Dimensional Analysis**: Analyzing experiment impacts on specific user segments
- **Sticky bucketing**: Ensuring consistent user experiences across sessions

## Ecosystem Integration

Amplitude Experiment is designed to work seamlessly within the Amplitude product ecosystem:

1. It leverages Amplitude Analytics for tracking exposure events and analyzing experiment results
2. It uses Amplitude's user identification and property system for targeting
3. It can target users based on Amplitude cohorts (both static and dynamic)
4. It maintains its own permissions system that can be managed independently from Analytics permissions
5. Enterprise customers can set flag-level access controls with role-based permissions

## API and SDK Implementation

### SDK Initialization

```typescript
// TypeScript
import * as amplitude from '@amplitude/analytics-browser';

amplitude.init('API_KEY', 'user@company.com', {
  defaultTracking: new amplitude.DefaultTrackingOptions(
    true, // sessions
    true, // pageViews
    true, // formInteractions
    true, // fileDownloads
  )
});
```

### Exposure Tracking API

```javascript
// Manual exposure tracking via Analytics REST API v2.0
{
  "api_key": "API_KEY",
  "events": [
    {
      "user_id": "user@company.com",
      "event_type": "$exposure",
      "event_properties": {
        "flag_key": "flag-key",
        "variant": "on"
      }
    }
  ]
}
```

The evaluation process follows a specific sequence:
1. **Pre-targeting**: Checks activation conditions, dependencies, inclusions, and sticky bucketing
2. **Targeting segments**: Evaluates if users match defined segments
3. **Consistent bucketing**: Uses murmur3 hashing algorithm to determine allocation and variant assignment

SDKs are available for multiple platforms including JavaScript/TypeScript, HTML, Kotlin, Objective-C, Swift, and Python, with similar initialization patterns across all platforms.