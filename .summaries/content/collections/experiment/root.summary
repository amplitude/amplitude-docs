Amplitude Experiment is a comprehensive behavioral experimentation and feature flag management platform that enables organizations to conduct A/B tests and manage feature rollouts through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and progressive feature delivery without metrics tracking.

## Core Architecture and Evaluation Modes

The platform operates through two primary evaluation modes: **remote evaluation** and **local evaluation**. Remote evaluation fetches variants from Amplitude's evaluation servers, providing advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership targeting, and user enrichment using historical analytics data. Local evaluation runs the evaluation logic directly in the SDK, delivering sub-millisecond performance but with limited targeting capabilities - it cannot perform Amplitude ID resolution, user enrichment, or sticky bucketing.

The system uses a hierarchical data model structured around organizations, projects, deployments (with client-side and server-side types using deployment keys), flags/experiments, variants (containing values and payloads), and users (requiring user_id/device_id with optional group support through the Accounts add-on).

## Experimentation Workflow and Statistical Analysis

Amplitude Experiment supports two statistical models: **sequential testing** (default) and **T-test** options. The experimentation workflow encompasses hypothesis formation, metric selection, variant creation with treatment groups and controls, user allocation through consistent bucketing using the murmur3 hashing algorithm with bucketing salts, and comprehensive statistical analysis.

The Analysis view provides detailed statistical evaluation through relative performance calculations, confidence intervals, significance testing with p-values, and absolute value measurements. The platform distinguishes between recommendation metrics and secondary metrics, tracking unique conversions percentages and aggregate events per exposed user for experiment success evaluation.

## Targeting and User Segmentation

The platform implements sophisticated targeting through multiple mechanisms including cohort targeting (supporting both static and dynamic cohorts with hourly sync intervals), individual inclusions, and targeting segments. The evaluation process follows a structured pre-targeting workflow involving flag activation, flag dependencies, mutual exclusion groups, holdout groups, and sticky bucketing before applying targeting segments.

For Enterprise customers, **Dimensional Analysis** enables filtering of QA users and internal traffic from experiment analysis by defining test users in Targeting settings and using exclusion options. The system also supports **project-level permissions** that operate independently from Analytics permissions, including role-based access controls (Viewer, Member, Manager, Admin) and flag-level access restrictions.

## Key Nomenclature and Definitions

- **Deployment Keys**: Authentication tokens for client-side and server-side deployments
- **Flag Key**: Unique identifier for feature flags in kebab-case format
- **Variants**: Experiment variations containing both value and payload properties
- **Bucketing Unit**: The entity used for consistent user assignment (user_id, device_id, or groups)
- **Allocation**: Percentage of users exposed to an experiment
- **Exposure Events**: Tracking events fired when users encounter feature flag variants
- **Assignment Events**: Events fired when users are bucketed into variants
- **Payload Variables**: JSON configuration data passed with variants
- **CUPED**: Controlled-experiment Using Pre-Experiment Data for variance reduction
- **Guardrail Metrics**: Secondary metrics monitoring for negative impacts
- **Sample Ratio Mismatch (SRM)**: Statistical anomaly in variant distribution

## SDK Integration and API Endpoints

The platform provides multi-platform SDKs supporting TypeScript, Kotlin, Objective-C, Swift, and Python with consistent initialization patterns. SDKs support configuration options including defaultTracking settings, session management (minTimeBetweenSessionsMillis), and automatic exposure tracking capabilities.

Key API integrations include:
- **Evaluation REST API**: For fetching variant assignments
- **Analytics REST API v2.0**: For exposure tracking with curl requests
- **Management API**: For programmatic flag and experiment management (bypasses project-level permissions)

## Enterprise Features and Governance

The platform includes advanced governance features for Growth and Enterprise customers through **Experiment Approvals**, which implements approval workflows for feature flags and experiments. This includes both peer approvals and specific approver configurations, with approval requirements for activation/scheduling and critical live changes to target segments, variants, distribution, exposure events, bucketing salt, and sticky bucketing.

**Notifications** can be configured through Slack channels and webhooks for experiment alerts, including flag lifecycle events (created/updated/deleted) and sample-ratio mismatch detection. Alert scoping is available by project, deployment, or tags with webhook schema configuration using signing keys for verification.

## Broader Product Ecosystem Integration

The system integrates deeply with the broader Amplitude ecosystem through cohort synchronization, user property enrichment from historical analytics data, and Amplitude ID resolution for comprehensive user targeting and analysis capabilities. This integration enables organizations to leverage their existing analytics infrastructure for more sophisticated experimentation and feature management, creating a unified approach to product optimization and user experience management.