# Amplitude Experiment: Product Overview

## High-Level Overview

Amplitude Experiment is a behavioral experimentation platform that enables product teams to conduct A/B testing, implement progressive feature delivery, and create dynamic in-product experiences without requiring code changes. The platform uses feature flags to control product behavior and supports two primary workflows: experimentation (with metrics analysis) and feature flagging (controlled rollouts without analysis).

The platform offers two evaluation modes to meet different performance needs:
- **Remote Evaluation**: Server-based evaluation with advanced targeting capabilities
- **Local Evaluation**: In-SDK evaluation for sub-millisecond performance in latency-sensitive applications

## Product Relationships and Architecture

Amplitude Experiment follows a hierarchical data model:
- **Organizations** contain projects
- **Projects** contain deployments, flags, and experiments
- **Flags** contain variants
- **Experiments** are specialized flags with metrics and analysis capabilities

The platform integrates with the broader Amplitude ecosystem:
1. **Amplitude Analytics**: 
   - Provides user properties and cohorts for targeting
   - Receives exposure tracking data
   - Powers experiment results analysis

2. **Amplitude SDKs**:
   - Client-side SDKs (web, mobile)
   - Server-side SDKs (backend)
   - Handle variant assignment and exposure tracking

## Key Nomenclature and Definitions

### Experimentation Terms
- **Variants**: Different versions of a feature being tested (control vs. treatment)
- **Allocation**: Percentage of users assigned to each variant
- **Bucketing**: Process of assigning users to variants using murmur3 hashing
- **p-value**: Statistical measure of significance in experiment results
- **Confidence Interval**: Range of likely true values for a metric
- **CUPED**: Controlled-experiment Using Pre-Experiment Data, a variance reduction technique
- **Sequential Testing**: Statistical approach allowing for early stopping
- **Type 1 Error**: False positive (incorrectly rejecting null hypothesis)
- **Type 2 Error**: False negative (incorrectly accepting null hypothesis)
- **Minimum Detectable Effect**: Smallest effect size that can be reliably detected

### Platform-Specific Terms
- **Deployments**: Environments for flags (e.g., development, production)
- **Exposure Events**: Records of users seeing a specific variant
- **Sticky Bucketing**: Ensuring users consistently see the same variant
- **Targeting Segments**: Rules defining which users see which variants
- **Cohort Targeting**: Using Amplitude cohorts for targeting experiment variants

## Evaluation Process and Targeting

The evaluation process follows these steps:
1. **Pre-targeting**:
   - Activation conditions
   - Dependencies on other flags
   - Inclusions for specific users
   - Sticky bucketing checks

2. **Targeting Segment Evaluation**:
   - User property matching
   - Cohort membership verification
   - IP geolocation (remote only)
   - Behavioral targeting (remote only)

3. **Allocation and Bucketing**:
   - Consistent assignment using murmur3 hashing
   - Percentage-based allocation to variants

4. **Variant Assignment**:
   - Return variant configuration to application

## API Endpoints and Implementation

### SDK Initialization
Amplitude Experiment provides SDKs for multiple platforms:
- TypeScript/JavaScript
- HTML
- Kotlin
- Objective-C
- Swift
- Python

Example initialization (TypeScript):
```typescript
import * as amplitude from '@amplitude/analytics-browser';

amplitude.init('API_KEY', {
  defaultTracking: {
    sessions: true,
    pageViews: true,
    formInteractions: true,
    fileDownloads: true
  }
});
```

### Exposure Tracking API
Manual exposure tracking via Analytics REST API:
```
POST https://api2.amplitude.com/2/httpapi
```

### Evaluation Modes
1. **Remote Evaluation**:
   - Makes requests to evaluation servers
   - Supports advanced targeting including Amplitude ID resolution, IP geolocation, property canonicalization, behavioral cohort targeting, and historical user properties
   - Higher latency but more powerful targeting

2. **Local Evaluation**:
   - Runs evaluation logic directly in the SDK
   - Sub-millisecond performance
   - Limited targeting capabilities
   - Requires flag configuration downloads

## Permission Management
- Project-level user permissions
- Flag-level access controls (Enterprise feature)
- Role-based permissions:
  - Viewer: Read-only access
  - Member: Can edit flags but not create
  - Manager: Can create and edit flags
  - Admin: Full control including user management