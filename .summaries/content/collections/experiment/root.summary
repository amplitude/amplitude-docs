Amplitude Experiment is a comprehensive behavioral experimentation and feature flag platform that enables organizations to run A/B tests and manage progressive feature delivery through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and feature flagging for controlled rollouts without metrics tracking.

## Core Architecture and Evaluation Modes

The platform operates through two primary evaluation modes: **remote evaluation** and **local evaluation**. Remote evaluation fetches variants from Amplitude's evaluation servers, providing advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership targeting, and user enrichment using historical analytics data. Local evaluation runs the evaluation logic directly in the SDK, delivering sub-millisecond performance but with limited targeting capabilities - it cannot perform Amplitude ID resolution, user enrichment, or sticky bucketing.

The hierarchical data model consists of organizations containing projects, which house deployments (client-side or server-side with deployment keys), flags/experiments with variants (containing value/payload properties), and users identified by user_id/device_id with optional group properties for Accounts add-on customers.

## Experimentation Workflow and Statistical Analysis

Amplitude Experiment supports comprehensive A/B testing workflows beginning with hypothesis formation, metric selection (primary success, secondary success, and guardrail metrics), treatment variant creation, and user allocation. The platform employs two statistical models: sequential testing and traditional T-test analysis. Key statistical concepts include minimum detectable effect (MDE), target lift calculations, confidence intervals, p-values, and CUPED variance reduction for improved statistical power.

The Analysis view provides detailed statistical measurements including relative performance lift calculations, confidence intervals, statistical significance indicators, and absolute conversion percentages. The platform includes dimensional analysis capabilities for filtering QA users and internal traffic from experiment results.

## Targeting and User Segmentation

User targeting operates through rule-based segments with support for cohort targeting (both static and dynamic cohorts with hourly sync intervals), individual inclusions, and audience segmentation. The evaluation implementation follows a structured process: pre-targeting steps (activation, flag dependencies, individual inclusions, sticky bucketing), targeting segments evaluation, and consistent bucketing using murmur3 hashing with bucketing salt and allocation percentages.

Bucketing units can be configured at user, organization, or custom company_id levels (with Accounts add-on), following SUTVA (Stable Unit Treatment Value Assumption) principles to prevent interference between experimental units.

## Key Nomenclature and Definitions

- **Variants**: Different versions of a feature with value/payload properties, formatted in kebab-case
- **Allocation**: Percentage of users exposed to an experiment
- **Exposure Events**: Tracked when users encounter feature flag variants
- **Assignment Events**: Recorded when users are bucketed into variants
- **Payload Variables**: JSON configuration data passed with variants
- **Bucketing Salt**: Ensures consistent user assignment across experiments
- **Mutual Exclusion Groups**: Prevent users from participating in conflicting experiments
- **Holdout Groups**: Control groups excluded from feature exposure
- **Sticky Bucketing**: Maintains consistent variant assignment for returning users

## SDK Integration and API Endpoints

The platform provides multi-platform SDK support including TypeScript/JavaScript, Kotlin, Objective-C, Swift, and Python with configuration options for session management, event tracking, and automatic exposure tracking. Key API endpoints include the Evaluation REST API for variant fetching and Analytics REST API v2.0 for exposure tracking.

Client-side SDKs offer automatic exposure tracking capabilities, while server-side implementations require manual exposure event tracking. The Management API provides programmatic access to experiment configuration with Enterprise-level permissions bypass capabilities.

## Notifications and Monitoring

The platform includes comprehensive notification systems supporting Slack channel alerts and webhook notifications for flag lifecycle events (created/updated/deleted), sample-ratio mismatch (SRM) detection, and experiment status changes. Webhook configurations include signing key verification and alert scoping by project, deployment, or tags.

## Permissions and Access Control

Amplitude Experiment implements project-level permissions independent from Analytics permissions, featuring role-based access control (Viewer, Member, Manager, Admin) and flag-level access controls for restricting experiment editing capabilities. Enterprise customers can utilize Management API permissions bypass for programmatic access.

## Product Ecosystem Integration

The platform integrates cohort targeting through server-side SDKs with CohortSyncConfig requiring API and secret keys, enabling sophisticated audience targeting based on user behavior patterns from Amplitude Analytics data. This creates a unified experimentation ecosystem where behavioral analytics data directly informs experiment targeting and user segmentation, allowing organizations to leverage their existing Amplitude Analytics investment for more precise experimentation workflows.