Amplitude Experiment is a comprehensive behavioral experimentation platform that enables organizations to run A/B tests and deploy feature flags through a workflow-driven approach. The platform supports both remote and local evaluation modes, with remote evaluation providing enhanced user targeting capabilities through Amplitude ID resolution, IP geolocation, and cohort membership, while local evaluation offers sub-millisecond performance by running evaluation logic directly in SDKs.

## Product Architecture and Feature Relationships

The platform operates on a hierarchical data model consisting of organizations, projects, deployments, flags/experiments, variants, and users. Deployments serve as the primary configuration layer, with distinct client-side and server-side deployment types that use deployment keys for SDK authentication. Each deployment supports both remote evaluation (network-based variant fetching with user enrichment) and local evaluation (SDK-based evaluation with limited targeting capabilities).

Feature flags and experiments share the same underlying infrastructure but serve different purposes - flags enable progressive feature delivery and configuration management, while experiments facilitate hypothesis-driven A/B testing with statistical analysis. Both utilize a sophisticated bucketing algorithm based on murmur3 hashing to ensure consistent user assignment across variants, with support for allocation percentages, variant distribution weights, and bucketing salts for randomization control.

The evaluation process follows a multi-stage pipeline including pre-targeting steps (activation checks, flag dependencies, individual inclusions, sticky bucketing), targeting segment evaluation, and final variant assignment through consistent bucketing. This ensures users receive stable variant assignments while supporting complex targeting scenarios including cohort membership, user properties, and geographic targeting.

## Key Nomenclature and Definitions

**Evaluation Modes**: Remote evaluation fetches variants from Amplitude's servers with full user enrichment capabilities, while local evaluation runs bucketing logic in SDKs for faster performance but limited targeting options.

**Bucketing Components**: The bucketing unit (user_id, device_id, or custom identifiers) determines assignment granularity, while bucketing keys and salts ensure consistent randomization. Allocation controls what percentage of users enter the experiment, and variant distribution weights determine how allocated users split between treatment variants.

**Targeting and Segmentation**: Targeting segments define user criteria for experiment participation, while cohort targeting enables audience-based experiments using both static and dynamic user cohorts. Individual inclusions allow manual user assignment overrides.

**Statistical Framework**: The platform defaults to sequential testing over traditional T-tests, supporting concepts like minimum detectable effect (MDE), statistical power, confidence intervals, and multiple comparison corrections through Bonferroni adjustment. CUPED variance reduction techniques improve statistical sensitivity.

**Event Types**: Assignment events track when users are bucketed into variants, exposure events record when users actually encounter the experimental treatment, and enrollment events mark experiment participation. These events feed into the Analysis view for statistical evaluation.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude Analytics ecosystem, leveraging user properties, behavioral cohorts, and historical analytics data for enhanced targeting capabilities. The platform supports the Accounts add-on for organization-level bucketing and group-based experiments, extending beyond individual user experimentation.

Cohort targeting requires specific destination configurations - the standard "Amplitude Experiment" destination for remote evaluation, or the "Experiment Local Evaluation" destination for server-side local evaluation with hourly sync capabilities. This integration enables sophisticated audience targeting based on user behavior patterns and segment membership.

The platform includes comprehensive notification systems supporting Slack channels and webhook integrations for experiment lifecycle events, sample ratio mismatch detection, and flag configuration changes. Project-level permissions operate independently from Analytics permissions, with role-based access controls (Viewer, Member, Manager, Admin) and Enterprise-level flag-specific access restrictions.

## API Endpoints and Implementation Details

The platform provides multiple SDK implementations across TypeScript, Kotlin, Objective-C, Swift, and Python, with consistent initialization patterns using deployment keys and configuration objects. Client-side SDKs support automatic exposure tracking, while server-side implementations require manual exposure event generation.

Key API integrations include the Analytics REST API v2.0 for exposure tracking via curl requests, and the Experiment Management API for programmatic permission and configuration management. The platform supports both client-side and server-side evaluation modes, with client-side implementations including built-in session management and event tracking capabilities.

The Analysis view provides comprehensive statistical evaluation through relative performance calculations, confidence interval displays, significance testing, and both recommendation metrics and secondary metric analysis. Dimensional Analysis features enable QA user filtering and internal traffic exclusion through the "Exclude testers" functionality, ensuring clean experimental results by removing non-representative user segments from statistical calculations.