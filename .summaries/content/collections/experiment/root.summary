# Amplitude Experiment

## Product Overview

Amplitude Experiment is a behavioral experimentation platform that combines A/B testing and feature flagging capabilities to help product teams make data-driven decisions. The platform enables two primary workflows:

1. **Experimentation**: Creating controlled experiments with hypotheses, metrics, variants, and statistical analysis
2. **Feature Flagging**: Implementing controlled feature rollouts without metrics analysis for progressive delivery and risk mitigation

Amplitude Experiment delivers sub-millisecond performance through local evaluation for latency-sensitive applications while also offering advanced targeting capabilities through remote evaluation when needed.

## Product Relationships and Architecture

### Organizational Structure
- **Organizations**: Top-level container for all Amplitude products
- **Projects**: Contain flags, experiments, and deployments
- **Deployments**: Represent different environments (development, staging, production)
  - Client-side deployments: For web/mobile applications
  - Server-side deployments: For backend services

### Core Components
- **Flags**: Feature toggles that control functionality
- **Experiments**: Structured tests with variants and metrics
- **Variants**: Different versions of a feature being tested
- **Users**: Identified by user IDs and properties for targeting and bucketing

### Evaluation Modes
1. **Local Evaluation**: 
   - Runs evaluation logic directly in the SDK without network requests
   - Provides sub-millisecond performance
   - Supports consistent bucketing and targeting segments
   - Lacks Amplitude ID resolution and user enrichment capabilities

2. **Remote Evaluation**:
   - Makes requests to evaluation servers to fetch variants
   - Supports advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, behavioral cohort targeting, and historical user properties

### Integration with Amplitude Ecosystem
- Uses Amplitude Analytics for tracking exposure events
- Leverages Amplitude's user identification system for consistent targeting
- Utilizes Amplitude Cohorts for advanced targeting capabilities

## Key Nomenclature and Definitions

- **Experimentation**: Creating controlled experiments with hypotheses, metrics, variants, and statistical analysis
- **Feature Flagging**: Implementing controlled feature rollouts without metrics analysis
- **Flags**: Feature toggles that control functionality
- **Variants**: Different versions of a feature being tested
- **Deployments**: Different environments (development, staging, production)
- **Bucketing**: The process of assigning users to experiment variants
- **Sticky Bucketing**: Maintains user assignment to variants over time
- **Bucketing Salt**: Ensures consistent user assignment across experiments
- **Exposure Tracking**: Recording when users are exposed to experiment variants
- **Cohort Targeting**: Using static or dynamic user cohorts for audience targeting
- **CUPED** (Controlled-experiment Using Pre-Experiment Data): Statistical method to reduce variance in metrics
- **Sequential Testing**: Statistical approach allowing continuous monitoring of results

## Evaluation Process

The platform uses a sophisticated evaluation process that includes:
1. Pre-targeting steps (activation, dependencies, inclusions, sticky bucketing)
2. Targeting segments evaluation
3. Consistent bucketing using murmur3 hashing
4. Allocation and variant distribution calculations

## SDK Support and Implementation

### Supported Languages
- TypeScript/JavaScript
- HTML
- Kotlin
- Objective-C
- Swift
- Python

### SDK Initialization Examples

```typescript
// TypeScript example
import * as amplitude from '@amplitude/analytics-browser';

amplitude.init('API_KEY', {
  defaultTracking: {
    sessions: true,
    pageViews: true,
    formInteractions: true,
    fileDownloads: true
  }
});
```

```python
# Python example
from amplitude import Amplitude

amplitude = Amplitude('API_KEY')
```

## API Endpoints and Integration

- **Analytics REST API v2.0**: Used for manual exposure tracking
- **Evaluation API endpoints**: Used for remote variant fetching

## Access Control and Permissions

- Project-level permissions with different access levels:
  - **Viewer**: Read-only access
  - **Member**: Can edit flags but not create new ones
  - **Manager**: Can create and edit flags
  - **Admin**: Full access including permission management
- Enterprise customers have flag-level access controls

## Technical Considerations

- **Cohort Sync Configuration**: Required for local evaluation with cohort targeting
- **Propagation Delay**: Cohort membership updates may have hourly sync delays
- **Exposure Tracking**: Essential for reliable experiment results but optional for feature flags not requiring analysis
- **Statistical Methodology**: Includes Sequential Testing, Confidence Intervals, CUPED, Bonferroni Correction, and T-tests