Amplitude Experiment is a comprehensive behavioral experimentation platform that enables organizations to run A/B tests and deploy feature flags through a workflow-driven approach. The platform supports both remote and local evaluation modes, with remote evaluation providing enhanced user enrichment capabilities including Amplitude ID resolution, IP geolocation, and cohort targeting, while local evaluation offers sub-millisecond performance by running evaluation logic directly in SDKs.

## Product Architecture and Feature Relationships

The platform operates on a hierarchical data model consisting of organizations, projects, deployments (client-side and server-side with deployment keys), flags/experiments, variants (with value/payload properties), and users. Deployments serve as the primary configuration layer, supporting both Remote and Local evaluation modes with distinct capabilities and performance characteristics.

Remote evaluation leverages Amplitude's evaluation servers to provide advanced targeting features including user enrichment from historical analytics data, behavioral cohort membership, sticky bucketing, and IP geolocation targeting. Local evaluation prioritizes performance by executing evaluation logic within SDKs, achieving sub-millisecond response times but with limited targeting capabilities - notably excluding Amplitude ID resolution, user enrichment, and sticky bucketing.

The experimentation workflow encompasses hypothesis creation, variant configuration with JSON payloads, user allocation through consistent bucketing algorithms, and statistical analysis using sequential testing as the default method over traditional T-tests. The platform implements a sophisticated bucketing algorithm using murmur3 hashing for consistent user-to-variant assignment, incorporating pre-targeting steps including flag dependencies, individual inclusions, and sticky bucketing logic.

## Key Nomenclature and Definitions

**Evaluation Modes**: Remote evaluation (server-based with user enrichment) vs Local evaluation (SDK-based for performance)

**Bucketing Components**: Bucketing units (user_id, device_id, or group identifiers), bucketing salt for randomization, allocation percentage for experiment participation, and variant distribution weights

**Experiment Elements**: Treatment variants vs control groups, payload variables for dynamic configuration, exposure events for user interaction tracking, assignment events for variant allocation logging

**Statistical Framework**: Sequential testing methodology, CUPED variance reduction, confidence intervals, minimum detectable effect (MDE), sample ratio mismatch (SRM) detection, and Bonferroni correction for multiple comparisons

**Targeting Capabilities**: User segments, cohort targeting (static and dynamic with hourly sync), individual inclusions, mutual exclusion groups, and holdout groups

**Metrics Classification**: Primary success metrics, secondary success metrics, guardrail metrics, and recommendation metrics for comprehensive experiment evaluation

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the Amplitude Analytics ecosystem, leveraging historical user data for enrichment and cohort-based targeting. The platform supports cohort synchronization through two destinations: Amplitude Experiment destination for remote evaluation and Experiment Local Evaluation destination for server-side local evaluation, both operating on hourly sync schedules.

The system maintains strong integration with Amplitude Analytics projects, utilizing user properties, behavioral data, and cohort memberships to enhance targeting precision. User identification follows Amplitude's standard model using user_id and device_id, with support for group properties through the Accounts add-on for organization-level bucketing.

## API Endpoints and Implementation

The platform provides comprehensive SDK support across multiple languages including TypeScript, Kotlin, Objective-C, Swift, and Python, with consistent initialization patterns and configuration options. Key SDKs include `@amplitude/experiment-browser` and `@amplitude/analytics-browser` for client-side implementations.

Exposure tracking utilizes the Analytics REST API v2.0 for manual event logging, while client-side Experiment SDKs provide automatic exposure tracking capabilities. The Management API enables programmatic experiment configuration and permission management.

The platform includes sophisticated notification systems supporting Slack channel integration and webhook notifications with signing key verification for experiment lifecycle events including flag creation, updates, and deletions, with alert scoping capabilities by project, deployment, or tags.

## Analysis and Permissions Framework

The Analysis view provides comprehensive statistical evaluation including relative performance calculations, confidence intervals, significance testing, and absolute value comparisons between variants and control groups. Dimensional Analysis capabilities enable filtering of QA users and internal traffic through targeting settings and the "Exclude testers" functionality.

Project-level permissions operate independently from Analytics permissions, featuring role-based access controls (Viewer, Member, Manager, Admin) with Enterprise-level flag-specific access restrictions. Permission management operates through both the Experiment UI and Management API, supporting organization-wide settings and granular deployment-level controls.