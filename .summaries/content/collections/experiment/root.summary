Amplitude Experiment is a comprehensive behavioral experimentation platform that enables organizations to conduct A/B tests and manage feature flags through a workflow-driven approach. The platform supports both client-side and server-side implementations with flexible evaluation modes, statistical analysis capabilities, and sophisticated user targeting mechanisms.

## Core Architecture and Evaluation Modes

Amplitude Experiment operates through two primary evaluation modes: **remote evaluation** and **local evaluation**. Remote evaluation fetches variants from Amplitude's evaluation servers and provides advanced capabilities including Amplitude ID resolution, IP geolocation targeting, user enrichment from historical analytics data, cohort membership targeting, and sticky bucketing. This mode supports behavioral cohorts with hourly sync capabilities and property canonicalization for consistent user targeting.

Local evaluation runs the evaluation logic directly within SDKs, achieving sub-millisecond performance by eliminating network requests. However, this mode has limitations including no Amplitude ID resolution, restricted user enrichment capabilities, and unavailable sticky bucketing. Local evaluation supports consistent bucketing and targeting segments but with reduced functionality compared to remote evaluation.

## Data Model and Organizational Structure

The platform follows a hierarchical data model structure: **Organizations** contain **Projects**, which contain **Deployments** (with client-side or server-side types and unique deployment keys), which contain **Flags/Experiments** with **Variants**. Each variant includes value and payload properties, supporting JSON payloads for complex configuration delivery.

User identification operates through user_id and device_id requirements, with group properties support available through the Accounts add-on. The system uses kebab-case and snake_case formatting conventions and integrates with Amplitude Analytics project structures for seamless data flow.

## Bucketing and Implementation Logic

Experiment implementation follows a sophisticated evaluation process using the **murmur3 hashing algorithm** with bucketing salts to ensure consistent user assignment. The evaluation process includes pre-targeting steps (activation checks, flag dependencies, individual inclusions, sticky bucketing), targeting segments evaluation, and final variant assignment through allocation and variant bucketing percentages.

The system supports **mutual exclusion groups** and **holdout groups** to prevent sample ratio mismatch (SRM) and variant jumping. Bucketing units can be configured at user, device, or group levels (with Accounts add-on), ensuring SUTVA (Stable Unit Treatment Value Assumption) compliance.

## Statistical Analysis and Metrics

The Analysis view provides comprehensive statistical evaluation including relative performance calculations, confidence intervals, statistical significance testing (p-values), and absolute value measurements. The platform supports both **sequential testing** and **T-test** statistical models for determining experiment success.

Key metrics include recommendation metrics versus secondary metrics, unique conversions percentages, and aggregate events per exposure. The system incorporates advanced statistical techniques like **CUPED variance reduction**, **Bonferroni correction** for multiple comparisons, and configurable minimum detectable effect (MDE) calculations.

## Targeting and Segmentation

Amplitude Experiment provides sophisticated targeting capabilities through **cohort targeting**, supporting both static and dynamic user cohorts. Remote evaluation enables hourly sync to the Amplitude Experiment destination, while local evaluation requires server-side SDK support with sync to the Experiment Local Evaluation destination.

**Dimensional Analysis** allows filtering of QA users and internal traffic through the "Exclude testers" functionality, enabling clean statistical analysis by removing non-representative user segments from experiment results.

## Permissions and Access Control

The platform implements **project-level permissions** that operate independently from Analytics permissions, featuring role-based access controls (Viewer, Member, Manager, Admin). Enterprise customers receive additional **flag-level access controls** for granular permission management. Permission management operates through both the Experiment UI and Management API.

## SDK Integration and Tracking

Multi-platform SDK support includes TypeScript, Kotlin, Objective-C, Swift, and Python implementations with consistent configuration options including defaultTracking settings, session management, and event tracking capabilities. **Exposure tracking** operates through the Analytics REST API v2.0 and client-side SDKs with automatic exposure tracking.

The system generates **exposure events** (client-side) and **assignment events** (server-side) for comprehensive experiment monitoring and analysis through the Exposures chart and user exposure analysis tools.

## API Endpoints and Configuration

Key API integrations include:
- **Analytics REST API v2.0** for exposure tracking
- **Experiment Management API** for permission and configuration management
- **CohortSyncConfig** for cohort targeting setup with API key and secret key authentication
- **REST API** integration alongside SDK implementations for variant fetching

The platform supports both **Amplitude Experiment destination** and **Experiment Local Evaluation destination** configurations depending on evaluation mode requirements, with hourly sync capabilities and propagation delay considerations for cohort targeting implementations.