Amplitude Experiment is a comprehensive behavioral experimentation and feature flag platform that enables organizations to run A/B tests and manage progressive feature delivery through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and feature flagging for controlled rollouts without metrics tracking.

## Core Architecture and Data Model

The platform operates on a hierarchical data model consisting of organizations, projects, deployments, flags/experiments, variants, and users. Deployments are categorized as either client-side or server-side, each with unique deployment keys for SDK configuration. Flags and experiments can operate in two evaluation modes: local evaluation (running logic within the SDK for sub-millisecond performance) and remote evaluation (leveraging server-side capabilities for advanced targeting).

The system supports multiple bucketing units including user_id, device_id, and organization-level identifiers (available with the Accounts add-on). Variants contain both value and payload properties, with payloads supporting JSON structures for complex feature configurations. Users are identified through user_id/device_id requirements with optional group properties for organizational bucketing.

## Evaluation Modes and Implementation

**Local Evaluation** runs evaluation logic directly in the SDK, providing sub-millisecond performance through consistent bucketing algorithms. However, it has limitations including no Amplitude ID resolution, user enrichment, or sticky bucketing capabilities. Flag configurations are polled periodically, and client-side implementations risk data leakage by exposing targeting logic.

**Remote Evaluation** fetches variants from Amplitude's evaluation servers, offering advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership targeting, and user enrichment using historical analytics data. This mode supports sticky bucketing and handles race conditions more effectively.

The evaluation process follows a systematic approach: activation checks, flag dependencies evaluation, individual inclusions processing, sticky bucketing (remote only), targeting segment evaluation, and finally consistent bucketing using murmur3 hashing with bucketing salt and allocation percentages.

## Experimentation Workflow and Statistical Analysis

Amplitude Experiment supports comprehensive A/B testing workflows beginning with hypothesis formation, metric selection (primary success metrics, secondary metrics, and guardrail metrics), variant creation with treatment and control groups, and statistical analysis. The platform offers two statistical models: sequential testing for continuous monitoring and traditional T-test approaches.

The Analysis view provides detailed statistical measurements including relative performance lift calculations, confidence intervals, statistical significance (p-values), and absolute value conversions. Key statistical concepts include CUPED variance reduction, Bonferroni correction for multiple comparisons, minimum detectable effect (MDE) calculations, and sample ratio mismatch (SRM) detection.

## Targeting and Segmentation

The platform provides sophisticated targeting capabilities through multiple mechanisms:

- **Cohort Targeting**: Integration with Amplitude Analytics cohorts, supporting both static and dynamic cohorts with hourly sync intervals
- **Individual Inclusions**: Direct user targeting through user IDs or device IDs
- **Targeting Segments**: Complex audience definitions using user properties and behavioral data
- **Dimensional Analysis**: Filtering capabilities to exclude QA users and internal traffic from experiment analysis

Cohort targeting requires specific SDK configuration for local evaluation, including CohortSyncConfig with API and secret keys for server-side SDKs.

## Key Nomenclature and Definitions

- **Assignment Events**: Initial user bucketing into variants
- **Exposure Events**: User interaction with feature flag variants, tracked for analysis
- **Payload Variables**: JSON-structured configuration data within variants
- **Bucketing Salt**: Randomization seed ensuring consistent user bucketing
- **Allocation**: Percentage of users included in experiments
- **Mutual Exclusion Groups**: Preventing user overlap between conflicting experiments
- **Holdout Groups**: Control populations excluded from multiple experiments
- **Sticky Bucketing**: Maintaining consistent variant assignment across sessions

## API Endpoints and Integration

The platform provides multiple integration points:

- **Evaluation REST API**: For fetching variant assignments using project API keys
- **Analytics REST API v2.0**: For tracking exposure events with curl requests
- **Management API**: Enterprise-level programmatic experiment management with permissions bypass capabilities
- **SDK Integration**: Multi-platform support including TypeScript/JavaScript, Kotlin, Objective-C, Swift, and Python with configuration options for session management and event tracking

## Permissions and Access Control

Amplitude Experiment implements project-level permissions independent from Analytics permissions, featuring role-based access control with four levels: Viewer, Member, Manager, and Admin. The system supports flag-level access controls for restricting experiment editing capabilities and provides Enterprise customers with Management API bypass functionality for programmatic access.

## Notifications and Monitoring

The platform includes comprehensive notification systems supporting Slack channel integration and webhook notifications for flag lifecycle events (created/updated/deleted), sample ratio mismatch alerts, and experiment status changes. Notifications can be scoped by project, deployment, or tags, with webhook schema configuration supporting signing key verification for security.

## Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude ecosystem, leveraging historical analytics data for user enrichment, cohort definitions, and advanced targeting while maintaining the flexibility to operate independently through local evaluation modes for performance-critical applications. The platform serves as a bridge between Amplitude's analytics capabilities and feature delivery, enabling data-driven experimentation workflows that utilize existing user behavioral data for sophisticated targeting and analysis.