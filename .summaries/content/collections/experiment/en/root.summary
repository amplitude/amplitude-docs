Amplitude Experiment is a comprehensive behavioral experimentation platform that enables organizations to conduct A/B tests and deploy feature flags through a workflow-driven approach. The platform supports both remote and local evaluation modes, with remote evaluation providing enhanced user targeting capabilities through Amplitude ID resolution, IP geolocation, and cohort membership, while local evaluation offers sub-millisecond performance by running evaluation logic directly in SDKs.

## Product Architecture and Feature Relationships

The platform operates on a hierarchical data model consisting of organizations, projects, deployments, flags/experiments, variants, and users. Deployments serve as the core organizational unit, available in both client-side and server-side configurations with unique deployment keys for SDK authentication. Each deployment can contain multiple feature flags and experiments that utilize consistent bucketing algorithms based on murmur3 hashing to ensure reliable variant assignment.

The evaluation system supports two primary modes: remote evaluation leverages Amplitude's servers for user enrichment and advanced targeting capabilities including cohort membership and sticky bucketing, while local evaluation runs entirely within SDKs for optimal performance but with limited targeting features. Both modes support targeting segments, individual inclusions, and consistent bucketing to maintain experiment integrity.

Experiment analysis is conducted through a dedicated Analysis view that provides statistical measurements including relative performance lift, confidence intervals, p-values, and both recommendation and secondary metrics. The platform defaults to sequential testing over traditional T-tests for improved statistical accuracy and supports dimensional analysis for filtering out QA users and internal traffic.

## Key Nomenclature and Definitions

**Bucketing Units**: The entity level for randomization (user_id, device_id, or group identifiers with Accounts add-on)
**Allocation**: The percentage of users exposed to an experiment
**Variants**: Different versions of a feature with associated values and JSON payloads
**Exposure Events**: Tracked when users encounter a feature flag variant
**Assignment Events**: Server-side tracking of variant assignments
**CUPED**: Controlled-experiment Using Pre-Experiment Data for variance reduction
**Sequential Testing**: Statistical methodology that allows continuous monitoring without inflating Type 1 error rates
**Sticky Bucketing**: Ensures users remain in the same variant across sessions (remote evaluation only)
**Minimum Detectable Effect (MDE)**: The smallest change in metrics that an experiment can reliably detect
**Guardrail Metrics**: Secondary metrics monitored to ensure experiments don't negatively impact key business indicators

## Product Ecosystem Integration

Amplitude Experiment integrates tightly with Amplitude Analytics, sharing project structures and leveraging historical user data for enhanced targeting. The platform supports cohort targeting through hourly synchronization with Amplitude destinations, enabling both static and dynamic user segments. For local evaluation, cohorts sync to the Experiment Local Evaluation destination and require server-side SDK implementation.

The system includes comprehensive permission management with project-level access controls independent of Analytics permissions, featuring role-based access (Viewer, Member, Manager, Admin) and Enterprise-level flag-specific restrictions. Notification systems integrate with Slack workspaces and custom webhooks for real-time alerts on experiment changes and statistical anomalies like sample ratio mismatches.

## API Endpoints and Implementation

The platform provides multiple integration points including Experiment SDKs for various languages (TypeScript, Kotlin, Objective-C, Swift, Python), a REST API for programmatic access, and the Management API for permission and configuration management. Exposure tracking utilizes the Analytics REST API v2.0 with curl request capabilities and automatic tracking through client-side SDKs.

Key configuration options include defaultTracking settings, session management parameters (minTimeBetweenSessionsMillis), and deployment-specific API keys. The platform supports both feature flag implementations with simple boolean values and complex payload-based experiments using JSON configurations for advanced use cases like UI modifications and algorithmic variations.