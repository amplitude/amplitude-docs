# Amplitude Experiment: Product Overview

## Product Overview

Amplitude Experiment is a behavioral experimentation platform that enables product teams to conduct A/B testing, implement progressive feature delivery, and create dynamic in-product experiences. The platform uses feature flags to control product behavior without requiring code changes. Amplitude Experiment supports two primary workflows:

1. **Experimentation**: Creating experiments with hypotheses, metrics, variants, and statistical analysis to test product changes.
2. **Feature Flagging**: Implementing controlled feature rollouts without metrics analysis.

The platform offers two evaluation modes:
- **Remote Evaluation**: Server-based evaluation with advanced targeting capabilities
- **Local Evaluation**: In-SDK evaluation for sub-millisecond performance in latency-sensitive applications

## Key Features and Concepts

### Core Components

1. **Data Model Hierarchy**:
   - Organizations contain projects
   - Projects contain deployments, flags, and experiments
   - Flags contain variants
   - Experiments are specialized flags with metrics and analysis

2. **Evaluation Modes**:
   - **Remote Evaluation**: Makes requests to evaluation servers with advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, behavioral cohort targeting, and historical user properties
   - **Local Evaluation**: Runs evaluation logic directly in the SDK without network requests, providing sub-millisecond performance but with limited targeting capabilities

3. **Targeting and Bucketing**:
   - Pre-targeting steps (activation, dependencies, inclusions, sticky bucketing)
   - Targeting segments for user filtering
   - Consistent bucketing using murmur3 hashing algorithms
   - Cohort targeting using static or dynamic user cohorts

4. **Analysis Capabilities**:
   - Statistical measurements (p-values, confidence intervals)
   - Relative and absolute performance metrics
   - Dimensional analysis for segment-specific results
   - Ability to exclude test users from analysis

5. **Permission Management**:
   - Project-level user permissions
   - Flag-level access controls (Enterprise)
   - Role-based permissions (Viewer, Member, Manager, Admin)

## Key Nomenclature and Definitions

### Experimentation Terms

- **Variants**: Different versions of a feature being tested (control vs. treatment)
- **Allocation**: Percentage of users assigned to each variant
- **Bucketing**: Process of assigning users to variants
- **p-value**: Statistical measure of significance
- **Confidence Interval**: Range of likely true values for a metric
- **CUPED**: Controlled-experiment Using Pre-Experiment Data, a variance reduction technique
- **Sequential Testing**: Statistical approach that allows for early stopping
- **Type 1 Error**: False positive (incorrectly rejecting null hypothesis)
- **Type 2 Error**: False negative (incorrectly accepting null hypothesis)
- **Minimum Detectable Effect**: Smallest effect size that can be reliably detected

### Platform-Specific Terms

- **Deployments**: Environments for flags (e.g., development, production)
- **Exposure Events**: Records of users seeing a specific variant
- **Sticky Bucketing**: Ensuring users consistently see the same variant
- **Targeting Segments**: Rules defining which users see which variants
- **Cohort Targeting**: Using Amplitude cohorts for targeting experiment variants

## Integration with Amplitude Ecosystem

Amplitude Experiment integrates with the broader Amplitude ecosystem:

1. **Amplitude Analytics**: 
   - Exposure tracking sends data to Analytics for analysis
   - User properties and cohorts from Analytics can be used for targeting
   - Analytics data powers experiment results

2. **Amplitude SDKs**:
   - Client-side SDKs for web and mobile platforms
   - Server-side SDKs for backend implementations
   - SDKs handle variant assignment and exposure tracking

## Implementation Details

### SDK Initialization

Amplitude Experiment provides SDKs for multiple platforms:
- TypeScript/JavaScript
- HTML
- Kotlin
- Objective-C
- Swift
- Python

Example initialization (TypeScript):
```typescript
import * as amplitude from '@amplitude/analytics-browser';

amplitude.init('API_KEY', {
  defaultTracking: {
    sessions: true,
    pageViews: true,
    formInteractions: true,
    fileDownloads: true
  }
});
```

### Exposure Tracking

Two methods for tracking exposures:
1. **Automatic tracking** via Experiment SDKs
2. **Manual tracking** via Analytics REST API v2.0

Example manual exposure tracking:
```
POST https://api2.amplitude.com/2/httpapi
```

### Evaluation Process

The evaluation process follows these steps:
1. Pre-targeting (activation, dependencies, inclusions)
2. Targeting segment evaluation
3. Allocation bucketing using murmur3 hashing
4. Variant assignment

## Key Considerations and Best Practices

1. **Evaluation Mode Selection**:
   - Use remote evaluation for advanced targeting needs
   - Use local evaluation for performance-critical applications

2. **Cohort Targeting**:
   - Be aware of sync delays (hourly updates)
   - Ensure user IDs match between Amplitude and your application
   - Verify server-side SDK support for local evaluation cohort targeting

3. **Analysis**:
   - Use dimensional analysis to exclude test users
   - Consider segment-specific impacts when analyzing results
   - Understand statistical concepts for proper interpretation

4. **Permissions**:
   - Set appropriate access levels for team members
   - Use flag-level controls for sensitive features (Enterprise)