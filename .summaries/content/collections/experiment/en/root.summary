# Amplitude Experiment: Product Overview

Amplitude Experiment is a behavioral experimentation platform that enables product teams to conduct A/B testing, implement progressive feature delivery, and create dynamic in-product experiences through feature flags. The platform integrates with the broader Amplitude analytics ecosystem to provide data-driven experimentation capabilities.

## Key Features and Concepts

### Core Capabilities
- **A/B Testing**: Run controlled experiments to test hypotheses about product changes
- **Feature Flags**: Implement controlled feature rollouts without requiring metrics analysis
- **Progressive Feature Delivery**: Gradually release features to users
- **Dynamic In-Product Experiences**: Modify user experiences based on targeting criteria

### Evaluation Modes
1. **Remote Evaluation**: Server-based evaluation with advanced targeting capabilities
   - Amplitude ID resolution
   - IP geolocation
   - Property canonicalization
   - Behavioral cohort targeting
   - Historical user properties

2. **Local Evaluation**: In-SDK evaluation for latency-sensitive applications
   - Sub-millisecond performance
   - No network requests required
   - Limited to basic targeting capabilities
   - Available for both client-side and server-side SDKs

### Analysis Capabilities
- **Experiment Analysis View**: Statistical measurements to evaluate experiment success
- **Dimensional Analysis**: Filter experiment results by user segments
- **Sequential Testing**: Continuously monitor experiment results
- **Statistical Rigor**: p-values, confidence intervals, and significance testing

## Product Architecture and Relationships

### Data Model Hierarchy
1. **Organizations**: Top-level container for all Amplitude resources
2. **Projects**: Collection of flags, experiments, and deployments within an organization
3. **Deployments**: Environment-specific configurations (e.g., development, production)
4. **Flags**: Feature toggles that can be turned on/off or used for experiments
5. **Experiments**: Structured tests with variants, metrics, and statistical analysis
6. **Variants**: Different versions of a feature being tested
7. **Users**: End-users who are assigned to variants based on targeting criteria

### Integration with Amplitude Analytics
- Experiment leverages user data from Amplitude Analytics for targeting
- Exposure events are tracked in Amplitude Analytics for analysis
- Cohorts created in Analytics can be used for experiment targeting

## Key Nomenclature and Definitions

### Experimentation Terms
- **p-value**: Probability of observing results as extreme as the current results under the null hypothesis
- **Confidence Interval**: Range of values that likely contains the true effect size
- **CUPED (Controlled-experiment Using Pre-Experiment Data)**: Variance reduction technique
- **Type 1 Error**: False positive (incorrectly rejecting null hypothesis)
- **Type 2 Error**: False negative (incorrectly accepting null hypothesis)
- **Minimum Detectable Effect**: Smallest effect size that can be reliably detected
- **Bonferroni Correction**: Method to adjust for multiple comparisons

### Platform-Specific Terms
- **Bucketing**: Process of assigning users to variants
- **Sticky Bucketing**: Ensuring users consistently receive the same variant
- **Targeting Segments**: Criteria for including users in an experiment
- **Exposure Event**: Analytics event recording when a user sees a variant
- **Deployment Keys**: Environment-specific keys for SDK initialization
- **Variant Payload**: Additional configuration data attached to a variant

## Implementation Details

### Evaluation Process
1. **Pre-targeting Steps**:
   - Activation check
   - Flag dependencies verification
   - Inclusion/exclusion rules
   - Sticky bucketing application

2. **Targeting Segments**: Evaluate if user matches targeting criteria

3. **Bucketing Algorithm**:
   - Uses murmur3 hashing for consistent assignment
   - Applies allocation percentages
   - Assigns specific variants based on hash value

### SDK Integration

The platform provides SDKs for multiple languages:
- TypeScript/JavaScript
- HTML
- Kotlin
- Objective-C
- Swift
- Python

Example SDK initialization (TypeScript):
```typescript
import * as amplitude from '@amplitude/analytics-browser';

amplitude.init('API_KEY', {
  defaultTracking: {
    sessions: true,
    pageViews: true,
    formInteractions: true,
    fileDownloads: true
  }
});
```

### Exposure Tracking

Two methods for tracking user exposures:
1. **Automatic Tracking**: Built into Experiment SDKs
2. **Manual Tracking**: Using Analytics REST API v2.0

Example manual exposure tracking:
```
POST https://api2.amplitude.com/2/httpapi
```

## Access Control and Permissions

### Project-Level Permissions
- **Admin**: Full access to all features
- **Manager**: Can create/edit flags and experiments but cannot manage users
- **Member**: Can edit existing flags but cannot create new ones
- **Viewer**: Read-only access

### Enterprise Features
- Flag-level access controls
- Restricted flag editors
- Permission bypassing capabilities

## Best Practices and Considerations

### Cohort Targeting
- Static and dynamic user cohorts can be used for audience targeting
- Cohort sync configuration required for local evaluation
- Hourly sync available for dynamic cohorts
- User ID requirement for cohort targeting

### Experiment Design
- Formulate clear hypotheses
- Define primary and secondary metrics
- Consider statistical power and sample size
- Avoid interference between experiments (SUTVA - Stable Unit Treatment Value Assumption)
- Exclude test users from analysis to prevent skewed results

Amplitude Experiment provides a comprehensive platform for product teams to implement data-driven experimentation and feature management, with flexible evaluation options and robust statistical analysis capabilities.