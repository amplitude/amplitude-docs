Amplitude Experiment is a comprehensive behavioral experimentation and feature flag platform that enables organizations to conduct A/B tests and manage progressive feature delivery through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and feature flagging for controlled rollouts without metrics tracking.

## Core Architecture and Evaluation Models

The platform operates on two primary evaluation paradigms:

**Remote Evaluation** leverages Amplitude's evaluation servers to fetch variants with advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership targeting, and user enrichment using historical analytics data. This approach provides comprehensive targeting features but requires network requests.

**Local Evaluation** runs evaluation logic directly within SDKs, delivering sub-millisecond performance through consistent bucketing algorithms but with limited targeting capabilities - excluding Amplitude ID resolution, user enrichment, and sticky bucketing functionality.

## Data Model and Hierarchy

The system follows a hierarchical structure: Organizations contain Projects, which house Deployments (client-side or server-side with deployment keys), Flags/Experiments (supporting local/remote evaluation modes), Variants (with value/payload properties), and Users (requiring user_id/device_id with optional group support via Accounts add-on).

## Experimentation Implementation

The evaluation process follows a systematic pre-targeting workflow including activation checks, flag dependencies, individual inclusions, and sticky bucketing, followed by targeting segments and consistent bucketing using murmur3 hashing with bucketing salt and allocation percentages. This ensures reliable variant assignment while preventing sample ratio mismatch (SRM) and variant jumping.

The platform supports two statistical models: Sequential Testing for continuous monitoring and T-test for traditional fixed-horizon analysis. Experiments utilize SUTVA (Stable Unit Treatment Value Assumption) with configurable bucketing units including user, organization, or custom company_id through the Accounts add-on.

## Key Features and Capabilities

**Cohort Targeting** enables audience segmentation using both static and dynamic cohorts that sync hourly to remote and local evaluation destinations, requiring SDK configuration with API/secret keys for local evaluation scenarios.

**Dimensional Analysis** provides QA filtering capabilities, allowing teams to exclude internal traffic and test users from experiment analysis through targeting settings and user segment controls.

**Exposure Tracking** captures user interactions with feature flag variants through the Analytics REST API v2.0 and client-side SDKs with automatic exposure tracking, feeding into comprehensive analysis views.

## Analysis and Monitoring

The Analysis View presents statistical measurements including relative performance lift calculations, confidence intervals, significance testing with p-values, and absolute conversion percentages. The system distinguishes between recommendation metrics and secondary metrics for comprehensive experiment evaluation.

**Notifications** support Slack channel integration and webhook configurations with signing key verification, enabling alerts for flag lifecycle events (created/updated/deleted) and sample-ratio mismatch detection across projects, deployments, or tag-based scoping.

## Access Control and Permissions

Project-level permissions operate independently from Analytics permissions, featuring role-based access control (Viewer, Member, Manager, Admin) with flag-level access controls for restricting experiment editing capabilities. Enterprise customers can leverage Management API bypass capabilities for advanced permission management.

## API and SDK Integration

The platform provides comprehensive SDK support across multiple platforms (TypeScript/JavaScript, Kotlin, Objective-C, Swift, Python) with standardized initialization patterns, configuration options for session management, and event tracking through BaseEvent classes. The Evaluation REST API enables programmatic access to experiment configurations and variant assignments.

## Technical Implementation Details

Key API endpoints include the Analytics REST API v2.0 for exposure tracking and the Management API for experiment configuration. Command-line integration supports curl requests for exposure event submission. The system employs kebab-case variant formatting and supports JSON payloads for complex variant configurations.

The platform integrates cohort propagation with hourly sync intervals, maintains consistent bucketing through murmur3 hashing algorithms, and provides edge evaluation capabilities for Node.js environments with optimized cold start performance characteristics.