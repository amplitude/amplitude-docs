# Amplitude Experiment

## Product Overview

Amplitude Experiment is a behavioral experimentation platform that enables product teams to conduct A/B testing, implement progressive feature delivery, and create dynamic in-product experiences through feature flags. The platform supports two primary workflows:

1. **Experimentation**: Creating controlled experiments with hypotheses, metrics, variants, and statistical analysis to make data-driven product decisions.
2. **Feature Flagging**: Implementing controlled feature rollouts without metrics analysis, focusing on progressive delivery and risk mitigation.

The platform offers sub-millisecond performance through local evaluation for latency-sensitive applications, while also providing advanced targeting capabilities through remote evaluation when needed.

## Key Features and Concepts

### Evaluation Modes

1. **Local Evaluation**: 
   - Runs evaluation logic directly in the SDK without network requests
   - Provides sub-millisecond performance for latency-sensitive systems
   - Supports consistent bucketing and targeting segments
   - Lacks Amplitude ID resolution and user enrichment capabilities

2. **Remote Evaluation**:
   - Makes requests to evaluation servers to fetch variants
   - Supports advanced targeting capabilities:
     - Amplitude ID resolution
     - IP geolocation
     - Property canonicalization
     - Behavioral cohort targeting
     - Historical user properties

### Experiment Analysis

The Experiment Analysis view provides statistical measurements to evaluate experiment success, including:
- Relative performance metrics
- Confidence intervals
- Statistical significance indicators (p-values)
- Absolute values for each variant compared to the control

### Targeting Capabilities

- **Cohort Targeting**: Use static or dynamic user cohorts for audience targeting
- **Dimensional Analysis**: 
  - Exclude test users (QA or internal traffic) from experiment analysis
  - Analyze experiment impacts on specific user segments

### Evaluation Process

The platform uses a sophisticated evaluation process that includes:
1. Pre-targeting steps (activation, dependencies, inclusions, sticky bucketing)
2. Targeting segments evaluation
3. Consistent bucketing using murmur3 hashing
4. Allocation and variant distribution calculations

## Data Model and Architecture

### Organizational Structure

- **Organizations**: Top-level container for all Amplitude products
- **Projects**: Contain flags, experiments, and deployments
- **Deployments**: Represent different environments (development, staging, production)
  - Client-side deployments: For web/mobile applications
  - Server-side deployments: For backend services

### Core Components

- **Flags**: Feature toggles that control functionality
- **Experiments**: Structured tests with variants and metrics
- **Variants**: Different versions of a feature being tested
- **Users**: Identified by user IDs and properties for targeting and bucketing

### Exposure Tracking

- **Manual tracking**: Via the Analytics REST API v2.0
- **Automatic tracking**: Through Experiment SDKs
- Essential for reliable experiment results but optional for feature flags not requiring analysis

## Implementation and Integration

### SDK Support

The platform provides SDKs for multiple programming languages:
- TypeScript/JavaScript
- HTML
- Kotlin
- Objective-C
- Swift
- Python

### SDK Initialization Examples

```typescript
// TypeScript example
import * as amplitude from '@amplitude/analytics-browser';

amplitude.init('API_KEY', {
  defaultTracking: {
    sessions: true,
    pageViews: true,
    formInteractions: true,
    fileDownloads: true
  }
});
```

```python
# Python example
from amplitude import Amplitude

amplitude = Amplitude('API_KEY')
```

## Access Control and Permissions

### Project-Level Permissions

- Admins can manage access separately from Analytics permissions
- Enterprise customers have flag-level access controls
- Permission levels include:
  - **Viewer**: Read-only access
  - **Member**: Can edit flags but not create new ones
  - **Manager**: Can create and edit flags
  - **Admin**: Full access including permission management

## Statistical Methodology

Amplitude Experiment employs various statistical methods:
- **Sequential Testing**: Allows for continuous monitoring of results
- **Confidence Intervals**: Measure the precision of estimates
- **CUPED** (Controlled-experiment Using Pre-Experiment Data): Reduces variance in metrics
- **Bonferroni Correction**: Adjusts for multiple comparisons
- **T-tests**: Determine if differences between variants are statistically significant

## Technical Considerations

- **Bucketing Salt**: Ensures consistent user assignment across experiments
- **Sticky Bucketing**: Maintains user assignment to variants over time
- **Cohort Sync Configuration**: Required for local evaluation with cohort targeting
- **Propagation Delay**: Cohort membership updates may have hourly sync delays
- **Server-Side SDK Configuration**: Specific requirements for local evaluation with cohort targeting

## Integration with Amplitude Ecosystem

Amplitude Experiment integrates with the broader Amplitude product suite:
- Uses Amplitude Analytics for tracking exposure events
- Leverages Amplitude's user identification system for consistent targeting
- Utilizes Amplitude Cohorts for advanced targeting capabilities

## API Endpoints

The platform provides several key API endpoints:
- Analytics REST API v2.0 for manual exposure tracking
- Evaluation API endpoints for remote variant fetching

## Command Line Invocations

While not explicitly detailed in the documentation, the SDKs provide programmatic interfaces for:
- Fetching flag variants
- Tracking exposures
- Managing experiment configurations

This comprehensive platform enables product teams to make data-driven decisions through rigorous experimentation while also providing the flexibility of feature flagging for controlled rollouts and risk mitigation.