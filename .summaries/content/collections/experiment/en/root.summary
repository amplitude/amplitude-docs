Amplitude Experiment is a comprehensive behavioral experimentation platform that enables teams to run A/B tests and manage feature flags through a workflow-driven approach. The platform provides both statistical rigor and operational flexibility for progressive feature delivery and hypothesis-driven product development.

## Core Architecture and Evaluation Models

Amplitude Experiment operates on two primary evaluation paradigms:

**Remote Evaluation** leverages Amplitude's evaluation servers to fetch variants with sophisticated user enrichment capabilities. This includes Amplitude ID resolution, IP geolocation targeting, property canonicalization, cohort membership targeting, and access to user properties from historical analytics data. Remote evaluation supports sticky bucketing and behavioral cohorts with hourly sync capabilities.

**Local Evaluation** runs evaluation logic directly within SDKs, achieving sub-millisecond performance by eliminating network requests. However, this approach has limitations including no Amplitude ID resolution, restricted user enrichment, and unavailable sticky bucketing. Local evaluation is particularly suited for edge computing scenarios and high-performance requirements.

## Data Model and Hierarchy

The platform follows a hierarchical structure: Organizations contain Projects, which house Deployments (client-side or server-side with deployment keys), Flags/Experiments (supporting local/remote evaluation modes), Variants (with value/payload properties), and Users (requiring user_id/device_id with optional group support via Accounts add-on).

## Targeting and Bucketing Implementation

Experiment evaluation follows a sophisticated multi-step process involving pre-targeting steps (activation, flag dependencies, individual inclusions, sticky bucketing), targeting segments, and consistent bucketing using murmur3 hashing algorithms with bucketing salts and allocation percentages. The system supports cohort targeting with both static and dynamic user cohorts, synchronized hourly to either Amplitude Experiment destination (remote evaluation) or Experiment Local Evaluation destination (local evaluation, server-side SDKs only).

## Statistical Analysis Framework

The Analysis view provides comprehensive statistical measurements including relative performance lift calculations, confidence intervals, significance testing with p-values, and both absolute and percentage-based metrics. The platform supports sequential testing and T-test statistical models, with features like CUPED variance reduction, Bonferroni correction for multiple comparisons, and guardrail metrics monitoring.

**Dimensional Analysis** enables filtering of QA users and internal traffic through the "Exclude testers" functionality, allowing teams to analyze experiment results without contamination from testing activities.

## Key Nomenclature and Definitions

- **Allocation**: Percentage of users bucketed into experiment variants
- **Payload Variables**: Configuration data passed with variant assignments
- **Exposure Events**: Tracked when users are exposed to feature flag variants
- **Assignment Events**: Server-side tracking of variant assignments
- **Bucketing Units**: User identifiers used for consistent assignment (supports SUTVA compliance)
- **Guardrail Metrics**: Secondary metrics monitored to ensure experiment safety
- **MDE (Minimum Detectable Effect)**: Smallest effect size the experiment can reliably detect

## Integration and Implementation

The platform provides multi-platform SDK support across TypeScript, Kotlin, Objective-C, Swift, and Python with configuration options for defaultTracking settings, session management, and automatic event tracking. Integration options include:

- **Experiment SDK**: Full-featured client and server-side SDKs with automatic exposure tracking
- **REST API**: Direct API access for custom implementations
- **Analytics REST API v2.0**: For manual exposure tracking via curl requests
- **Management API**: Enterprise-level programmatic control with permissions bypass capabilities

## Permissions and Access Control

Amplitude Experiment implements project-level permissions independent from Analytics permissions, featuring user roles (Viewer, Member, Manager, Admin) and flag-level access controls. Enterprise customers can leverage Management API bypass capabilities for programmatic experiment management.

## Ecosystem Integration

The platform integrates deeply with Amplitude Analytics for user data enrichment, cohort targeting, and behavioral analysis. Exposure tracking feeds into Amplitude's analytics pipeline, enabling comprehensive user journey analysis and experiment impact measurement through the Exposures chart and user exposure analysis tools.