Amplitude Experiment is a comprehensive behavioral experimentation and feature flag platform that enables teams to run A/B tests and manage feature rollouts through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and feature flag management for progressive delivery without metrics.

## Core Architecture and Evaluation Modes

The platform operates through two primary evaluation modes:

**Remote Evaluation** fetches variants from Amplitude's evaluation servers and provides advanced targeting capabilities including Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership targeting, user enrichment using historical analytics data, and sticky bucketing. This mode offers the most comprehensive feature set but requires network requests.

**Local Evaluation** runs evaluation logic directly in the SDK, providing sub-millisecond performance through consistent bucketing and targeting segments. However, it has limitations compared to remote evaluation, lacking Amplitude ID resolution, user enrichment, and sticky bucketing capabilities. Client-side local evaluation also presents potential data leakage concerns.

## Data Model and Hierarchy

Amplitude Experiment follows a hierarchical structure: Organizations contain Projects, which contain Deployments (client-side or server-side with deployment keys), which contain Flags/Experiments with variants. Users are identified through user_id/device_id requirements, with group support available through the Accounts add-on. Variants contain both value and payload properties, with kebab-case formatting requirements.

## Evaluation Implementation

The evaluation process follows a systematic approach with pre-targeting steps including flag activation, flag dependencies, individual inclusions, and sticky bucketing, followed by targeting segments. The platform uses consistent bucketing with the murmur3 hashing algorithm and bucketing salts to ensure reliable variant assignment. Key concepts include mutual exclusion groups, holdout groups, and allocation bucketing to prevent sample ratio mismatch (SRM) and variant jumping.

## Experimentation Workflow

The platform supports comprehensive A/B testing workflows with hypothesis formation, metric selection (primary success metrics, secondary metrics, and guardrail metrics), treatment variant creation, and statistical analysis. The default statistical model uses sequential testing, with T-test options available. Key statistical concepts include CUPED variance reduction, Bonferroni correction for multiple comparisons, minimum detectable effect (MDE), statistical power, and confidence intervals.

## Analysis and Reporting

The Analysis view provides detailed statistical evaluation through columns showing relative performance, confidence intervals, significance testing, and absolute values. The metrics table enables variant comparison against control groups through unique conversions percentage and aggregate events per exposed user. Dimensional Analysis allows filtering of QA users and internal traffic through the "Exclude testers" functionality in Targeting settings.

## Targeting and Segmentation

The platform supports sophisticated targeting through static and dynamic cohorts that sync hourly to remote and local evaluation destinations. Cohort targeting requires SDK configuration with API/secret keys for local evaluation. Individual inclusions, user ID targeting, device ID targeting, and cohort targeting provide granular control over experiment exposure.

## Permissions and Access Control

Amplitude Experiment implements project-level permissions that operate independently from Analytics permissions. The role-based system includes Viewer, Member, Manager, and Admin roles. Enterprise customers receive flag-level access restrictions and restricted flag editors. The Management API provides bypass capabilities for programmatic access.

## Integration and SDKs

Multi-platform SDK support includes TypeScript, Kotlin, Objective-C, Swift, and Python implementations with consistent initialization patterns. SDKs support defaultTracking configuration, session management, and automatic exposure tracking. The platform integrates with the Analytics REST API v2.0 for exposure tracking and provides both client-side and server-side deployment options.

## Key API Endpoints and Configuration

- Evaluation REST API for variant fetching
- Analytics REST API v2.0 for exposure tracking  
- Management API for programmatic flag management
- CohortSyncConfig for local evaluation cohort targeting
- Deployment keys (DEPLOYMENT_KEY) for SDK authentication
- Project API keys for evaluation access

## Notifications and Monitoring

The platform provides notification capabilities through Slack channel integration and webhook configurations with signing key verification. Alerts can be scoped by project, deployment, or tags for flag lifecycle events (created/updated/deleted) and sample-ratio mismatch detection.

## Feature Flag Management

Beyond experimentation, the platform serves as a feature flag system for progressive feature delivery. Flags can be managed without metrics for simple on/off toggles or gradual rollouts. The system supports JSON payloads for complex configuration delivery and bulk archive operations for lifecycle management.