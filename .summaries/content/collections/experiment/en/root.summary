Amplitude Experiment is a comprehensive behavioral experimentation and feature flag platform that enables organizations to run A/B tests and manage progressive feature delivery through a workflow-driven approach. The platform supports both statistical experimentation with hypothesis testing and feature flagging for controlled rollouts without metrics tracking.

## Core Architecture and Data Model

The platform follows a hierarchical data structure consisting of organizations, projects, deployments, flags/experiments, variants, and users. Deployments are categorized as either client-side or server-side, each with unique deployment keys for SDK configuration. The system supports two primary evaluation modes: remote evaluation for advanced targeting capabilities and local evaluation for sub-millisecond performance.

Flags and experiments utilize variants with configurable values and JSON payloads, enabling complex feature configurations. The user model requires either user_id or device_id for identification, with optional group properties available through the Accounts add-on for organization-level bucketing.

## Evaluation Engine and Implementation

The evaluation process follows a sophisticated multi-step approach beginning with pre-targeting validation including flag activation status, dependency checks, individual inclusions, and sticky bucketing. The system then processes targeting segments before applying consistent bucketing using murmur3 hashing algorithms combined with bucketing salt and allocation percentages to ensure uniform variant distribution.

Remote evaluation provides comprehensive targeting through Amplitude ID resolution, IP geolocation, property canonicalization, cohort membership validation, and user enrichment using historical analytics data. Local evaluation trades advanced targeting features for performance, running evaluation logic directly in SDKs with sub-millisecond response times but without Amplitude ID resolution, user enrichment, or sticky bucketing capabilities.

## Statistical Analysis and Experimentation

The platform offers two statistical models: sequential testing for continuous monitoring and traditional T-test approaches. The Analysis view presents comprehensive statistical measurements including relative performance lift calculations, confidence intervals, significance testing with p-values, and absolute conversion percentages. Experiments support primary success metrics alongside secondary and guardrail metrics for comprehensive impact assessment.

Advanced features include CUPED variance reduction for improved statistical power, Bonferroni correction for multiple comparison adjustments, and dimensional analysis capabilities for filtering QA users and internal traffic from results.

## Targeting and Audience Management

Cohort targeting enables both static and dynamic audience segmentation with hourly sync intervals to evaluation destinations. The system supports individual user inclusions, segment-based targeting, and mutual exclusion groups to prevent user overlap between experiments. Holdout groups allow for long-term impact measurement across multiple experiments.

## Key Nomenclature and Definitions

**Deployment Keys**: Authentication tokens for SDK configuration, specific to client-side or server-side environments
**Bucketing Units**: The entity level for experiment assignment (user_id, device_id, or custom group identifiers)
**Allocation vs Variant Bucketing**: Two-stage process determining experiment participation then variant assignment
**Exposure Events**: Analytics events fired when users encounter feature flag variants
**Assignment Events**: Events tracking initial experiment enrollment
**Payload Variables**: JSON configuration objects passed with variant assignments
**SUTVA**: Stable Unit Treatment Value Assumption ensuring user independence in experiments
**Sample Ratio Mismatch (SRM)**: Statistical anomaly indicating bucketing irregularities

## API Endpoints and Integration

The platform provides REST API endpoints for experiment management, with the Analytics REST API v2.0 handling exposure tracking. SDKs are available across multiple platforms (TypeScript/JavaScript, Kotlin, Objective-C, Swift, Python) with consistent initialization patterns and configuration options including defaultTracking settings, session management, and event tracking capabilities.

Local evaluation requires CohortSyncConfig with API and secret keys for server-side cohort targeting. The Management API offers Enterprise customers programmatic access with permission bypass capabilities.

## Ecosystem Integration and Permissions

The platform integrates deeply with Amplitude Analytics for user data enrichment and historical behavior analysis. Project-level permissions operate independently from Analytics permissions, featuring role-based access control (Viewer, Member, Manager, Admin) and flag-level access restrictions.

Notification systems support Slack channel integration and webhook configurations with signing key verification for experiment lifecycle events including flag creation, updates, deletions, and SRM detection. Alert scoping is configurable by project, deployment, or tag-based filtering.

The system supports progressive feature delivery workflows alongside traditional A/B testing, enabling teams to manage feature rollouts and conduct statistical experiments within a unified platform architecture.