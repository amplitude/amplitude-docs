Amplitude Experiment is a comprehensive behavioral experimentation platform that enables teams to run A/B tests and manage feature flags through a workflow-driven approach. The platform provides both statistical rigor and operational flexibility for progressive feature delivery and hypothesis-driven product development.

## Core Architecture and Evaluation Models

Amplitude Experiment operates on two primary evaluation paradigms:

**Remote Evaluation** leverages Amplitude's evaluation servers to fetch variants with sophisticated user enrichment capabilities. This includes Amplitude ID resolution, IP geolocation targeting, property canonicalization, cohort membership targeting, and access to user properties from historical analytics data. Remote evaluation supports sticky bucketing and behavioral cohorts with hourly sync capabilities.

**Local Evaluation** runs evaluation logic directly within SDKs, achieving sub-millisecond performance by eliminating network requests. However, this approach has limitations including no Amplitude ID resolution, restricted user enrichment, and unavailable sticky bucketing. Local evaluation is particularly suited for edge computing scenarios and high-performance requirements.

## Data Model and Organizational Structure

The platform follows a hierarchical data model: Organizations contain Projects, which house Deployments (client-side or server-side with deployment keys), Flags/Experiments (supporting local/remote evaluation modes), Variants (with value/payload properties), and Users (requiring user_id/device_id with optional group support via Accounts add-on).

## Statistical Framework and Analysis

Amplitude Experiment employs robust statistical methodologies including sequential testing and T-test models for determining statistical significance. The Analysis view provides comprehensive metrics including relative performance lift calculations, confidence intervals, p-value significance testing, and both recommendation and secondary metrics evaluation. The platform incorporates advanced statistical concepts like CUPED variance reduction, Bonferroni correction for multiple comparisons, and sample ratio mismatch (SRM) detection.

## Implementation and Bucketing System

The evaluation implementation follows a systematic process through pre-targeting steps (activation, flag dependencies, individual inclusions, sticky bucketing), targeting segments, and consistent bucketing using the murmur3 hashing algorithm with bucketing salts and allocation percentages. This ensures SUTVA (Stable Unit Treatment Value Assumption) compliance and prevents variant jumping.

## Key Nomenclature and Definitions

- **Allocation**: Percentage of users exposed to experiment variants
- **Payload Variables**: Configuration data passed with variant assignments
- **Exposure Events**: Tracked when users encounter feature flag variants
- **Assignment Events**: Server-side tracking of variant assignments
- **Guardrail Metrics**: Secondary metrics monitoring for negative impacts
- **Bucketing Units**: User identification method (user_id, device_id, or groups)
- **Targeting Segments**: User criteria for experiment inclusion
- **Mutual Exclusion Groups**: Preventing user overlap between experiments

## Advanced Features and Capabilities

**Cohort Targeting** enables sophisticated audience segmentation using both static and dynamic cohorts. Remote evaluation supports hourly sync to Amplitude Experiment destination, while local evaluation requires server-side SDKs and syncs to Experiment Local Evaluation destination.

**Dimensional Analysis** provides filtering capabilities to exclude QA users and internal traffic from experiment analysis using targeting settings and the "Exclude testers" option.

**Project-Level Permissions** operate independently from Analytics permissions, offering role-based access controls (Viewer, Member, Manager, Admin) and Enterprise-level flag-specific access restrictions managed through the Experiment UI and Management API.

## Integration and API Access

The platform provides comprehensive integration options through multi-platform SDKs (TypeScript, Kotlin, Objective-C, Swift, Python) with configurable options for session management, event tracking, and defaultTracking settings. Exposure tracking utilizes the Analytics REST API v2.0 with automatic exposure tracking capabilities in client-side SDKs.

## Ecosystem Integration

Amplitude Experiment integrates seamlessly with the broader Amplitude Analytics ecosystem, leveraging existing user data, behavioral cohorts, and project structures. The platform supports both the Experiment SDK and REST API integration patterns, enabling flexible implementation across various technical architectures while maintaining consistent bucketing and statistical rigor across all evaluation modes.