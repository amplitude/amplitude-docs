Amplitude Experiment is an enterprise-grade feature flagging and A/B testing platform designed for sophisticated experimentation at scale. The platform enables organizations to conduct controlled experiments, manage feature rollouts, and perform complex statistical analyses through a comprehensive suite of advanced techniques and methodologies.

## Product Architecture and Feature Relationships

The platform operates through a multi-layered architecture with distinct evaluation modes and traffic management systems. **Local evaluation flags** utilize Device ID bucketing and execute server-side with streamlined targeting capabilities, while **remote evaluation flags** support complex targeting rules and leverage Amplitude ID for bucketing decisions. Migration between these modes requires careful consideration of bucketing behavior changes and deployment implications.

The **traffic management system** forms the core of experiment orchestration, implementing sophisticated allocation mechanisms through holdout groups, mutual exclusion groups, and sticky bucketing algorithms. Holdout groups systematically exclude user percentages to measure cumulative program impact, while mutual exclusion groups prevent cross-contamination between related experiments. These systems interact multiplicatively, creating compounded effects where users in multiple holdout groups experience reduced traffic exposure (e.g., dual 5% holdouts result in 90.25% effective traffic).

**Flag dependencies and prerequisites** enable complex experiment orchestration through dependency chains, where dependent flags require prerequisite flags to maintain specific variant values. This architecture supports chained testing scenarios while preventing circular dependencies, available exclusively on enterprise plans.

The platform maintains deep **analytics integration** with Amplitude Analytics, providing seamless metric analysis, outlier detection, and data visualization capabilities. Chart configurations can be imported and exported as JSON between Experiment and Analytics, ensuring consistency across analysis workflows.

## Core Nomenclature and Technical Definitions

**Sticky Bucketing** represents a deterministic hashing algorithm that maintains consistent variant assignments for users despite changes to experiment targeting criteria, percentage rollout, or rollout weights. This prevents variant jumping and ensures user experience consistency throughout experiment lifecycles.

**Sample Ratio Mismatch (SRM)** identifies critical data quality issues where actual variant allocation deviates from expected traffic distribution. Common causes include instrumentation errors, mid-experiment configuration changes, or authentication patterns that systematically affect user distribution across variants.

**Exposure Events** distinguish between user assignment and actual variant exposure, capturing when users genuinely encounter experiment variants rather than merely being assigned to them. These can be custom mid-funnel events or default events triggered by flag evaluation.

**Cumulative Exposures Graph** provides temporal visualization of exposure event accumulation, with slope variations indicating exposure rate changes. Increasing slopes demonstrate steady growth, decreasing slopes reveal slowing exposure rates, and inflection points highlight traffic allocation or targeting modifications.

**Winsorization** implements statistical outlier mitigation by capping extreme values at specified percentiles rather than removing observations entirely, preserving sample size while reducing statistical noise.

**Multiple Hypothesis Testing** applies statistical corrections when evaluating multiple variants or metrics simultaneously, utilizing Bonferroni correction to control family-wise error rates and prevent false positive inflation.

## Ecosystem Integration and Deployment Architecture

Amplitude Experiment integrates with diverse external systems and deployment patterns. **AWS CloudFront integration** enables reverse proxy configuration to bypass domain blocking, utilizing specific cache policies including CachingDisabled, AllViewExceptHostHeader, and CORS-with-preflight-and-SecurityHeadersPolicy configurations with the `/v1/vardata` endpoint for evaluation requests.

**Server-Side Rendering (SSR)** implementations coordinate the JavaScript Server SDK (`@amplitude/experiment-node-server`) with the Client SDK (`@amplitude/experiment-js-client`), enabling server-side variant fetching and client initialization using pre-fetched variants for seamless user experiences.

**Split URL Testing** facilitates A/B testing through URL redirects without developer intervention, requiring Experiment SDK script tags in HTML `<head>` sections and Analytics SDK for event tracking, though limited to local evaluation mode and User bucketing unit requirements.

## API Endpoints and Technical Implementation

The primary **evaluation endpoint** `/v1/vardata` handles variant data retrieval and supports CloudFront proxy configurations for enhanced reliability and performance.

**SDK Integration Points** include:
- `ExperimentServer.fetchV2()` for server-side variant fetching
- `ExperimentClient.initialVariants` for client-side initialization with pre-fetched variants
- `[Experiment] Exposure` events for default exposure tracking
- `Experiment Assignment` events for debugging sticky bucketing and variant assignments

**Configuration APIs** support JSON import/export functionality for programmatic chart configuration, with schemas encompassing variants arrays, bucketingGroupType, userProperty, exposureEvent, and comprehensive metrics definitions.

**Advanced Metric Analysis** capabilities include complex funnel analyses with mid-funnel exposure events, threshold metrics using funnel counting methodologies, and multi-event conversion tracking with experiment flag key filtering to reduce statistical noise and improve signal detection.

The platform emphasizes enterprise-scale considerations including sophisticated traffic management complexity, statistical rigor through multiple hypothesis testing corrections, and advanced debugging approaches utilizing cumulative exposure analysis and comprehensive data quality monitoring systems.