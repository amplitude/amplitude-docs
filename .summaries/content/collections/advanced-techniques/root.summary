# Amplitude Experiment: Advanced Techniques

## Product Overview

Amplitude Experiment is a feature experimentation and flag management platform that enables teams to test new features, conduct A/B tests, and manage feature rollouts. The platform combines sophisticated experiment design capabilities with powerful analytics integration to help teams make data-driven decisions about product changes.

Key features include:
- Dual evaluation modes (local and remote) for different implementation needs
- Holdout groups for measuring long-term feature impacts
- Mutual exclusion groups to prevent experiment interference
- Flag prerequisites for creating feature dependencies
- Advanced metrics analysis with outlier handling
- Sticky bucketing for consistent user experiences across sessions

The platform supports various use cases including A/B testing, gradual feature rollouts, split URL testing, server-side rendering implementations, and complex multi-variant experiment designs.

## Product Relationships and Features

Amplitude Experiment's architecture consists of several interconnected components:

1. **Evaluation Modes**:
   - **Local Evaluation**: Client-side evaluation using device ID for bucketing, ideal for frontend applications and URL redirect testing. Provides faster performance with reduced network requests.
   - **Remote Evaluation**: Server-side evaluation supporting complex targeting rules and multiple bucketing options. Offers greater security and flexibility for sensitive features.

2. **Flag Types**:
   - **Feature Flags**: Simple toggles for enabling/disabling features
   - **Feature Experiments**: More complex tests with multiple variants and controlled distribution

3. **Bucketing and Assignment**:
   - Device ID-based bucketing for anonymous users
   - User ID-based bucketing for logged-in experiences
   - Sticky bucketing to maintain consistent user experiences
   - Variant assignment controls to manage experiment exposure

4. **Analytics Integration**:
   - Direct connection to Amplitude Analytics for results analysis
   - Experiment results visualization with statistical significance testing
   - Custom metrics and conversion funnels based on experiment data
   - Chart settings exportable between platforms

5. **Enterprise Features**:
   - Holdout groups for measuring long-term impacts
   - Mutual exclusion groups to prevent experiment interference
   - Flag prerequisites for creating dependencies between features
   - Multiple hypothesis testing corrections for statistical validity

## Key Nomenclature and Definitions

- **Bucketing**: The process of assigning users to experiment variants based on identifiers like device ID or user ID
- **Sticky Bucketing**: Mechanism ensuring users consistently see the same variant even when targeting criteria change
- **Variant Jumping**: Undesirable situation where users see different variants across sessions
- **Exposure Event**: Analytics event tracking when a user is exposed to an experiment variant
- **Assignment Event**: Record of which variant a user is assigned to
- **Sample Ratio Mismatch (SRM)**: Statistical anomaly where observed variant allocations differ significantly from specified allocations
- **Holdout Group**: A percentage of users excluded from experiments to measure long-term impacts
- **Mutual Exclusion Group**: Configuration ensuring users in one experiment cannot participate in another related experiment
- **Flag Prerequisites**: Dependencies between flags where one flag's evaluation depends on another flag's state
- **Multiple Hypothesis Testing**: Statistical challenge when running experiments with multiple variants or metrics
- **Bonferroni Correction**: Method to adjust significance thresholds when testing multiple hypotheses
- **Cumulative Exposures Graph**: Visualization showing experiment exposure patterns over time

## Product Ecosystem Integration

Amplitude Experiment integrates within a broader product ecosystem:

1. **Amplitude Analytics Integration**:
   - Experiment results are automatically analyzed in Amplitude Analytics
   - User segments from experiments can be used for cohort and funnel analysis
   - Metrics defined in Analytics can be reused in Experiment
   - Shared user identity system across platforms

2. **Development Workflow Integration**:
   - SDKs available for various platforms (JavaScript, React, iOS, Android, server-side languages)
   - Support for CI/CD pipelines through flag management
   - Integration with server-side rendering frameworks like Next.js
   - Feature flag management for controlled rollouts

3. **Infrastructure Integration**:
   - AWS CloudFront can be used as a reverse proxy for evaluation servers
   - Support for cross-region deployments
   - Edge evaluation capabilities for performance optimization

4. **Marketing Tool Integration**:
   - Split URL testing for marketing campaigns
   - Integration with CMS platforms for content testing
   - Support for multi-page user journeys

## API Endpoints and SDK Usage

### JavaScript Client SDK
```javascript
// Initialize the client
const experimentClient = Experiment.initialize({
  apiKey: 'API_KEY',
  userId: 'user@example.com'
});

// Fetch variants
experimentClient.fetch().then(() => {
  const variant = experimentClient.variant('flag-key');
});
```

### JavaScript Server SDK
```javascript
// Initialize the server
const experimentServer = new ExperimentServer({
  apiKey: 'SERVER_API_KEY',
  serverUrl: 'https://api.lab.amplitude.com'
});

// Fetch variants for a user
const variants = await experimentServer.fetchV2({
  user: {
    user_id: 'user@example.com'
  }
});
```

### Server-Side Rendering Implementation
```javascript
// Server-side
const variants = await experimentServer.fetchV2({
  user: { user_id: req.user.id }
});

// Pass to client during render
const experimentClient = Experiment.initialize({
  apiKey: 'API_KEY',
  userId: req.user.id,
  initialVariants: variants
});
```

### Flag Prerequisites Configuration
```json
{
  "prerequisites": [
    {
      "flag_key": "prerequisite-flag",
      "variant": "on"
    }
  ]
}
```

### Evaluation Endpoints
- Remote evaluation API: `https://api.lab.amplitude.com`
- Local evaluation CDN: Used for downloading flag configurations
- Both can be proxied through AWS CloudFront for regions with access restrictions

Amplitude Experiment provides a comprehensive platform for sophisticated experimentation with particular strengths in handling complex experimental scenarios, ensuring statistical validity, and integrating deeply with analytics for data-driven decision making.