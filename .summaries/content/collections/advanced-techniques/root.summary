Amplitude Experiment is a comprehensive A/B testing and feature flag platform that enables sophisticated experimentation workflows with advanced statistical analysis, traffic management, and evaluation capabilities. The platform supports both local and remote evaluation modes, providing flexibility for different deployment scenarios while maintaining statistical rigor and data integrity.

## Product Architecture and Feature Relationships

The platform operates through several interconnected components that work together to deliver comprehensive experimentation capabilities:

**Evaluation Infrastructure**: The core evaluation system supports both local and remote evaluation modes. Local evaluation flags use Device ID bucketing and run server-side with limited targeting capabilities, while remote evaluation flags support more complex targeting rules and use Amplitude ID bucketing. The system includes proxy support through AWS CloudFront for bypassing domain blocking, with specific cache policies (CachingDisabled, AllViewExceptHostHeader, CORS-with-preflight-and-SecurityHeadersPolicy) and testing via the `/v1/vardata` endpoint.

**Traffic Management System**: Advanced traffic control is achieved through multiple layers including holdout groups (which exclude users to measure long-term cumulative impact), mutual exclusion groups (preventing users from exposure to multiple related experiments), and sophisticated traffic allocation mechanisms. Multiple holdout groups compound traffic reduction (e.g., 0.95 * 0.95 = 90.25%), while mutual exclusion groups further limit experiment traffic allocation.

**Statistical Analysis Engine**: The platform implements robust statistical methods including Bonferroni correction for multiple hypothesis testing, sample ratio mismatch (SRM) detection, and outlier identification using standard deviations, boxplots, and percentiles. Advanced metric analysis supports funnel analyses with mid-funnel exposure events and threshold metrics using funnel counting for multi-event conversions.

**Dependency Management**: Flag prerequisites (Enterprise feature) enable complex dependency chains between feature flags and experiments, where evaluation requires prerequisite flags to have specific variant values. This supports release groups patterns and chained mutual exclusion groups while preventing circular dependencies.

## Key Nomenclature and Definitions

**Sticky Bucketing**: A deterministic hashing algorithm that maintains consistent variant assignments for users even when experiment targeting criteria, percentage rollout, or rollout weights change. Only available for feature experiments, not flags.

**Variant Jumping**: The phenomenon where users switch between experiment variants due to changes in experiment configuration, prevented by sticky bucketing.

**Sample Ratio Mismatch (SRM)**: A data quality issue where the observed ratio of users in experiment variants doesn't match the expected allocation, often caused by instrumentation errors or mid-experiment configuration changes.

**Exposure Events**: Events that indicate when a user has been exposed to an experiment variant, critical for accurate metric calculation and can be custom-defined or automatically generated.

**Bucketing Key**: The identifier used to assign users to variants, typically Device ID for local evaluation or Amplitude ID for remote evaluation.

**Evaluation Mode**: Determines whether flag evaluation happens locally (server-side with cached rules) or remotely (via API calls with full targeting capabilities).

**Winsorization**: A statistical technique for handling outliers by capping extreme values at specified percentiles rather than removing them entirely.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude ecosystem, particularly Amplitude Analytics for comprehensive data analysis. The platform supports JSON import/export of experiment chart settings between Experiment and Analytics, enabling seamless workflow transitions. Key integration points include:

**Analytics Integration**: Experiment Results charts can be exported as JSON with schema including variants, bucketingGroupType, userProperty, metrics, and exposureEvent fields. Advanced outlier detection leverages Analytics features like FREQPERCENTILE, PROPSUM, and PERCENTILE formulas across different metric types.

**Server-Side Rendering Support**: Dual SDK implementation using both JavaScript Server SDK (@amplitude/experiment-node-server) and JavaScript Client SDK (@amplitude/experiment-js-client) with initialVariants for seamless SSR experiences.

**Visual Experimentation**: URL redirect testing capabilities enable A/B testing with visitor redirection to different URLs, requiring minimal developer involvement but with specific constraints including local evaluation mode and User bucketing unit limitations.

## API Endpoints and Technical Implementation

Key technical interfaces include:

- `/v1/vardata` endpoint for testing CloudFront proxy configurations
- ExperimentServer and ExperimentClient classes for dual SDK implementation
- fetchV2 method for server-side variant retrieval
- Script tag deployment in `<head>` section for URL redirect testing
- JSON schema support for experiment configuration import/export

The platform emphasizes data quality through comprehensive debugging tools including cumulative exposure graphs with slope analysis, inflection point detection, and divergent line interpretation to identify issues like traffic allocation changes, targeting criteria modifications, and the risks of Simpson's paradox from mid-experiment adjustments.