# Amplitude Experiment: Advanced Techniques

## Product Overview

Amplitude Experiment is a feature experimentation and flag management platform that enables product teams to test hypotheses, roll out features, and measure impact through controlled experiments. The platform supports both local and remote evaluation modes, allowing for flexible implementation across client-side and server-side environments. Amplitude Experiment helps teams make data-driven decisions by providing robust tools for variant assignment, traffic allocation, exposure tracking, and statistical analysis of experiment results.

## Product Relationships and Features

Amplitude Experiment integrates with the broader Amplitude ecosystem, particularly with Amplitude Analytics:

1. **Amplitude Analytics Integration**
   - Experiment results can be exported to Analytics for deeper analysis
   - Chart settings can be shared between platforms using JSON import/export
   - Analytics provides the user data and event tracking infrastructure for measuring experiment results

2. **Evaluation Modes**
   - **Local Evaluation**: Client-side processing of flag rules directly on the user's device
     - Optimized for performance-critical applications
     - Reduces network requests and latency
     - Suitable for mobile and web applications with offline capabilities
   - **Remote Evaluation**: Server-side decision making on Amplitude's servers
     - Offers more flexibility for complex targeting rules
     - Enables immediate updates to targeting logic without client updates
     - Better for sensitive business logic that shouldn't be exposed client-side

3. **Experiment Types**
   - **Feature Experiments**: Tests comparing different variants of a feature
   - **Holdout Groups**: Special experiment groups that exclude users to measure long-term cumulative impacts
   - **Mutually Exclusive Experiments**: Groups where users participate in only one experiment at a time

4. **SDK Ecosystem**
   - Client SDKs (JavaScript, iOS, Android) for local evaluation
   - Server SDKs for remote evaluation and server-side rendering
   - Support for combined implementations (server-side rendering with client-side hydration)

5. **Deployment Options**
   - Direct integration with Amplitude servers
   - Proxy configurations (e.g., AWS CloudFront) for regions with domain restrictions

## Key Nomenclature and Definitions

- **Flag**: A feature toggle that controls access to functionality
- **Experiment**: A controlled test comparing different variants
- **Variant**: A specific version or implementation being tested
- **Exposure Event**: Tracks when a user is exposed to an experiment variant
- **Assignment Event**: Records which variant a user is assigned to
- **Bucketing**: The process of assigning users to variants
- **Sticky Bucketing**: Ensures users consistently see the same variant even when targeting criteria change
- **Bucketing Keys**: Identifiers (like device ID or user ID) used to determine variant assignment
- **Traffic Allocation**: Controls what percentage of users are included in experiments
- **Holdout Group**: A segment of users excluded from experiments to measure baseline metrics
- **Mutual Exclusion Group**: A collection of experiments where users can only be assigned to one experiment
- **Flag Prerequisites**: Dependencies between flags where one flag's evaluation depends on another flag's result
- **Sample Ratio Mismatch (SRM)**: A statistical anomaly where observed variant allocations differ from expected allocations
- **Variant Jumping**: When users see different variants across sessions, creating inconsistent experiences

## Product Ecosystem Integration

Amplitude Experiment functions as part of a comprehensive product analytics and experimentation suite:

1. **Data Flow**
   - User interactions generate events captured by Amplitude Analytics
   - Experiment exposures are tracked as special events
   - Results are analyzed within Amplitude's analytics platform

2. **Cross-Platform Capabilities**
   - Consistent experiment experiences across web, mobile, and server environments
   - Unified reporting regardless of where experiments are implemented

3. **Development Workflow Integration**
   - Feature flags can be managed separately from code deployment cycles
   - Experiments can be configured, launched, and analyzed without engineering involvement
   - Integration with CI/CD pipelines for automated testing and deployment

4. **Decision Making Framework**
   - Provides statistical rigor through multiple hypothesis testing correction
   - Offers cumulative exposure graphs to visualize experiment impact over time
   - Includes SRM detection to identify potential issues with experiment implementation

## API and Implementation Details

### SDK Initialization

JavaScript Client SDK:
```javascript
import { Experiment } from '@amplitude/experiment-js-client';
const experiment = Experiment.initialize('<API_KEY>', {
  userID: 'user-id'
});
```

JavaScript Server SDK:
```javascript
import { ExperimentServer } from '@amplitude/experiment-node-server';
const experiment = new ExperimentServer({
  apiKey: '<API_KEY>',
  serverUrl: 'https://api.lab.amplitude.com'
});
```

### Variant Fetching

Local Evaluation:
```javascript
const variants = await experiment.fetch();
const variant = variants['flag-key'];
```

Remote Evaluation:
```javascript
const variants = await experiment.fetch({
  user_id: 'user-id',
  device_id: 'device-id'
});
```

Server-Side Rendering with Client Hydration:
```javascript
// Server-side
const variants = await experimentServer.fetchV2({
  user_id: userId,
  device_id: deviceId
});

// Client-side hydration
const experimentClient = Experiment.initialize('<API_KEY>', {
  userID: userId,
  initialVariants: variants
});
```

### Advanced Configuration

Flag Prerequisites:
```json
{
  "prerequisites": [
    {
      "flag_key": "prerequisite-flag",
      "variants": ["on", "control"]
    }
  ]
}
```

The platform includes advanced analysis capabilities for detecting outliers, interpreting exposure patterns, and maintaining statistical validity across complex experimental designs, including Bonferroni correction for multiple hypothesis testing.