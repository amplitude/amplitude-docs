Amplitude Experiment is a comprehensive feature flagging and A/B testing platform designed for enterprise-scale experimentation and controlled feature rollouts. The platform enables organizations to run sophisticated experiments with advanced statistical analysis, complex traffic management, and deep analytics integration.

## Product Relationships and Feature Integration

The platform operates through interconnected components forming a complete experimentation ecosystem:

**Dual Evaluation Architecture**: The system supports local and remote evaluation modes with distinct characteristics. Local evaluation flags utilize Device ID bucketing for server-side execution with limited targeting capabilities, while remote evaluation flags support complex targeting rules using Amplitude ID bucketing. Migration between modes requires careful consideration of bucketing behavior changes and deployment implications.

**Sophisticated Traffic Management**: Experiments employ a multi-layered traffic allocation system incorporating holdout groups, mutual exclusion groups, and sticky bucketing mechanisms. Holdout groups exclude user percentages to measure cumulative program impact, while mutual exclusion groups prevent overlapping experiment exposure. These systems interact multiplicatively, creating compounded traffic effects (e.g., multiple 95% holdout groups result in 90.25% effective traffic).

**Dependency Management**: Enterprise plans support flag prerequisites, enabling complex dependency chains where dependent flags require specific prerequisite flag variant values. This facilitates orchestrated experiment sequences and chained testing scenarios while preventing circular dependencies.

**Analytics Platform Integration**: Deep integration with Amplitude Analytics provides comprehensive metric analysis, outlier detection, and data visualization capabilities. Chart configurations can be imported/exported as JSON between Experiment and Analytics platforms, ensuring analytical consistency.

## Key Nomenclature and Definitions

**Sticky Bucketing**: Deterministic hashing algorithm maintaining consistent variant assignments despite changes to targeting criteria, percentage rollout, or rollout weights, preventing variant jumping and ensuring treatment consistency throughout experiment lifecycles.

**Sample Ratio Mismatch (SRM)**: Data quality issue occurring when actual variant allocation deviates from expected traffic distribution, typically caused by instrumentation errors, mid-experiment modifications, or variant jumping behaviors.

**Exposure Events**: Events indicating actual user exposure to experiment variants, distinguished from assignment events. Custom exposure events can be configured for specific measurement requirements and analytical scenarios.

**Cumulative Exposures Graph**: Time-series visualization displaying exposure event accumulation patterns. Slope variations indicate exposure rate changes: increasing slopes show steady growth, decreasing slopes indicate slowing rates, and inflection points reveal traffic or targeting modifications.

**Variant Jumping**: User switching between experiment variants during active experiments, potentially caused by inconsistent bucketing keys, authentication state changes, or sticky bucketing modifications.

**Winsorization**: Statistical outlier handling technique capping extreme values at specified percentiles rather than removal, applicable to totals, sum of property, and average of property metrics.

**Multiple Hypothesis Testing**: Statistical correction methodology for simultaneous testing of multiple variants or metrics, employing Bonferroni correction to control family-wise error rates and prevent false positive inflation.

## Broader Product Ecosystem Integration

Amplitude Experiment functions within an integrated experimentation and analytics ecosystem:

**Multi-SDK Architecture**: Platform supports diverse SDK implementations including JavaScript Server SDK (`@amplitude/experiment-node-server`) and Client SDK (`@amplitude/experiment-js-client`) for server-side rendering scenarios, enabling variant delivery across various deployment architectures.

**Infrastructure Integration**: Advanced deployment scenarios include AWS CloudFront reverse proxy configurations for domain blocking bypass, utilizing specific cache policies (CachingDisabled, AllViewExceptHostHeader, CORS-with-preflight-and-SecurityHeadersPolicy) and API endpoint routing.

**Analytics Platform Connectivity**: Comprehensive integration with Amplitude Analytics enables advanced metric analysis including funnel analyses with mid-funnel exposure events, threshold metrics using funnel counting, and sophisticated outlier detection through statistical methods and visualization tools.

**Enterprise Feature Set**: Advanced capabilities including holdout groups, mutual exclusion groups, flag prerequisites, and sticky bucketing are available on Enterprise plans, providing sophisticated experiment orchestration for large-scale experimentation programs.

## API Endpoints and Technical Implementation

**Core API Endpoints**:
- `/v1/vardata` - Primary variant data retrieval endpoint for CloudFront proxy testing and variant fetching
- Regional evaluation servers: `api.lab.amplitude.com` and `api.lab.eu.amplitude.com`

**SDK Methods**:
- `ExperimentServer.fetchV2()` - Server-side variant fetching for SSR implementations
- `ExperimentClient.initialVariants` - Client-side initialization with pre-fetched variants

**Configuration Management**: JSON-based import/export functionality for experiment chart settings, incorporating schema fields including `variants`, `bucketingGroupType`, `userProperty`, `exposureEvent`, and `metrics` arrays.

**Split URL Testing**: Requires Experiment SDK script tag placement in HTML `<head>` section with Analytics SDK for event tracking, supporting low-code visual experimentation workflows.

The platform emphasizes statistical rigor through automatic multiple hypothesis testing correction, comprehensive outlier detection and mitigation strategies, and sophisticated traffic management systems enabling complex experimental designs while maintaining data quality and statistical validity across enterprise-scale implementations.