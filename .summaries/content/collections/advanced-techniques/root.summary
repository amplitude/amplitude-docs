Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that enables organizations to run controlled experiments and manage feature rollouts at scale. The platform provides both local and remote evaluation modes, sophisticated traffic allocation mechanisms, and advanced statistical analysis capabilities for measuring experiment impact.

## Core Product Architecture and Features

Amplitude Experiment operates through a dual-SDK architecture supporting both server-side and client-side implementations. The platform offers local evaluation flags for high-performance scenarios using device ID bucketing, and remote evaluation flags with more sophisticated targeting capabilities using Amplitude ID bucketing. The system supports server-side rendering through combined use of the JavaScript Server SDK (@amplitude/experiment-node-server) and JavaScript Client SDK (@amplitude/experiment-js-client), enabling seamless variant delivery across different rendering contexts.

The platform includes URL redirect testing capabilities for low-code A/B testing scenarios, requiring only the Experiment SDK script tag in the HTML head section and Analytics SDK for event tracking. For enterprise deployments requiring domain bypass, AWS CloudFront reverse proxy configurations are supported with specific cache policies (CachingDisabled, AllViewExceptHostHeader, CORS-with-preflight-and-SecurityHeadersPolicy) targeting evaluation servers at api.lab.amplitude.com and api.lab.eu.amplitude.com.

## Advanced Traffic Management and User Assignment

Amplitude Experiment provides sophisticated traffic control through multiple mechanisms. Sticky bucketing ensures consistent variant assignments for users even when experiment parameters change, using deterministic hashing algorithms to prevent variant jumping. This feature is exclusive to feature experiments and maintains assignment consistency across targeting criteria modifications, percentage rollout changes, and rollout weight adjustments.

The platform supports mutually exclusive experiments through mutual exclusion groups, preventing users from simultaneous exposure to related experiments to avoid interaction effects. Holdout groups enable long-term impact measurement by excluding specified user percentages from experimentation programs. These systems can compound, where multiple holdout groups multiply traffic reduction (e.g., 0.95 * 0.95 = 90.25% remaining traffic), and mutual exclusion groups further constrain available experiment slots.

Flag prerequisites create dependency chains between experiments, requiring prerequisite flags to have specific variant values before dependent flags can evaluate. This Enterprise feature supports both local and remote evaluation modes while preventing circular dependencies.

## Statistical Analysis and Data Quality

The platform implements Bonferroni correction for multiple hypothesis testing, automatically adjusting p-values when testing multiple variants or metrics to control family-wise error rates. This correction applies to primary metrics with multiple treatments and secondary metrics with multiple treatments or metrics.

Sample Ratio Mismatch (SRM) detection identifies allocation discrepancies through cumulative assignment charts and exposure charts, helping diagnose instrumentation errors, mid-experiment traffic changes, variant jumping, and sticky bucketing modifications. The system provides comprehensive data quality monitoring through exposure event analysis and variant distribution tracking.

Advanced outlier detection and mitigation techniques include statistical methods using standard deviations, boxplots, and percentiles, with visualization approaches and mitigation strategies like winsorization and log transforms for different metric types (totals, sum of property, average of property, funnel metrics).

## Key Nomenclature and Definitions

**Evaluation Modes**: Local evaluation (device ID bucketing, high performance) vs. remote evaluation (Amplitude ID bucketing, advanced targeting)

**Bucketing Systems**: Sticky bucketing (consistent assignment), variant jumping (assignment changes), bucketing keys (Device ID vs. Amplitude ID)

**Traffic Control**: Mutual exclusion groups (prevent simultaneous experiments), holdout groups (exclude users for impact measurement), traffic allocation percentages

**Metrics and Analysis**: Exposure events (experiment entry tracking), assignment events (variant allocation tracking), cumulative exposures graphs (exposure pattern visualization), funnel analysis with mid-funnel exposure events

**Statistical Concepts**: Sample Ratio Mismatch (SRM), Bonferroni correction, multiple hypothesis testing, family-wise error rate, winsorization

## API Endpoints and Technical Integration

The platform exposes the `/v1/vardata` endpoint for variant data retrieval, commonly used for testing CloudFront proxy configurations. Server-side implementations utilize the `fetchV2` method for variant retrieval, while client-side SDKs support `initialVariants` for SSR scenarios.

Chart settings can be imported/exported as JSON between Amplitude Experiment and Analytics, with schema including variants arrays, bucketingGroupType, userProperty, exposureEvent, experimentStartDate/EndDate, and metrics arrays. The [Experiment] Exposure event serves as the default exposure tracking mechanism.

## Advanced Use Cases and Patterns

The platform supports complex funnel analyses where exposure events occur mid-funnel, requiring careful variant jumping consideration and threshold metrics implementation using funnel counting for multi-event conversions. Cumulative exposure graph interpretation reveals experiment health through slope analysis (increasing for steady growth, decreasing for slowing exposure), inflection points indicating traffic allocation changes, and divergent lines showing staggered rollouts or caching effects.

URL redirect testing enables visual experimentation and CMS variant testing with configuration limits including local evaluation mode requirements, User bucketing units, deviceID keys, and all users audience targeting. The system integrates with WordPress and other content management systems for low-code experimentation scenarios.

## Product Ecosystem Integration

Amplitude Experiment integrates deeply with the broader Amplitude ecosystem, particularly with Amplitude Analytics for event tracking and metric calculation. The platform leverages Amplitude's user identification and segmentation capabilities, utilizing Amplitude IDs for advanced targeting in remote evaluation mode. Chart configurations and analysis results can be seamlessly shared between Experiment and Analytics platforms, enabling comprehensive user journey analysis and experiment impact measurement across the entire product analytics stack.

This comprehensive experimentation platform serves organizations requiring sophisticated A/B testing capabilities, advanced statistical analysis, and enterprise-grade traffic management across web, mobile, and server-side applications.