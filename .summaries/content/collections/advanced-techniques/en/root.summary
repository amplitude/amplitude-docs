## Amplitude Experiment Advanced Techniques

Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that enables organizations to run controlled experiments and manage feature rollouts. The advanced techniques documentation covers sophisticated experimentation methodologies, statistical analysis approaches, and complex deployment scenarios for enterprise-scale implementations.

## Product Relationships and Feature Integration

The platform operates through multiple interconnected components that work together to provide a complete experimentation ecosystem:

**Core Evaluation Modes**: The system supports both local and remote evaluation modes. Local evaluation flags use Device ID bucketing and run server-side with limited targeting capabilities, while remote evaluation flags support more complex targeting rules and use Amplitude ID for bucketing. Migration between modes requires careful consideration of bucketing behavior changes and deployment workflows.

**Traffic Management System**: Experiments utilize a sophisticated traffic allocation system involving holdout groups, mutual exclusion groups, and sticky bucketing. Holdout groups exclude user percentages to measure cumulative program impact, while mutual exclusion groups prevent users from exposure to multiple related experiments. These systems interact multiplicatively - users in multiple holdout groups experience compounded traffic reduction (e.g., 0.95 * 0.95 = 90.25% traffic).

**Flag Dependencies and Prerequisites**: Enterprise plans support flag prerequisites, creating dependency chains where evaluation of dependent flags requires prerequisite flags to have specific variant values. This enables complex experiment orchestration and chained testing scenarios while preventing circular dependencies.

**Analytics Integration**: The platform integrates deeply with Amplitude Analytics for metric analysis, outlier detection, and data visualization. Chart settings can be imported/exported as JSON between Experiment and Analytics, maintaining consistency across analysis workflows.

## Key Nomenclature and Definitions

**Sticky Bucketing**: A deterministic hashing algorithm that maintains consistent variant assignments for users even when experiment targeting criteria, percentage rollout, or rollout weights change, preventing variant jumping and ensuring user experience consistency.

**Sample Ratio Mismatch (SRM)**: A data quality issue where actual variant allocation differs from expected traffic allocation, often caused by instrumentation errors, mid-experiment changes, or authentication patterns affecting user distribution.

**Exposure Events**: Events that indicate when users are actually exposed to experiment variants, distinct from assignment events. These can be custom events occurring mid-funnel or default events triggered by flag evaluation.

**Cumulative Exposures Graph**: A visualization showing exposure event accumulation over time, with slope changes indicating exposure rate variations. Increasing slopes show steady growth, decreasing slopes indicate slowing exposure rates, and inflection points reveal traffic allocation or targeting changes.

**Winsorization**: A statistical technique for outlier mitigation that caps extreme values at specified percentiles rather than removing them entirely, preserving sample size while reducing noise.

**Multiple Hypothesis Testing**: Statistical correction applied when testing multiple variants or metrics simultaneously, using Bonferroni correction to control family-wise error rates and prevent false positive inflation.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates with several external systems and deployment architectures:

**AWS CloudFront Integration**: The platform supports reverse proxy configuration through AWS CloudFront to bypass domain blocking, using specific cache policies (CachingDisabled, AllViewExceptHostHeader, CORS-with-preflight-and-SecurityHeadersPolicy) and the `/v1/vardata` endpoint for evaluation requests.

**Server-Side Rendering (SSR)**: Implementation involves coordinating JavaScript Server SDK (`@amplitude/experiment-node-server`) and Client SDK (`@amplitude/experiment-js-client`), with server-side variant fetching and client initialization using pre-fetched variants for seamless user experiences.

**Split URL Testing**: Enables A/B testing through URL redirects without developer involvement, requiring Experiment SDK script tags in HTML `<head>` sections and Analytics SDK for event tracking, with limitations including local evaluation mode and User bucketing unit requirements.

## API Endpoints and Technical Implementation

**Primary Evaluation Endpoint**: `/v1/vardata` - Used for variant data retrieval and can be tested through CloudFront proxy configurations.

**SDK Integration Points**:
- `ExperimentServer.fetchV2()` - Server-side variant fetching
- `ExperimentClient.initialVariants` - Client-side initialization with pre-fetched variants
- `[Experiment] Exposure` events - Default exposure tracking
- `Experiment Assignment` events - Debugging sticky bucketing and variant assignments

**Configuration APIs**: JSON import/export functionality supports programmatic chart configuration with schema including variants arrays, bucketingGroupType, userProperty, exposureEvent, and metrics definitions.

**Advanced Metric Analysis**: The platform supports complex funnel analyses with mid-funnel exposure events, threshold metrics using funnel counting, and multi-event conversion tracking with experiment flag key filtering to reduce statistical noise.

The documentation emphasizes enterprise-scale considerations including traffic management complexity, statistical rigor through multiple hypothesis testing corrections, and sophisticated debugging approaches using cumulative exposure analysis and data quality monitoring.