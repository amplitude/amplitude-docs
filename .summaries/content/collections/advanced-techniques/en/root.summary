# Amplitude Experiment: Advanced Techniques

## Product Overview

Amplitude Experiment is a feature experimentation and flag management platform that enables teams to test new features, conduct A/B tests, and manage feature rollouts. The product allows for sophisticated experiment design, variant assignment, and results analysis while integrating with Amplitude Analytics for deeper insights.

Key features include:
- Local and remote evaluation modes for feature flags and experiments
- Holdout groups for measuring long-term impacts
- Mutual exclusion groups to prevent experiment interference
- Flag prerequisites for creating dependencies between features
- Advanced metrics analysis and outlier handling
- Sticky bucketing to ensure consistent user experiences

Primary use cases include A/B testing, feature rollouts, split URL testing, server-side rendering implementations, and complex experiment designs with multiple variants.

## Product Relationships and Architecture

Amplitude Experiment consists of several interconnected components:

1. **Evaluation Modes**:
   - **Local Evaluation**: Client-side evaluation using device ID for bucketing, suitable for frontend applications and URL redirect testing
   - **Remote Evaluation**: Server-side evaluation with more complex targeting rules and bucketing options

2. **Flag Types**:
   - **Feature Flags**: Simple on/off toggles for features
   - **Feature Experiments**: More complex tests with multiple variants and sticky bucketing

3. **Integration with Amplitude Analytics**:
   - Experiment results can be analyzed in Amplitude Analytics
   - Chart settings can be exported/imported between platforms
   - Custom metrics and funnels can be created based on experiment data

4. **Deployment Options**:
   - Client-side SDKs for browser applications
   - Server-side SDKs for backend implementations
   - Support for server-side rendering architectures

5. **Advanced Features** (primarily Enterprise tier):
   - Holdout groups
   - Mutual exclusion groups
   - Flag prerequisites
   - Multiple hypothesis testing corrections

## Key Nomenclature and Definitions

- **Bucketing**: The process of assigning users to experiment variants, which can be based on device ID, user ID, or other properties
- **Sticky Bucketing**: Ensures users consistently see the same variant even when targeting criteria change
- **Variant Jumping**: When users see different variants across sessions, creating inconsistent experiences
- **Exposure Event**: Tracks when a user is exposed to an experiment variant
- **Assignment Event**: Records which variant a user is assigned to
- **Sample Ratio Mismatch (SRM)**: When observed variant allocations differ significantly from specified allocations
- **Holdout Group**: A percentage of users excluded from experiments to measure long-term impacts
- **Mutual Exclusion Group**: Configuration ensuring users in one experiment cannot participate in another related experiment
- **Flag Prerequisites**: Dependencies between flags where one flag's evaluation depends on another flag's state
- **Multiple Hypothesis Testing**: Statistical challenge when running experiments with multiple variants or metrics
- **Bonferroni Correction**: Method to adjust significance thresholds when testing multiple hypotheses
- **Cumulative Exposures Graph**: Visualization showing experiment exposure patterns over time

## Product Ecosystem Integration

Amplitude Experiment integrates with:

1. **Amplitude Analytics**:
   - Experiment results are analyzed in Analytics
   - User segments from experiments can be used for funnel analysis
   - Metrics defined in Analytics can be used in Experiment

2. **Development Workflows**:
   - SDKs available for various platforms (JavaScript, server-side languages)
   - Support for CI/CD pipelines through flag management
   - Integration with server-side rendering frameworks

3. **Infrastructure**:
   - AWS CloudFront can be used as a reverse proxy for evaluation servers
   - Support for cross-region deployments

4. **Marketing Tools**:
   - Split URL testing for marketing campaigns
   - Integration with CMS platforms for content testing

## API Endpoints and SDK Usage

### JavaScript Client SDK
```javascript
// Initialize the client
const experimentClient = Experiment.initialize({
  apiKey: 'API_KEY',
  userId: 'user@example.com'
});

// Fetch variants
experimentClient.fetch().then(() => {
  const variant = experimentClient.variant('flag-key');
});
```

### JavaScript Server SDK
```javascript
// Initialize the server
const experimentServer = new ExperimentServer({
  apiKey: 'SERVER_API_KEY',
  serverUrl: 'https://api.lab.amplitude.com'
});

// Fetch variants for a user
const variants = await experimentServer.fetchV2({
  user: {
    user_id: 'user@example.com'
  }
});
```

### Server-Side Rendering Implementation
```javascript
// Server-side
const variants = await experimentServer.fetchV2({
  user: { user_id: req.user.id }
});

// Pass to client during render
const experimentClient = Experiment.initialize({
  apiKey: 'API_KEY',
  userId: req.user.id,
  initialVariants: variants
});
```

### Flag Prerequisites Configuration
```json
{
  "prerequisites": [
    {
      "flag_key": "prerequisite-flag",
      "variant": "on"
    }
  ]
}
```

### Evaluation Endpoints
- Remote evaluation: `https://api.lab.amplitude.com`
- Can be proxied through AWS CloudFront for regions with access restrictions

The platform provides comprehensive tools for experiment design, implementation, analysis, and optimization, with particular strengths in handling complex experimental scenarios and ensuring statistical validity of results.