# Amplitude Analytics Platform

Amplitude is a comprehensive digital analytics and experimentation platform that enables organizations to understand user behavior, optimize product experiences, and drive growth through data-driven insights. The platform combines behavioral analytics, A/B testing, feature flagging, customer data management, and user engagement tools in an integrated ecosystem designed for product teams, marketers, and data professionals.

## Core Product Architecture and Feature Relationships

The platform operates through several interconnected product areas that work synergistically to provide end-to-end analytics capabilities:

**Analytics Core** serves as the foundational layer, providing behavioral event tracking, user journey analysis, and conversion optimization through sophisticated charting capabilities. This includes Event Segmentation for trend analysis, Funnel Analysis for conversion tracking, Retention Analysis for user engagement measurement, and advanced visualizations like Compass for multi-dimensional analysis and Journeys for path discovery.

**Amplitude Experiment** functions as the experimentation engine, offering A/B testing and feature flagging capabilities with advanced statistical methods including mixture sequential probability ratio test (mSPRT) for continuous monitoring. The system supports both remote and local evaluation modes, deterministic randomization using murmur3 hashing, and complex experimental designs with flag dependencies and holdout groups.

**Customer Data Platform (CDP)** manages data ingestion, transformation, and activation across the ecosystem. This includes source and destination catalogs for third-party integrations, audience management for user segmentation, and warehouse-native capabilities for organizations using modern data stack architectures.

**Amplitude Guides and Surveys** provides in-product messaging and feedback collection capabilities, enabling teams to gather user insights and drive engagement through targeted communications.

**Session Replay** captures and analyzes user interactions at the session level, providing qualitative insights to complement quantitative analytics data.

## Key Nomenclature and Definitions

**Monthly Tracked Users (MTU)**: The primary billing metric representing unique users tracked within a calendar month, serving as the foundation for plan pricing across Amplitude's subscription tiers.

**Event Segmentation**: The core analytics methodology for analyzing user behavior patterns by segmenting events based on user properties, event properties, and cohort membership.

**Behavioral Cohorts**: Dynamic user segments computed hourly based on user actions and properties, enabling sophisticated targeting and analysis capabilities across the platform.

**Assignment vs. Exposure Events**: In experimentation, assignment events track when users are allocated to experiment variants through `fetch()` or `evaluate()` calls, while exposure events indicate actual feature interaction through `variant()` method access.

**Flag Dependencies**: Experimental design relationships that define evaluation order between feature flags, enabling prerequisites, mutual exclusion groups, and complex traffic allocation strategies.

**Amplitude Query Language (AQL)**: The underlying query system that powers analytics calculations, supporting complex event filtering, property aggregation, and user segmentation logic.

**Synthetic MTUs**: Additional billing units generated when event volume exceeds standard thresholds (1,000 events per MTU), ensuring fair billing for high-volume usage patterns.

**Instrumentation**: The process of implementing event tracking through Amplitude's SDKs and APIs, including proper event taxonomy, property definition, and data validation.

## Broader Product Ecosystem Integration

Amplitude operates as a comprehensive platform where individual products enhance each other's capabilities:

**Cross-Product Data Flow**: Events tracked through Analytics SDKs automatically become available for Experiment targeting, Guides and Surveys triggering, and CDP audience building, creating a unified data foundation across all product areas.

**Unified User Identity**: The platform maintains consistent user identification across all products, enabling seamless experience personalization and comprehensive user journey tracking from acquisition through retention.

**Shared Infrastructure**: All products leverage Amplitude's globally distributed infrastructure with Fastly CDN edge caching, AWS services integration, and high-availability architecture supporting enterprise-scale deployments.

**Integrated Analytics**: Experiment results appear in Analytics charts, Guides and Surveys engagement feeds into behavioral analytics, and Session Replay insights complement quantitative analysis with qualitative context.

## API Endpoints and Technical Integration

**Analytics API** (`/apis/analytics/`): Provides programmatic access to event ingestion, user property management, and query execution for custom integrations and data pipeline automation.

**Experiment APIs** (`/apis/experiment/`): Supports remote evaluation through `fetch()` endpoints, local evaluation through SDK integration, and experiment management through configuration APIs.

**Guides and Surveys API** (`/apis/guides-and-surveys/`): Enables programmatic campaign management, response collection, and integration with external messaging systems.

**SDK Ecosystem**: Comprehensive client and server-side SDKs including Browser, Android, iOS, React Native, Node.js, Python, Java, Go, Flutter, Unity, and Unreal implementations, each optimized for platform-specific requirements and performance characteristics.

**Ampli**: A code-generation tool that creates type-safe analytics implementations, ensuring data quality and reducing instrumentation errors across development teams.

The platform supports both real-time and batch data processing, with event ingestion APIs handling high-volume data streams and query APIs providing flexible access to processed analytics results. Integration patterns include direct SDK implementation, server-side API integration, and warehouse-native deployment for organizations with existing data infrastructure investments.