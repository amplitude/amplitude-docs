# Amplitude Experiment: Feature Flag and Experimentation Platform

## Overview

Amplitude Experiment is a feature flag and experimentation platform that enables developers to implement feature flags, conduct A/B tests, and manage experiments across multiple platforms. The platform supports both remote evaluation (server-controlled) and local evaluation (client-side) methods, giving developers flexibility based on their performance and reliability requirements.

Key features include:
- Feature flag management for controlled rollouts
- A/B testing capabilities
- User targeting and segmentation
- Cohort-based targeting
- Automatic exposure tracking
- Integration with Amplitude Analytics and third-party analytics
- Account-level bucketing for consistent user experiences
- Flag streaming for real-time updates
- Multi-environment support (development, staging, production)

## Product Relationships and Architecture

Amplitude Experiment consists of several interconnected components:

1. **SDK Ecosystem**:
   - Client-side SDKs: JavaScript, iOS (Swift), Android, React Native
   - Server-side SDKs: Node.js, Python, Ruby, PHP, Go, JVM (Java/Kotlin)

2. **Evaluation Methods**:
   - **Remote Evaluation**: Server-controlled evaluation where the SDK makes API calls to determine flag variants
   - **Local Evaluation**: Client-side evaluation where flag configurations are downloaded to the client for local processing

3. **Evaluation Proxy**: A service that enables local evaluation within your infrastructure, supporting platforms without native SDKs and enhancing evaluation with large cohort targeting.

4. **Integration Layer**: Connects with Amplitude Analytics and third-party analytics providers for tracking and analysis.

## Key Nomenclature and Definitions

- **Feature Flag**: A toggle that enables or disables features in an application
- **Experiment**: A controlled test comparing different variants of a feature
- **Variant**: A specific version or configuration of a feature being tested
- **Remote Evaluation**: Server-controlled flag evaluation requiring API calls
- **Local Evaluation**: Client-side flag evaluation using downloaded configurations
- **Exposure**: When a user is shown a specific variant of a feature flag
- **Exposure Tracking**: Recording which users have been exposed to which variants
- **Deployment Key**: API key used to authenticate SDK requests
- **ExperimentUser**: Object containing user properties used for targeting and evaluation
- **Cohort Targeting**: Targeting users based on predefined segments
- **Flag Configuration**: The complete set of rules and settings for a feature flag
- **Assignment Tracking**: Recording which variant a user has been assigned to
- **Account-level Bucketing**: Ensuring consistent variant assignment across multiple users in the same account

## Ecosystem Integration

Amplitude Experiment integrates with:

1. **Amplitude Analytics**: Automatic integration for tracking exposures and analyzing results
2. **Third-party Analytics**: Support for other analytics providers
3. **Amplitude Cohorts**: Using user segments defined in Amplitude for targeting
4. **EU Data Residency**: Support for EU data centers for compliance requirements

## API Endpoints and Implementation

### Common SDK Methods

1. **Initialization**:
   ```
   initialize(apiKey, config)
   ```

2. **Remote Evaluation**:
   ```
   fetch(user)
   fetchV2(user)
   ```

3. **Local Evaluation**:
   ```
   evaluate(user, flagKey)
   evaluateV2(user, flagKey)
   ```

4. **Variant Access**:
   ```
   variant(flagKey, defaultValue)
   ```

5. **Exposure Tracking**:
   ```
   exposure(flagKey, variant)
   ```

### Evaluation Proxy API Endpoints

- `GET /v1/flags`: Retrieve all flag configurations
- `POST /v1/evaluate`: Evaluate flags for a user
- `POST /v1/track`: Track assignment events

### Deployment Options

The Evaluation Proxy can be deployed using:
- Docker: `docker run amplitude/experiment-evaluation-proxy`
- Kubernetes with Helm charts
- Configuration via YAML or environment variables

Redis is recommended for persistent storage with the Evaluation Proxy to support deduplication of assignment events and caching of flag configurations.

Each SDK implementation provides language-specific features while maintaining a consistent API across platforms, with appropriate configuration options for timeout, caching, and error handling specific to each environment.