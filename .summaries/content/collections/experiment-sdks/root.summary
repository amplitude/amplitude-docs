# Amplitude Experiment SDK Suite

## High-Level Overview

Amplitude Experiment is a comprehensive feature flagging and experimentation platform that enables developers to implement feature flags, run A/B tests, and manage experiments across multiple platforms. The platform provides two primary evaluation methods:

1. **Remote Evaluation**: Server-side evaluation where Amplitude servers determine which variant a user should receive
2. **Local Evaluation**: Client-side evaluation where the SDK determines the variant based on downloaded flag configurations

Key features include:
- Feature flag management for controlled feature rollouts
- User targeting and segmentation for precise audience control
- Cohort targeting for experiment group management
- Automatic exposure tracking for analytics integration
- Integration with Amplitude Analytics for results analysis
- Support for third-party analytics providers
- Account-level bucketing for consistent user experiences
- Flag configuration streaming and polling for real-time updates
- EU data residency compliance for regulatory requirements

## Product Relationships and Features

The Experiment SDK ecosystem is structured to support various development environments:

### Client-Side SDKs:
- **JavaScript SDK**: For web applications
- **React Native SDK**: For cross-platform mobile applications
- **iOS SDK**: For native iOS applications using Swift
- **Android SDK**: For native Android applications

### Server-Side SDKs:
- **Node.js SDK**: For JavaScript server environments
- **JVM SDK**: For Java and Kotlin applications
- **Python SDK**: For Python applications
- **Ruby SDK**: For Ruby applications
- **PHP SDK**: For PHP applications
- **Go SDK**: For Go applications

### Supporting Infrastructure:
- **Experiment Evaluation Proxy**: A service enabling local evaluation within your infrastructure, particularly useful for platforms without native SDKs

The client-side SDKs typically support both remote and local evaluation methods, while server-side SDKs are optimized for local evaluation with flag configuration polling and streaming capabilities. This architecture allows for flexible implementation based on specific application requirements and performance considerations.

## Key Nomenclature and Definitions

- **Variant**: A specific version or configuration of a feature flag that can be assigned to users
- **ExperimentUser**: Object representing a user with properties used for targeting and variant assignment
- **ExperimentConfig/ExperimentClient**: Configuration and client objects used to initialize and interact with the SDK
- **Deployment Key**: API key used to authenticate with Amplitude Experiment services
- **Remote Evaluation**: Server-side determination of flag variants, requiring network requests for each evaluation
- **Local Evaluation**: Client-side determination of flag variants using downloaded flag configurations, reducing latency
- **Exposure Tracking**: Recording when a user is exposed to a specific variant for analytics purposes
- **Assignment Tracking**: Recording which variant a user is assigned to for consistency
- **Bootstrapping**: Pre-loading flag configurations for faster initial evaluation
- **Cohort Targeting**: Targeting users based on predefined cohorts from Amplitude Analytics
- **Flag Streaming**: Real-time updates to flag configurations via persistent connections
- **Flag Configuration Polling**: Periodic fetching of updated flag configurations at specified intervals
- **Account-level Bucketing**: Consistent variant assignment across multiple applications for the same user

## Product Ecosystem Integration

Amplitude Experiment integrates seamlessly with the broader Amplitude product ecosystem:

1. **Amplitude Analytics**: Automatic integration for tracking exposures and analyzing experiment results through user behavior metrics
2. **Amplitude Cohorts**: Used for targeting specific user segments in experiments based on behavioral data
3. **Amplitude Data**: Leverages user properties and behaviors for sophisticated targeting rules

The SDKs can be deployed in various environments:
- Client applications (web, mobile)
- Server applications and backend services
- Microservices architectures
- Serverless functions
- Edge computing environments

For platforms without a native SDK, the Evaluation Proxy provides a REST API that can be accessed from any environment capable of making HTTP requests.

## API Endpoints and Implementation

### Common SDK Methods

Most SDKs implement these core methods:
- `initialize(apiKey, config)`: Initialize the client with your deployment key and configuration options
- `fetch(user)`: Fetch variants for a specific user
- `variant(key, user, fallback)`: Get a specific variant for a user with an optional fallback value
- `all(user)`: Get all variants for a user
- `exposure(key, user)`: Manually track exposure to a variant

### Evaluation Proxy API

The Evaluation Proxy exposes these endpoints:
- `POST /v1/vardata`: Fetch flag configurations for local evaluation
- `GET /v1/flags`: Get all flag configurations
- `POST /v1/evaluate`: Evaluate flags for a specific user

### Configuration Options

Common configuration parameters across SDKs include:
- `serverUrl`: API endpoint (supports EU data residency through regional endpoints)
- `fetchTimeoutMillis`: Timeout for fetch requests
- `fetchRetries`: Number of retry attempts for failed requests
- `fetchRetryBackoffMinMillis`/`fetchRetryBackoffMaxMillis`: Retry backoff timing parameters
- `flagConfigPollingIntervalMillis`: Interval for polling flag configurations in local evaluation mode
- `automaticExposureTracking`: Enable/disable automatic exposure tracking when variants are accessed

The SDKs provide a consistent developer experience across platforms while accommodating platform-specific requirements and best practices, enabling seamless implementation of feature flags and experiments in diverse technical environments.