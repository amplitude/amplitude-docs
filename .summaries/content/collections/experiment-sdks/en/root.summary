## Amplitude Experiment SDK Suite

Amplitude Experiment is a comprehensive feature flagging and experimentation platform that provides SDKs across multiple programming languages and platforms for both client-side and server-side implementations. The platform enables A/B testing, feature rollouts, and dynamic configuration management through a unified evaluation system.

## Product Architecture and Relationships

The Experiment SDK ecosystem is built around two primary evaluation modes:

**Remote Evaluation**: Client applications fetch pre-computed variant assignments from Amplitude's servers via API calls. This approach is typically used for client-side implementations where evaluation logic remains server-controlled.

**Local Evaluation**: Applications download flag configurations and perform evaluation logic locally. This mode supports real-time flag config polling, streaming updates, and cohort targeting capabilities, making it suitable for high-performance server-side implementations.

The **Evaluation Proxy** serves as a bridge component, enabling local evaluation capabilities on platforms without native SDK support through a Docker-based service that provides HTTP endpoints for flag evaluation.

## Platform Coverage and Integration Patterns

### Client-Side SDKs
- **Mobile**: Android (Kotlin/Java), iOS (Swift/Objective-C), React Native
- **Web**: JavaScript (browser), with SSR support
- **Integration Methods**: 
  - Native Amplitude Analytics integration via `initializeWithAmplitudeAnalytics()`
  - Third-party analytics (Segment, mParticle) through provider interfaces
  - Standalone implementation with custom tracking

### Server-Side SDKs
- **Languages**: Node.js, Python, Ruby, PHP, Go, JVM (Java/Kotlin)
- **Capabilities**: Both remote and local evaluation modes, cohort targeting, assignment tracking
- **Enterprise Features**: EU data residency, custom HTTP clients, advanced configuration options

## Key Nomenclature and Definitions

**Deployment Key**: Authentication credential that identifies the experiment environment and determines available flag configurations.

**Variants**: The different experiences or values that users can receive for a given feature flag, accessed via `variant()` methods across SDKs.

**ExperimentUser**: User context object containing `device_id`, `user_id`, `user_properties`, and `group_properties` used for flag evaluation and targeting.

**Exposure Tracking**: Automatic or manual logging of when users are exposed to experiment variants, enabling accurate experiment analysis.

**Cohort Targeting**: Advanced targeting capability that syncs user cohorts from Amplitude Analytics for precise audience segmentation in local evaluation mode.

**Bootstrapping**: Pre-loading initial flag configurations or variants to reduce latency, implemented through `initialVariants` or `initialFlags` configuration.

**Flag Configurations**: The complete rule set and targeting logic for feature flags, downloaded and cached for local evaluation.

## Ecosystem Integration and Data Flow

The Experiment platform integrates deeply with the broader Amplitude ecosystem:

**Analytics Integration**: Native integration with Amplitude Analytics automatically handles exposure event tracking and user identity management. The `AmplitudeCookie` utility enables device ID consistency across web implementations.

**Data Residency**: EU data center support through `serverZone` configuration ensures compliance with regional data requirements.

**Assignment Tracking**: Automatic assignment event logging with 24-hour deduplication prevents duplicate experiment exposures from skewing results.

**Provider Interfaces**: Standardized interfaces (`ExperimentUserProvider`, `ExposureTrackingProvider`) enable custom user management and analytics integration patterns.

## Technical Implementation Patterns

### Common API Patterns
- **Initialization**: `ExperimentClient.initialize()` or `initializeWithAmplitudeAnalytics()`
- **Remote Evaluation**: `fetch()` or `fetchV2()` methods retrieve variants from API
- **Local Evaluation**: `start()` method initializes local evaluation with config polling
- **Variant Access**: `variant(flagKey)` returns experiment assignments
- **Evaluation**: `evaluate()` or `evaluateV2()` performs local flag evaluation

### Configuration Management
- **Polling**: `flagConfigPollingIntervalMillis` controls config refresh frequency
- **Streaming**: `streamUpdates` enables real-time flag configuration updates
- **Timeouts**: `fetchTimeoutMillis` and retry configurations for network resilience
- **Cohort Sync**: `CohortSyncConfig` manages audience targeting data synchronization

### Deployment Options
- **Docker**: Evaluation Proxy available as `amplitudeinc/evaluation-proxy` image
- **Kubernetes**: Helm chart deployment support for containerized environments
- **Package Managers**: Native distribution through npm, CocoaPods, Maven Central, PyPI, RubyGems, Composer, and Go modules

The platform supports both stateless and stateful deployment patterns, with Redis persistence for the Evaluation Proxy and local caching capabilities across all SDKs to ensure high availability and performance.