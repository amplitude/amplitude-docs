# Amplitude Experiment: Feature Flag and Experimentation Platform

## Product Overview

Amplitude Experiment is a comprehensive feature flag and experimentation platform that enables developers to implement feature flags, run A/B tests, and manage experiments across various platforms and programming languages. The platform supports both remote evaluation (server-controlled flag evaluation) and local evaluation (client-side flag evaluation) methods, allowing for flexible implementation based on performance and reliability requirements.

Key features include:
- Feature flag management for controlled feature rollouts
- A/B testing and experimentation capabilities
- User targeting and segmentation
- Cohort targeting for advanced user segmentation
- Automatic exposure tracking for analytics
- Integration with Amplitude Analytics and third-party analytics providers
- Account-level bucketing for consistent user experiences
- Flag streaming for real-time updates
- Support for multiple deployment environments

## Product Architecture and Relationships

Amplitude Experiment consists of several interconnected components:

1. **SDK Ecosystem**: Language-specific SDKs for client and server implementations:
   - Client-side: JavaScript, iOS (Swift), Android, React Native
   - Server-side: Node.js, Python, Ruby, PHP, Go, JVM (Java/Kotlin)

2. **Evaluation Methods**:
   - **Remote Evaluation**: Server-controlled flag evaluation where the SDK makes API calls to Amplitude's servers to determine flag variants
   - **Local Evaluation**: Client-side flag evaluation where flag configurations are downloaded to the client and evaluation happens locally

3. **Evaluation Proxy**: A service that enables local evaluation within your infrastructure, supporting platforms without native SDKs and enhancing evaluation with large cohort targeting.

4. **Integration Layer**: Connects with Amplitude Analytics and third-party analytics providers for tracking exposures and analyzing experiment results.

## Key Nomenclature and Definitions

- **Feature Flag**: A toggle that enables or disables features in an application
- **Experiment**: A controlled test comparing different variants of a feature
- **Variant**: A specific version or configuration of a feature being tested
- **Remote Evaluation**: Server-controlled flag evaluation requiring API calls for each evaluation
- **Local Evaluation**: Client-side flag evaluation using downloaded flag configurations
- **Exposure**: When a user is shown a specific variant of a feature flag
- **Exposure Tracking**: Recording which users have been exposed to which variants
- **Deployment Key**: API key used to authenticate SDK requests to Amplitude
- **ExperimentUser**: Object containing user properties used for targeting and evaluation
- **Cohort Targeting**: Targeting users based on predefined segments or cohorts
- **Flag Configuration**: The complete set of rules and settings for a feature flag
- **Assignment Tracking**: Recording which variant a user has been assigned to
- **Account-level Bucketing**: Ensuring consistent variant assignment across multiple users in the same account

## Product Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude product ecosystem:

1. **Amplitude Analytics**: Automatic integration for tracking exposures and analyzing experiment results
2. **Third-party Analytics**: Support for integration with other analytics providers
3. **Amplitude Cohorts**: Leveraging user segments defined in Amplitude for targeting
4. **EU Data Residency**: Support for EU data centers for compliance with data regulations

The platform is designed to work alongside other development and analytics tools, with specific integrations for popular frameworks and platforms.

## API Endpoints and Command Line Usage

### Common API Methods Across SDKs

1. **Initialization**:
   ```
   initialize(apiKey, config)
   ```

2. **Remote Evaluation**:
   ```
   fetch(user)
   fetchV2(user)
   ```

3. **Local Evaluation**:
   ```
   evaluate(user, flagKey)
   evaluateV2(user, flagKey)
   ```

4. **Variant Access**:
   ```
   variant(flagKey, defaultValue)
   ```

5. **Exposure Tracking**:
   ```
   exposure(flagKey, variant)
   ```

### Evaluation Proxy API Endpoints

- `GET /v1/flags`: Retrieve all flag configurations
- `POST /v1/evaluate`: Evaluate flags for a user
- `POST /v1/track`: Track assignment events

### Deployment Options

The Evaluation Proxy can be deployed using:
- Docker: `docker run amplitude/experiment-evaluation-proxy`
- Kubernetes with provided Helm charts
- Configuration via YAML or environment variables

Redis is recommended for persistent storage with the Evaluation Proxy to support deduplication of assignment events and caching of flag configurations.

## SDK-Specific Features

Each SDK implementation provides language-specific features while maintaining a consistent API across platforms:

- **JavaScript**: Support for server-side rendering and browser environments
- **iOS/Android**: Native mobile implementation with automatic exposure tracking
- **React Native**: Cross-platform mobile support with AsyncStorage integration
- **Server-side SDKs**: Support for flag streaming, cohort targeting, and high-performance evaluation
- **JVM/Go/Python**: Enterprise-focused implementations with advanced configuration options

All SDKs support both remote and local evaluation methods, with appropriate configuration options for timeout, caching, and error handling specific to each platform.