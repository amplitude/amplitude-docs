# Amplitude Experiment Troubleshooting

Amplitude Experiment is a feature flagging and A/B testing platform that enables controlled rollouts and experimentation through variant assignment and exposure tracking. The platform operates on a sophisticated dual-event model designed to ensure data integrity and accurate experiment analysis across diverse user scenarios and technical implementations.

## Product Relationships and Features

The troubleshooting framework centers around four interconnected diagnostic areas that form the backbone of Amplitude Experiment's architecture:

**Assignment and Exposure Event System** serves as the foundational tracking mechanism, where assignment events capture user bucketing through `fetch()` calls while exposure events record actual feature interactions via `variant()` calls. This dual-event architecture enables precise measurement of experiment reach versus actual feature usage, forming the basis for most troubleshooting scenarios and data validation processes.

**Variant Distribution and Bucketing** manages user allocation through a sophisticated system of configurable distribution weights, bucketing salts, and sticky bucketing mechanisms. These components work in concert to maintain consistent user experiences while enabling controlled experiment modifications and ensuring statistical validity of results.

**Identity Management** handles complex user identity scenarios including multiple identity types (device ID, user ID, Amplitude ID), identity transitions during anonymous-to-authenticated flows, and account switching scenarios. This system ensures proper user tracking across sessions and platforms while maintaining experiment integrity.

**Experiment Lifecycle Management** supports dynamic experiment operations through configurable parameters, experiment restarts, configuration changes, and analysis window adjustments without compromising historical data or ongoing experiment validity.

## Key Nomenclature and Definitions

**Assignment Event**: Generated automatically when users are bucketed into variants through `fetch()` calls, establishing definitive experiment participation and forming the denominator for exposure rate calculations.

**Exposure Event**: Triggered when users interact with variant-controlled features via `variant()` calls, indicating actual feature exposure and forming the numerator for engagement metrics.

**Variant Jumping**: A critical data integrity issue where users are exposed to multiple variants within a single experiment, potentially caused by identity mismatches, targeting changes, or improper SDK implementation.

**Sample Ratio Mismatch (SRM)**: Statistical anomaly occurring when observed variant allocation significantly deviates from configured allocation percentages, indicating potential experiment integrity issues or implementation problems.

**Sticky Bucketing**: Persistence mechanism ensuring users remain consistently assigned to their designated variants across sessions, devices, and time periods, maintaining experiment validity and user experience consistency.

**Bucketing Salt**: Randomization parameter controlling user-to-variant assignment distribution, modifiable to create new experiment runs while maintaining statistical randomization properties.

**Fallback Variant**: Default variant configuration served when users fail to meet targeting criteria or during local evaluation exclusions, ensuring graceful degradation of experiment functionality.

**Experiment Key (expKey)**: Unique identifier for specific experiment runs, automatically updated when creating new experiment iterations or restarting experiments with modified parameters.

## Broader Product Ecosystem Integration

Amplitude Experiment integrates seamlessly with the comprehensive Amplitude Analytics platform, leveraging unified user streams and event data for sophisticated experiment analysis and user behavior insights. The system supports both remote and local evaluation modes, with local evaluation providing performance optimization while requiring careful identity management and configuration synchronization.

The platform maintains compatibility across multiple SDK implementations spanning web and mobile platforms (JavaScript, Android, iOS, React Native), each with specific version requirements for advanced features like experiment restart capabilities and enhanced identity management. The centralized Evaluation API serves as the primary orchestration point for variant assignment and configuration distribution.

Experiment data flows directly into Amplitude's analytics ecosystem, enabling advanced analysis through user journey mapping, diagnostic visualization, and cumulative exposure tracking. The system supports sophisticated targeting mechanisms including rule-based criteria, mutual exclusion groups, inclusion lists, and dynamic user property evaluation.

## API Endpoints and Technical Implementation

**Core SDK Methods**:
- `fetch()`: Primary method for retrieving variant assignments and automatically generating assignment events for experiment participation tracking
- `variant()`: Accessor method for variant values that triggers exposure events when users interact with experiment-controlled features

**Evaluation API**: Central service endpoint processing experiment configurations, user targeting rules, and variant assignments based on real-time user properties and experiment parameters.

**SDK Version Requirements for Advanced Features**:
- JavaScript SDK v1.10.2+ for experiment restart support and enhanced identity management
- Android SDK v1.10.0+ for experiment restart capabilities and improved local evaluation
- iOS SDK v1.11.0+ for experiment restart functionality and optimized performance
- React Native SDK v1.2.0+ for cross-platform experiment restart support

The troubleshooting methodology emphasizes maintaining proper identity consistency between assignment and exposure tracking, with particular focus on device ID and user ID alignment throughout the complete experiment lifecycle. This approach prevents common implementation issues including exposures without assignments, variant jumping anomalies, and identity-related data integrity problems that can compromise experiment validity and statistical analysis accuracy.