# Amplitude Experiment Troubleshooting Guide

## Product Overview

Amplitude Experiment is an experimentation platform for creating, deploying, and analyzing A/B tests and feature flags. It uses a system of assignments and exposures to track user interactions with experiments, enabling data-driven decision making. The platform provides robust diagnostic tools to ensure experiment integrity and reliable results.

## Key Features and Concepts

### Core Functionality
- **Experiment Runs**: Configurable instances that can be restarted with new parameters
- **Variant Assignment**: Process of allocating users to specific experiment variants
- **Exposure Tracking**: Recording when users are shown a specific variant
- **Sticky Bucketing**: Ensures users consistently see the same variant across sessions
- **Diagnostic Charts**: Tools to identify and troubleshoot experiment data quality issues

### Data Quality Monitoring
- **Sample Ratio Mismatch (SRM)**: Detection of statistically significant deviations from expected variant distributions
- **Variant Jumping Detection**: Identifies when users see multiple variants in a single experiment
- **Exposure-Assignment Correlation**: Ensures proper tracking of both assignment and exposure events

## Product Relationships and Architecture

Amplitude Experiment operates within the broader Amplitude ecosystem with these interconnected components:

1. **Client-Side SDKs**: JavaScript, React Native, iOS, and Android SDKs that handle variant assignment and exposure tracking
2. **Evaluation API**: Server-side component that determines which variant a user should see
3. **Analytics Integration**: Experiment data flows into Amplitude Analytics for analysis
4. **Diagnostic Dashboard**: Built-in troubleshooting features to identify data quality issues

## Key Nomenclature and Definitions

- **Assignment Event**: Records which variant a user is assigned to when qualifying for an experiment
- **Exposure Event**: Tracks when a user actually sees or experiences a variant
- **Variant Jumping**: When a user sees multiple variants for a single experiment
- **Bucketing Salt**: Parameter that determines how users are randomly distributed across variants
- **Experiment Key**: Unique identifier for an experiment run
- **Fallback Variant**: Default variant shown when targeting or evaluation fails
- **Inclusion List**: Predefined list of users targeted for specific variants
- **Bucketing Key**: User identifier used for consistent variant assignment

## Common Issues and Troubleshooting

### Exposures without Assignments
This diagnostic chart identifies users who received experiment exposures without proper assignment events. Causes include:
- Identity mismatches between assignment and exposure tracking
- Account switching or user identity changes
- Improper tracking of fallback variants
- Issues with `fetch()` calls or `variant()` method implementation

### Sample Ratio Mismatch (SRM)
SRM occurs when observed variant allocations significantly differ from specified allocations. Common causes include:
- Mid-experiment changes to variant distribution weights
- Misalignment between experiment start and analysis window
- Variant jumping issues
- Instrumentation problems with exposure tracking

### Variant Jumping
This issue occurs when users see multiple variants for a single experiment. Normal causes include:
- Targeting changes during the experiment
- Anonymous identity merging with authenticated users
- Inclusion list configuration issues

Abnormal causes typically involve identity mismatches between assignment and exposure events.

## Implementation Details

### API Methods
- **fetch()**: SDK method to retrieve variant assignments
- **variant()**: Returns the assigned variant for a user
- **Evaluation API**: Server-side endpoint for determining variant assignments

### SDK Version Requirements
Specific SDK versions are required for features like experiment keys:
- Client-side JavaScript SDK v1.9.0+
- React Native SDK v1.1.0+
- iOS SDK v1.12.0+
- Android SDK v1.10.0+

### Creating New Experiment Runs
When data quality issues are detected, users can create new runs by:
1. Changing the experiment key to differentiate runs
2. Adjusting the bucketing salt to re-randomize user assignments
3. Ensuring SDK versions support the new experiment key parameter

## Debugging Approaches
- User timeline analysis to identify variant jumping patterns
- Diagnostic charts for detecting exposures without assignments
- Statistical analysis for sample ratio mismatch detection
- Identity verification between assignment and exposure events

The product emphasizes proper implementation of identity tracking and exposure events to ensure experimental integrity and reliable results.