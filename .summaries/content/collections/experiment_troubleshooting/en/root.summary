# Amplitude Experiment: Troubleshooting Guide

## Product Overview

Amplitude Experiment is an experimentation platform that allows users to create, deploy, and analyze A/B tests and feature flags. The platform uses a bucketing system to assign users to different variants of an experiment and tracks their interactions through assignment and exposure events. Key features include variant allocation, user targeting, experiment diagnostics, and integration with the broader Amplitude analytics ecosystem.

## Key Features and Concepts

### Core Experimentation Mechanics
- **Variants**: Different versions of a feature or experience being tested
- **Bucketing**: The process of assigning users to experiment variants
- **Sticky Bucketing**: Ensures users consistently see the same variant across sessions
- **Bucketing Salt**: A parameter that determines the randomization of user assignments
- **Experiment Key**: Unique identifier for an experiment run that can be changed to create a new run

### Event Tracking
- **Assignment Events**: Record when a user is assigned to a variant
- **Exposure Events**: Record when a user actually sees or experiences a variant
- **Conversion Events**: User actions tracked to measure experiment outcomes

### Diagnostic Tools
- **Sample Ratio Mismatch (SRM)**: Detects when observed variant allocations differ from specified allocations
- **Exposures without Assignments**: Identifies users who received experiment exposures without proper assignment events
- **Variant Jumping**: Detects when users see multiple variants for a single experiment

## Relationship Between Features

Amplitude Experiment uses a two-step process for experimentation:
1. **Assignment**: Users are assigned to variants based on targeting rules and allocation percentages
2. **Exposure**: Users experience the assigned variant when they encounter the feature

The diagnostic tools monitor the relationship between these steps to ensure experiment integrity:
- SRM checks if the distribution of users matches the intended allocation
- Exposures without Assignments identifies cases where the exposure step occurs without proper assignment
- Variant Jumping identifies users who experience multiple variants, potentially compromising results

## Key Nomenclature and Definitions

- **Experiment Run**: A specific instance of an experiment with its own bucketing salt and user assignments
- **Fallback Variant**: The default variant shown when targeting or assignment fails
- **Rule-based Targeting**: Criteria that determine which users are eligible for an experiment
- **Mutual Exclusivity**: Ensures users can only participate in one experiment from a group
- **Identity Mismatch**: When different user identifiers are used for assignment and exposure events
- **Variant Distribution Weights**: The percentage allocation of users to each variant
- **Analysis Window**: The time period used for analyzing experiment results
- **Assignment-to-Exposure Conversion**: The rate at which assigned users actually experience the variant

## Integration with Amplitude Ecosystem

Amplitude Experiment integrates with the broader Amplitude analytics platform:
- Experiment results can be analyzed using Amplitude Analytics
- User properties and behaviors from Amplitude can be used for experiment targeting
- Diagnostic tools leverage Amplitude's event tracking infrastructure
- User timeline analysis in Amplitude can be used to debug variant jumping and other issues

## API and Implementation Details

### SDK Methods
- **fetch()**: Retrieves variant assignments for a user
- **variant()**: Returns the assigned variant for a specific experiment

### Implementation Patterns
- Client-side SDKs track both assignment and exposure events
- Server-side implementations may require separate tracking of exposure events
- Custom exposure events can be implemented for specific use cases

## Troubleshooting Workflows

### Sample Ratio Mismatch (SRM)
1. Check if variant distribution weights were changed during the experiment
2. Verify alignment of analysis window with experiment start/end dates
3. Investigate variant jumping using diagnostic charts
4. Check assignment-to-exposure conversion rates
5. Review instrumentation for tracking issues

### Variant Jumping
1. Determine if jumping is due to normal causes (targeting changes, anonymous identity merging)
2. Check for identity mismatches between assignment and exposure events
3. Analyze user timelines to identify patterns
4. Implement consistent bucketing key usage across platforms

### Exposures without Assignments
1. Check for identity mismatches between fetch() and variant() calls
2. Investigate account switching scenarios
3. Verify proper tracking of fallback variants
4. Review implementation of rule-based targeting

### Creating a New Experiment Run
1. Use the experiment key property to differentiate runs
2. Understand that a new run re-randomizes users to variants
3. Ensure SDK compatibility with the new experiment key
4. Configure proper exposure event tracking for the new run