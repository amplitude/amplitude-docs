# Amplitude Experiment Troubleshooting Guide

## Product Overview

Amplitude Experiment is an experimentation platform that allows users to create, deploy, and analyze A/B tests and feature flags. The platform helps product teams make data-driven decisions by testing hypotheses about product changes and measuring their impact on user behavior. Key features include variant allocation, user targeting, experiment diagnostics, and integration with the broader Amplitude analytics ecosystem.

## Key Features and Concepts

- **Experiment Runs**: Configurable test iterations that can be restarted with different parameters
- **Variant Assignment**: The process of allocating users to specific test variations
- **Exposure Events**: Tracking events that record when users encounter an experiment variant
- **Assignment Events**: Events that record which variant a user is assigned to
- **Sticky Bucketing**: Ensures users consistently see the same variant across sessions
- **Sample Ratio Mismatch (SRM)**: Diagnostic tool that detects when observed variant distributions differ from expected allocations
- **Variant Jumping**: Issue where users see multiple variants in a single experiment
- **Diagnostic Charts**: Visual tools for identifying and troubleshooting experiment issues

## Product Relationships and Architecture

Amplitude Experiment works within the broader Amplitude ecosystem, connecting with:

1. **Amplitude Analytics**: For tracking user behavior and experiment results
2. **Amplitude SDKs**: Client-side libraries that handle variant assignment and exposure tracking
3. **Experiment Dashboard**: Interface for monitoring experiment health and results

The platform uses a client-server architecture where:
- Server-side logic handles user targeting and variant allocation
- Client-side SDKs manage variant assignment, exposure tracking, and fallback handling
- Analytics backend processes and analyzes experiment data

## Key Nomenclature and Definitions

- **Experiment Key**: Unique identifier for an experiment, used to differentiate between runs
- **Bucketing Salt**: Parameter that influences random variant assignment
- **Fallback Variant**: Default variant shown when targeting or assignment fails
- **Inclusion List**: Predefined list of users targeted for specific variants
- **Rule-based Targeting**: Conditions that determine which users are included in an experiment
- **Mutually Exclusive Experiments**: Experiments configured so users can only participate in one
- **Bucketing Key**: Identifier used to consistently assign users to variants
- **Analysis Window**: Time period used for evaluating experiment results
- **Individual Allocation**: Specific variant assignment for a particular user

## Troubleshooting Features

The product includes several diagnostic tools and troubleshooting workflows:

1. **Sample Ratio Mismatch (SRM) Detection**:
   - Identifies statistically significant deviations in variant distributions
   - Provides analysis of potential causes including variant distribution changes, misaligned analysis windows, and instrumentation issues

2. **Exposures without Assignments Diagnostic**:
   - Tracks users who received experiment exposures without proper assignment events
   - Helps identify identity mismatches, account switching issues, or improper tracking

3. **Variant Jumping Analysis**:
   - Detects when users see multiple variants in a single experiment
   - Distinguishes between normal causes (targeting changes, anonymous identity merging) and abnormal causes (identity mismatches)

4. **New Experiment Run Creation**:
   - Process for restarting experiments after fixing data quality issues
   - Requires specific SDK versions and configuration changes

## API and Implementation Details

The product offers several key methods for implementation:

1. **SDK Methods**:
   - `fetch()`: Retrieves variant assignments from the server
   - `variant()`: Returns the assigned variant for a user

2. **Required SDK Versions**:
   - Client-side JavaScript SDK v1.9.0+ for new experiment runs
   - Server-side Node.js SDK v1.7.0+ for new experiment runs

3. **Implementation Requirements**:
   - Proper identity management between assignment and exposure events
   - Consistent bucketing key usage across the user journey
   - Correct configuration of experiment keys when creating new runs

The documentation provides detailed guidance on diagnosing and resolving common experiment issues, ensuring data quality, and maintaining the integrity of experiment results.