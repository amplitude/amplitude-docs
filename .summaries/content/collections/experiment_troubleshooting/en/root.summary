# Amplitude Experiment Product Overview

## Product Overview

Amplitude Experiment is an experimentation platform that allows users to create, manage, and analyze A/B tests and feature flags. The platform helps product teams make data-driven decisions by testing hypotheses and measuring the impact of changes on user behavior. Amplitude Experiment integrates with the broader Amplitude analytics ecosystem to provide comprehensive experiment analysis.

## Key Features and Concepts

1. **Experiment Runs**: Configurable test iterations that can be restarted to exclude previous user data from monitoring and analysis.

2. **Variant Assignment**: The process of allocating users to different experiment variants based on configured distribution weights.

3. **Exposure Events**: Tracking events that record when users are shown a specific variant of an experiment.

4. **Assignment Events**: Events that record when users are assigned to a specific variant.

5. **Sticky Bucketing**: A mechanism that ensures users consistently see the same variant throughout an experiment.

6. **Diagnostic Charts**: Visual tools that help identify issues with experiment implementation, including "Exposures without Assignments" and Sample Ratio Mismatch detection.

7. **Fallback Variants**: Default variants shown to users when the primary assignment mechanism fails.

## Relationships Between Features

Amplitude Experiment uses a multi-step process for experimentation:

1. Users are assigned to variants through a bucketing mechanism that uses a bucketing salt to determine variant allocation.
2. When users encounter the experiment, exposure events are tracked.
3. The platform expects a consistent relationship between assignments and exposures.
4. Diagnostic tools monitor the integrity of this relationship to ensure valid experiment results.

The platform supports both client-side and server-side implementations, with SDKs that handle variant assignment and exposure tracking.

## Key Nomenclature and Definitions

- **Bucketing Salt**: A unique identifier used to randomize user assignments to variants.
- **Experiment Key**: A property that identifies a specific experiment run, allowing differentiation between multiple runs of the same experiment.
- **Sample Ratio Mismatch (SRM)**: A statistical anomaly where the observed distribution of users across variants differs significantly from the configured allocation.
- **Variant Jumping**: A phenomenon where users see multiple variants of the same experiment, potentially compromising experiment integrity.
- **Assignment-to-Exposure Conversion**: The rate at which users who are assigned to a variant actually experience (are exposed to) that variant.
- **Inclusion List**: A predefined list of users who should be included in a specific variant, overriding random assignment.
- **Targeting**: Rules that determine which users are eligible to participate in an experiment.

## Integration with Amplitude Ecosystem

Amplitude Experiment integrates with the broader Amplitude analytics platform, allowing:

1. User behavior analysis based on experiment variants
2. Cohort creation from experiment participants
3. Diagnostic tools that leverage Amplitude's analytics capabilities
4. Timeline analysis to track user interactions with experiments

## Troubleshooting Features

The platform includes several diagnostic tools to ensure experiment integrity:

1. **Exposures without Assignments Chart**: Identifies users who received experiment exposures without proper assignment events, which may indicate:
   - Identity mismatches between assignment and exposure
   - Account switching issues
   - Improper tracking of fallback variants

2. **Sample Ratio Mismatch Detection**: Identifies statistically significant deviations from expected variant distributions, which may be caused by:
   - Changes to variant distribution weights
   - Instrumentation issues
   - Analysis window misalignment
   - Traffic allocation changes

3. **Variant Jumping Diagnostics**: Helps identify and resolve issues where users see multiple variants, distinguishing between normal causes (targeting changes, anonymous identity merging) and abnormal causes (identity mismatches).

## Implementation Considerations

When implementing Amplitude Experiment, users should consider:

1. Proper configuration of experiment keys to differentiate between runs
2. Consistent identity management across assignment and exposure events
3. Careful management of targeting rules and inclusion lists
4. Monitoring of diagnostic charts to ensure experiment integrity
5. Proper handling of anonymous users and identity merging

The platform provides detailed user timeline analysis to help debug issues with individual user experiences within experiments.