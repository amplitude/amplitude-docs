# Amplitude Experiment Platform

Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that enables organizations to manage experiments, feature flags, and traffic allocation through both evaluation and management APIs. The platform supports both remote and local evaluation modes, allowing teams to control feature rollouts, conduct experiments, and manage user targeting at scale.

## Product Architecture and Feature Relationships

The Amplitude Experiment platform consists of two primary API layers that work together to provide complete experimentation capabilities:

**Evaluation API** serves as the runtime engine, providing real-time variant assignments through the `/v1/vardata` endpoint for remote evaluation and flag configuration downloads via `/v1/flags` for local evaluation. This API uses deployment key authentication and supports regional deployments across US and EU servers.

**Management API** functions as the control plane, offering comprehensive CRUD operations across six core entity types: flags, experiments, mutex groups, holdouts, deployments, and versions. This API uses management API key authentication and provides programmatic control over the entire experiment lifecycle.

The relationship between these APIs follows a configuration-to-execution pattern where the Management API defines experiment parameters, targeting rules, and traffic allocation, while the Evaluation API delivers these configurations to client applications for real-time decision making.

## Key Nomenclature and Definitions

**Experiments and Flags**: Experiments represent A/B tests with statistical analysis capabilities, while flags are feature toggles for controlling feature availability. Both support variants (different versions/treatments) and can be assigned to deployments.

**Deployments**: Environment-specific configurations (client/server types) that experiments and flags can be assigned to, enabling environment-based rollout control.

**Variants**: Different treatments or versions within an experiment or flag, each containing payload data and rollout weights for traffic distribution.

**Bucketing Configuration**: The mechanism for user assignment including bucketing keys (typically `amplitude_id` or `device_id`), bucketing salt for randomization, and rollout percentages for traffic control.

**Evaluation Modes**: 
- **Remote evaluation**: Server-side variant assignment through API calls
- **Local evaluation**: Client-side evaluation using downloaded flag configurations

**Traffic Management Entities**:
- **Mutex Groups**: Ensure mutually exclusive experiment participation through slot-based traffic allocation
- **Holdout Groups**: Control groups excluded from experiments for measuring overall impact
- **Target Segments**: User targeting rules based on cohorts, properties, and conditions

**User Inclusions/Exclusions**: Manual user assignment mechanisms supporting up to 500 individual user overrides per variant.

## Ecosystem Integration

The Amplitude Experiment platform integrates deeply with the broader Amplitude analytics ecosystem through several mechanisms:

**Event Tracking**: The `X-Amp-Exp-Track` header enables automatic generation of `[Experiment] Assignment` events, feeding experiment data into Amplitude's analytics pipeline for statistical analysis and reporting.

**User Identification**: The platform leverages Amplitude's user identification system using `user_id`, `device_id`, and `amplitude_id` for consistent user bucketing and cross-platform experiment tracking.

**Regional Architecture**: Supports data residency requirements with dedicated EU endpoints (`api.lab.eu.amplitude.com`, `experiment-eu.amplitude.com`) alongside standard US endpoints.

**Versioning System**: Maintains complete audit trails of experiment and flag changes through the versions API, enabling rollback capabilities and change tracking.

## API Endpoints and Technical Implementation

**Evaluation API Endpoints**:
- `GET /v1/vardata` - Retrieve variant assignments for remote evaluation
- `GET /v1/flags` - Download flag configurations for local evaluation

**Management API Endpoints** (all under `/api/1/`):
- **Flags**: `/flags` - Complete CRUD operations for feature flags
- **Experiments**: `/experiments` - Full experiment lifecycle management  
- **Deployments**: `/deployments` - Environment configuration management
- **Mutex Groups**: `/mutexs` - Traffic allocation and experiment exclusivity
- **Holdouts**: `/holdouts` - Control group management
- **Versions**: `/versions` - Configuration change history

**Authentication Methods**:
- Evaluation API: `Api-Key` header with deployment keys
- Management API: `Authorization: Bearer <management-api-key>`

**Rate Limiting**: Management API enforces 100 requests per second with 100,000 daily request limits, while supporting cursor-based pagination for large dataset retrieval.

The platform's technical architecture supports enterprise-scale experimentation with comprehensive targeting capabilities, statistical rigor through holdout groups and mutex controls, and flexible deployment options across client and server environments.