# Amplitude Experiment Platform

Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that enables teams to manage experiments, feature flags, and user targeting through both evaluation and management APIs. The platform supports both remote and local evaluation modes, allowing for flexible deployment strategies across client and server environments.

## Product Architecture and Feature Relationships

The Amplitude Experiment platform consists of two primary API layers:

**Evaluation API** serves as the runtime engine for retrieving variant assignments and flag configurations. It provides the `/v1/vardata` endpoint for remote evaluation of user assignments and the `/v1/flags` endpoint for downloading flag configurations for local evaluation. This API uses deployment key authentication and supports regional server deployments.

**Management API** provides comprehensive CRUD operations across six core entity types:
- **Flags**: Feature flag configuration with variants, targeting, and rollout controls
- **Experiments**: A/B test setup with bucketing, segments, and statistical configuration  
- **Deployments**: Environment assignments that flags and experiments can be deployed to
- **Mutex Groups**: Traffic allocation management with slot-based experiment assignment
- **Holdout Groups**: Control group management for experiment isolation
- **Versions**: Historical tracking of flag and experiment configurations

These entities work together hierarchically - flags and experiments are assigned to deployments, can be organized within mutex groups for traffic management, and can have associated holdout groups for control measurement.

## Key Nomenclature and Definitions

**Evaluation Modes**: 
- *Remote evaluation*: Server-side evaluation where the API returns final variant assignments
- *Local evaluation*: Client-side evaluation where flag configurations are downloaded for local processing

**Bucketing Configuration**:
- *bucketingKey*: User identifier for consistent assignment (typically `amplitude_id` or `device_id`)
- *bucketingSalt*: Randomization seed for traffic distribution
- *rolloutPercentage*: Traffic allocation percentage for experiments/flags

**Targeting and Segmentation**:
- *targetSegments*: Conditional logic for user inclusion/exclusion
- *individualInclusion/Exclusion*: Specific user targeting overrides
- *rolloutWeights*: Percentage distribution across variants

**Traffic Management**:
- *Mutex Groups*: Prevent experiment overlap through slot-based allocation
- *Holdout Groups*: Reserve traffic percentage for control measurement
- *Slots*: Individual traffic allocation units within mutex groups

## Ecosystem Integration

The platform integrates with Amplitude's broader analytics ecosystem through automatic event tracking. The `X-Amp-Exp-Track` header enables automatic generation of `[Experiment] Assignment` events, connecting experiment exposure to downstream analytics and conversion tracking.

Regional deployment support includes both US (`api.lab.amplitude.com`) and EU (`api.lab.eu.amplitude.com`) endpoints for data residency compliance. The management API similarly supports regional endpoints at `experiment.amplitude.com`.

## API Specifications

**Evaluation API Endpoints**:
- `GET /v1/vardata` - Retrieve variant assignments with user context
- `GET /v1/flags` - Download flag configurations for local evaluation

**Management API Endpoints**:
- `/api/1/flags/*` - Flag lifecycle management
- `/api/1/experiments/*` - Experiment configuration and control
- `/api/1/deployments/*` - Environment and deployment management  
- `/api/1/holdouts/*` - Holdout group configuration
- `/api/1/mutexs/*` - Mutex group and slot management
- `/api/1/versions` - Version history and configuration tracking

**Authentication**:
- Evaluation API: `Api-Key` header with deployment keys
- Management API: `Authorization: Bearer <management-api-key>`

**Rate Limiting**: Management API enforces 100 requests per second with 100,000 daily limits, while supporting cursor-based pagination through `nextCursor` parameters for large dataset traversal.

The platform supports multiple experiment types including A/B tests and multi-arm bandit configurations, with sticky bucketing ensuring consistent user experiences across sessions. Experiment states progress through planning, running, and decision-made phases with corresponding API controls for lifecycle management.