# Amplitude Experiment Platform

Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that provides both evaluation and management capabilities through distinct API layers. The platform enables organizations to run controlled experiments, manage feature rollouts, and implement sophisticated traffic allocation strategies across client and server-side applications.

## Product Architecture and Feature Relationships

The Amplitude Experiment platform consists of two primary API layers that work in tandem:

**Evaluation API Layer** - Handles real-time flag evaluation and variant assignment through two core endpoints:
- `/v1/vardata` for remote evaluation and variant assignment retrieval
- `/v1/flags` for downloading flag configurations to enable local evaluation

**Management API Layer** - Provides comprehensive CRUD operations for experiment lifecycle management across five main resource types:
- **Experiments**: Full A/B testing capabilities with multi-arm bandit support, variant management, and lifecycle states (planning/running/decision-made)
- **Feature Flags**: Boolean and multi-variant flags with rollout controls and targeting capabilities
- **Deployments**: Environment-specific configurations supporting both client and server deployment types
- **Mutex Groups**: Traffic allocation and mutual exclusion management to prevent experiment interference
- **Holdout Groups**: Control group management for measuring overall experiment program impact

The platform supports both **remote evaluation** (server-side decision making) and **local evaluation** (client-side decision making with downloaded configurations), enabling flexible deployment patterns based on performance and latency requirements.

## Key Nomenclature and Definitions

**Core Concepts:**
- **Deployment Key**: Authentication mechanism for evaluation API, tied to specific environments
- **Management API Key**: Bearer token for administrative operations via Management API
- **Bucketing**: User assignment mechanism using configurable keys (amplitude_id, device_id, custom) with salt values for deterministic randomization
- **Variants**: Different treatment versions within experiments or flags, each with rollout weights and payload configurations
- **Target Segments**: Audience definition rules that determine experiment/flag eligibility
- **Rollout Weights**: Percentage-based traffic distribution across variants
- **Sticky Bucketing**: Ensures consistent user experience by maintaining variant assignments across sessions

**Traffic Management:**
- **Mutex Groups**: Mutual exclusion containers that prevent users from being in multiple conflicting experiments
- **Holdout Groups**: Reserved user populations excluded from experiments to measure overall program impact
- **Individual Inclusions/Exclusions**: Granular user-level targeting overrides

**Evaluation Modes:**
- **Remote Evaluation**: Real-time server-side evaluation via `/v1/vardata`
- **Local Evaluation**: Client-side evaluation using pre-downloaded flag configurations via `/v1/flags`

## Ecosystem Integration and Workflow

The Amplitude Experiment platform integrates into the broader Amplitude analytics ecosystem through automatic event tracking and regional server support. The typical workflow involves:

1. **Configuration Phase**: Use Management API to create experiments/flags, define variants, set targeting rules, and configure deployments
2. **Deployment Phase**: Deploy experiments to specific environments using deployment keys
3. **Evaluation Phase**: Client/server applications call Evaluation API endpoints to retrieve variant assignments
4. **Analytics Integration**: Automatic `[Experiment] Assignment` event tracking (controlled via `X-Amp-Exp-Track` header) feeds into Amplitude's analytics platform
5. **Lifecycle Management**: Use Management API to monitor performance, adjust rollouts, and make experiment decisions (rollout/rollback/continue-running)

The platform supports **regional deployment** with dedicated servers (`api.lab.amplitude.com`, `api.lab.eu.amplitude.com`) for data residency compliance.

## API Specifications and Technical Implementation

**Evaluation API Endpoints:**
- `GET /v1/vardata` - Remote evaluation with user context parameters (user_id, device_id, user_properties)
- `GET /v1/flags` - Flag configuration download for local evaluation and client-side SDK bootstrapping

**Management API Base Endpoints:**
- `GET/POST/PATCH /api/1/experiments` - Experiment lifecycle management
- `GET/POST/PATCH /api/1/flags` - Feature flag operations  
- `GET/POST/PATCH /api/1/deployments` - Environment configuration
- `GET/POST/PATCH /api/1/holdouts` - Holdout group management
- `GET/POST/PATCH /api/1/mutexs` - Mutex group and slot management
- `GET /api/1/versions` - Version history with ISO 8601 time filtering

**Authentication and Rate Limiting:**
- Evaluation API: `Api-Key` header with deployment key
- Management API: `Authorization: Bearer <management-api-key>`
- Rate limiting: 100 requests/second
- Cursor-based pagination with `nextCursor` parameter

The platform's **version management system** maintains complete audit trails of configuration changes, enabling rollback capabilities and historical analysis of experiment performance across different configuration iterations.