# Amplitude Experiment Platform

Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that enables teams to manage experiments, feature flags, and traffic allocation through both evaluation and management APIs. The platform supports both remote and local evaluation modes, allowing for flexible deployment strategies across client and server environments.

## Product Architecture and Feature Relationships

The Amplitude Experiment platform consists of two primary API layers:

**Evaluation API** serves as the runtime engine for retrieving variant assignments and flag configurations. It provides the `/v1/vardata` endpoint for remote evaluation (returning specific variant assignments for users) and the `/v1/flags` endpoint for downloading complete flag configurations for local evaluation scenarios.

**Management API** provides comprehensive CRUD operations across six core entity types that work together to create a complete experimentation framework:

- **Flags and Experiments** form the core testing entities, with experiments specifically designed for A/B testing scenarios and flags for feature rollouts
- **Deployments** define the environments (client/server) where flags and experiments are assigned
- **Mutex Groups** manage traffic allocation and prevent experiment conflicts through slot-based assignment
- **Holdout Groups** enable control group management across multiple experiments
- **Versions** provide historical tracking and rollback capabilities
- **Variants** define the different experiences users can receive

## Key Nomenclature and Definitions

**Bucketing System**: The platform uses a sophisticated bucketing mechanism with configurable `bucketingKey` (user identifier), `bucketingSalt` (randomization seed), and `bucketingUnit` (typically `amplitude_id` or `device_id`) to ensure consistent user assignment across sessions.

**Evaluation Modes**: 
- **Remote evaluation** requires real-time API calls to retrieve assignments
- **Local evaluation** downloads flag configurations for client-side processing

**Traffic Management**:
- **Rollout percentage** controls what portion of traffic sees an experiment
- **Rollout weights** distribute traffic between variants
- **Target segments** define user conditions for experiment inclusion
- **Mutex groups** prevent traffic overlap between conflicting experiments

**User Targeting**:
- **Individual inclusions/exclusions** override standard bucketing for specific users
- **Context parameters** provide additional user attributes for targeting
- **Segments metadata** defines conditional logic for user targeting

## Ecosystem Integration

The platform integrates with Amplitude's broader analytics ecosystem through automatic event tracking. The `X-Amp-Exp-Track` header controls whether `[Experiment] Assignment` events are sent to Amplitude Analytics, enabling automatic experiment analysis and reporting.

Regional deployment is supported with dedicated endpoints (`api.lab.amplitude.com` for US, `api.lab.eu.amplitude.com` for EU residency requirements), and the management API is accessible via `experiment.amplitude.com`.

## API Specifications

**Authentication**:
- Evaluation API uses `Api-Key` header with deployment keys
- Management API uses `Bearer` token with management API keys

**Key Endpoints**:
- `GET /v1/vardata` - Remote variant assignment retrieval
- `GET /v1/flags` - Flag configuration download for local evaluation
- `GET /api/1/flags` - Flag management operations
- `GET /api/1/experiments` - Experiment management operations
- `GET /api/1/deployments` - Deployment environment management
- `GET /api/1/holdouts` - Holdout group management
- `GET /api/1/mutexs` - Mutex group traffic allocation
- `GET /api/1/versions` - Version history and rollback

**Rate Limiting**: Management API enforces 100 requests per second with 100,000 daily request limits, while supporting cursor-based pagination for large dataset retrieval.

The platform supports experiment lifecycle management from planning through decision-making phases, with capabilities for sticky bucketing, exposure event tracking, and automated rollout/rollback decisions based on experiment outcomes.