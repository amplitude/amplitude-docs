Amplitude Experiment is a comprehensive feature flagging and A/B testing platform that provides both evaluation and management capabilities through distinct API layers. The platform enables organizations to conduct controlled experiments, manage feature rollouts, and optimize user experiences through data-driven decision making.

## Core Product Architecture

The Amplitude Experiment platform consists of two primary API systems:

**Evaluation API** - Handles real-time flag evaluation and variant assignment for end users through remote evaluation endpoints and local evaluation flag downloads. This API serves production traffic and determines which users receive which variants.

**Management API** - Provides programmatic control over experiment and flag configuration, enabling CRUD operations across all experiment entities including flags, experiments, mutex groups, holdouts, deployments, and versions.

## Key Features and Capabilities

### Experiment and Flag Management
The platform supports both A/B tests and multi-arm bandit experiments with sophisticated targeting capabilities. Feature flags can operate in multiple evaluation modes (remote/local) and support complex rollout strategies with percentage-based traffic allocation, user segmentation, and individual user inclusions/exclusions.

### Traffic Control and Allocation
Mutex groups enable sophisticated traffic allocation by preventing experiment conflicts through slot-based assignment systems. Holdout groups provide control populations for measuring overall experiment impact. Both systems support bucketing configuration with customizable keys (amplitude_id, device_id) and salts for consistent user assignment.

### Deployment and Versioning
Deployments represent distinct environments (client/server) where experiments and flags can be assigned. The versioning system tracks configuration changes over time with ISO 8601 timestamps and provides historical access to flag configurations.

## Key Nomenclature and Definitions

**Variants** - Different versions of features or experiences that users can receive, containing payload data and configuration
**Bucketing** - The process of consistently assigning users to variants using bucketing keys (amplitude_id, device_id) and salts
**Target Segments** - User groups defined by conditions and properties for experiment targeting
**Rollout Weights** - Percentage-based traffic distribution across variants
**Evaluation Modes** - Remote (server-side evaluation) vs Local (client-side evaluation with downloaded configurations)
**Sticky Bucketing** - Ensures users remain in the same variant across sessions
**Mutex Groups** - Traffic allocation systems that prevent experiment conflicts through slot management
**Holdout Groups** - Control populations excluded from experiments to measure overall impact
**Deployments** - Environment assignments (client/server) for experiments and flags

## API Endpoints and Integration

### Evaluation API Endpoints
- `/v1/vardata` - Retrieves variant assignments for users with context parameters
- `/v1/flags` - Downloads flag configurations for local evaluation and bootstrapping

### Management API Endpoints
- `/api/1/flags` - Flag CRUD operations and configuration management
- `/api/1/experiments` - Experiment lifecycle management and variant configuration
- `/api/1/deployments` - Environment assignment and deployment management
- `/api/1/holdouts` - Holdout group configuration and user exclusions
- `/api/1/mutexs` - Mutex group traffic allocation and slot management
- `/api/1/versions` - Historical configuration access and version tracking

## Authentication and Infrastructure

The platform uses deployment keys for evaluation API access via Api-Key headers, and management API keys with Bearer token authentication for configuration operations. Rate limiting enforces 100 requests per second with 100,000 daily limits. Regional server support includes US (api.lab.amplitude.com) and EU (api.lab.eu.amplitude.com) endpoints for data residency compliance.

## Broader Ecosystem Integration

Amplitude Experiment integrates with the broader Amplitude analytics platform through automatic [Experiment] Assignment event tracking via the X-Amp-Exp-Track header. This enables measurement of experiment impact on key metrics and supports the complete experimentation lifecycle from hypothesis to statistical analysis. The platform supports both real-time decision making through remote evaluation and high-performance scenarios through local evaluation with downloaded flag configurations.