Amplitude Data is a comprehensive data governance and management platform that serves as the centralized control layer for analytics instrumentation across the entire data lifecycle. The platform enables teams to create structured tracking plans, generate type-safe code, and ensure data quality through real-time monitoring and validation, functioning as both a data catalog and governance framework for analytics implementations.

## Core Product Architecture

Amplitude Data operates through three integrated layers that collectively manage the analytics data lifecycle:

**Planning and Instrumentation Layer** provides the foundation for structured data collection through Tracking Plans (schema definitions specifying events, properties, and data types), Ampli Developer Tools (code generation toolkit creating type-safe wrappers for Amplitude SDKs), and a Branch Workflow system offering Git-like versioning for collaborative tracking plan development.

**Data Management Layer** handles data transformation and enrichment through Transformations (retroactive data modification including event/property merging, renaming, and hiding), Custom Events (logical combinations of existing events using OR clauses), Derived Properties (formula-based properties computed using functions like REGEXEXTRACT, SPLIT, and CURRENCY_CONVERT), and Lookup Tables (CSV-based data enrichment for mapping external data to event properties).

**Governance and Quality Layer** ensures data integrity through Schema Validation (real-time monitoring against tracking plans with violation handling), Data Access Control (DAC) with property classification and group-based permissions for sensitive data, and an AI Data Assistant providing automated suggestions for tracking plan optimization.

## Product Relationships and Feature Integration

The platform's components work synergistically to create a comprehensive data governance ecosystem. Tracking Plans serve as the single source of truth that feeds into both the Ampli code generation process and the Schema Validation system. The Branch Workflow enables teams to collaborate on tracking plan modifications while maintaining production stability through environment separation.

Data transformations operate at the ingestion layer, modifying data before it reaches analysis tools, while Custom Events and Derived Properties extend analytical capabilities without requiring new instrumentation. Lookup Tables bridge external data sources with event properties, enabling enriched analysis across business contexts.

The governance layer operates continuously, with Schema Validation monitoring incoming data against established plans, DAC controlling access to sensitive information, and the AI Data Assistant providing optimization recommendations based on usage patterns and data quality metrics.

## Key Nomenclature and Definitions

**Event Taxonomy:**
- **Events**: User actions or system occurrences tracked for analysis
- **Event Properties**: Instance-specific attributes attached to individual events
- **User Properties**: Persistent user attributes that update over time
- **Group Properties**: Account-level attributes for B2B analytics (requires Accounts add-on)

**Data Classification System:**
- **Planned Events**: Events pre-defined in tracking plans before implementation
- **Unexpected Events**: Events received but not defined in tracking plans
- **Live Events**: Events actively receiving data
- **Blocked Events**: Events prevented from ingestion through block filters
- **Transformed Events**: Events modified through transformation rules

**Schema Management Components:**
- **Sources**: Data ingestion points including SDKs, third-party integrations, and APIs
- **Environments**: Project mappings separating development and production contexts
- **Branches**: Isolated workspaces for tracking plan modifications
- **Property Groups**: Collections of properties enabling bulk management across events

**Data Operations:**
- **Drop Filters**: Query-time filtering excluding data from analysis while preserving storage
- **Block Filters**: Ingestion-time filtering preventing data storage entirely
- **Overrides**: Event-specific or property group-specific customizations
- **Autocapture**: Automatic tracking of user interactions without manual instrumentation

## Ecosystem Integration

Amplitude Data integrates across multiple layers of the analytics infrastructure, connecting planning tools with implementation SDKs and downstream analysis platforms.

**SDK Integration** spans both client-side implementations (Browser, Android, iOS, React Native, Unity, Flutter) and server-side SDKs (Node.js, Go, Python, Java) through the HTTP V2 API for real-time event ingestion, Batch Event Upload API for historical data backfill (500K events per device ID daily limits), and Evaluation API for feature flag and experiment evaluation.

**Data Infrastructure** connections include Warehouse Integrations with Snowflake, Databricks, and Amazon S3 supporting Mirror Sync strategies with INSERT/UPDATE/DELETE operations, Change Data Capture (CDC) for real-time synchronization enabling data mutability, and Profiles for customer profile data joining using CDC and incremental modeling.

**Third-Party Ecosystem** integration occurs through the Source Catalog for data ingestion from external platforms, Destination Catalog for event streaming to downstream tools (60-second p95 latency), and Cohort Synchronization supporting real-time, scheduled, or one-time user segment syncing.

## API Endpoints and Command Line Interface

**Ampli CLI Commands:**
- `ampli pull`: Downloads tracking plan and generates type-safe code for local development
- `ampli status --is-merged`: Checks branch merge status for CI/CD pipeline integration

**Core APIs:**
- **Taxonomy API**: Provides CRUD operations for categories, event types, and properties
- **User Privacy API**: Enables complete user data deletion for GDPR/CCPA compliance
- **HTTP V2 API**: Handles event ingestion with user_agent field for bot filtering
- **Batch Event Upload API**: Supports historical data import with `$skip_user_properties_sync` flag

**Configuration Parameters:**
- **Schema Settings**: Handle violations through "Mark As Unexpected" or "Reject" options
- **Currency Conversion**: CURRENCY_CONVERT() function supporting ISO 4217 currency codes
- **Time-to-Live (TTL)**: Configurable data retention at organization and project levels

The platform supports enterprise-scale operations through cross-project portfolio analysis (up to 5 projects), object management for reusable analysis components, comprehensive role-based access controls (Admin/Manager/Member/Viewer), Jira integration for automatic issue creation, and Git-like branching workflows that enable collaborative tracking plan development while maintaining data quality and governance standards across the entire analytics ecosystem.