Amplitude Data is a comprehensive data governance and management platform that serves as the central orchestration layer for analytics instrumentation across the entire data lifecycle. The platform functions as a centralized data catalog enabling organizations to create structured tracking plans, generate type-safe code, monitor data quality, and perform retroactive data transformations.

## Core Product Architecture

Amplitude Data operates through three integrated layers within Amplitude's analytics ecosystem:

**Planning and Instrumentation Layer:**
- **Tracking Plans**: Schema definitions specifying events, properties, and data types with validation rules
- **Ampli Developer Tools**: Code generation toolkit creating type-safe wrappers for Amplitude SDKs
- **Branch-based Workflow**: Git-like versioning system enabling collaborative tracking plan development
- **Visual Labeling**: No-code interface for event creation through direct website element interaction

**Data Ingestion and Processing:**
- **Client-side SDKs**: Browser, Android, iOS, React Native, Unity, Flutter implementations
- **Server-side SDKs**: Node.js, Go, Python, Java for backend event collection
- **Third-party Integrations**: Segment, mParticle, Tealium, Shopify, WordPress connectors
- **Cloud Storage Imports**: Amazon S3, Google Cloud Storage with converter configurations
- **Warehouse Integrations**: Snowflake, Databricks with Change Data Capture (CDC) and Change Data Feed (CDF) capabilities

**Data Management and Transformation:**
- **Schema Validation**: Real-time monitoring against tracking plans with configurable violation handling
- **Transformations**: Retroactive data modification including event/property merging and renaming
- **Derived Properties**: Formula-based property creation using mathematical and logical operators
- **Custom Events**: OR-clause combinations of existing events for complex behavioral definitions
- **Lookup Tables**: CSV-based property enrichment through external data mapping

## Product Relationships and Data Model

The platform implements a hierarchical data structure where **Organizations** contain **Projects**, which house **Tracking Plans** that define **Events** with **Event Properties** and **User Properties**. Events can be organized into **Property Groups** for bulk management operations, while **Sources** represent different SDK implementations or data ingestion endpoints.

**Autocapture** functionality automatically captures user interactions across Browser SDK, iOS Swift SDK, and Android-Kotlin SDK, generating standardized events including Element Clicked, Element Changed, Page Viewed, and Session tracking with configurable allowlists and privacy controls.

**Profiles** enable joining customer profile data from data warehouses with behavioral analytics data using Change Data Capture patterns, supporting INSERT/UPDATE/DELETE operations through mutation_type specifications for comprehensive customer 360 views.

## Key Nomenclature and Definitions

**Schema Violations**: Three categories of unplanned data - unexpected event types, properties, and property values - managed through "mark as unexpected" or "reject" policies with automatic remediation workflows

**Data Mutability**: INSERT, UPDATE, DELETE operations on historical event data through Mirror Sync strategies with warehouse integrations, enabling retroactive data correction

**Channels**: Retroactive property classification system based on UTM parameters and referrer data for marketing attribution tracking

**Time-to-Live (TTL)**: Configurable data retention periods at organization and project levels with irreversible deletion policies for compliance management

**Data Access Control (DAC)**: Enterprise feature for classifying properties as PII, revenue, or sensitive data with group-based permissions and audit trails

**Streaming Transformations**: Real-time forwarding of transformed events and properties to downstream destinations with sub-minute latency requirements

**Environment Settings**: Legacy feature mapping production/development environments to specific Analytics projects for deployment workflow management

## Ecosystem Integration

Amplitude Data functions as the central nervous system connecting multiple platform components:

**Analytics Platform Integration**: Provides schema validation and metadata enrichment for Event Segmentation, Funnel Analysis, Retention Analysis, and advanced chart types with consistent data definitions

**Experiment Platform**: Supports A/B testing through Web Experiment capabilities and Evaluation API integration for feature flag management and statistical analysis

**Destination Ecosystem**: Event streaming to marketing automation tools (Braze, Iterable), advertising networks (Facebook, Google, Twitter), and attribution providers with p95 latency targets of 60 seconds

**Developer Ecosystem**: Integration with Jira for automatic issue creation, Chrome Extension for real-time debugging, and comprehensive programmatic access through REST APIs

## API Endpoints and Command Line Interface

**Ampli CLI Commands:**
- `ampli pull` - Generate type-safe tracking libraries from tracking plans
- `ampli status --is-merged` - Verify branch merge status in collaborative workflows

**Core API Endpoints:**
- **HTTP V2 API**: Client-side event ingestion with real-time processing
- **Batch Event Upload API**: Server-side and backfill data ingestion with 500K events per device ID daily limits
- **Evaluation API**: Experiment feature flag evaluation with millisecond response times
- **Taxonomy API**: CRUD operations for categories, events, and properties with versioning support
- **User Privacy API**: GDPR/CCPA compliance for user data deletion and privacy rights management

**Configuration Parameters:**
- `config.autocapture.elementInteractions`: Enable Visual Labeling functionality
- `fetchRemoteConfig`: Browser SDK remote configuration management
- `$skip_user_properties_sync`: Control user property synchronization during backfill operations
- `convertToAmplitudeFunc`: Data transformation configuration for cloud storage imports

The platform enforces operational limits including 100MB/1M rows for Lookup Tables, 5-project limits for portfolio management, and specific event volume quotas with 429 HTTP response codes for rate limiting. Data processing maintains chronological order with idempotency design patterns and supports both real-time (minute-by-minute) and scheduled (hourly/daily) synchronization workflows for enterprise-scale data operations.