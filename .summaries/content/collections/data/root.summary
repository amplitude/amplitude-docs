Amplitude Data is a comprehensive data governance and management platform that provides end-to-end tools for planning, implementing, and managing analytics instrumentation across web and mobile applications. The platform serves as a centralized data catalog that enables organizations to maintain data quality, enforce taxonomy standards, and streamline the analytics implementation process.

## Core Platform Components

**Tracking Plans and Schema Management**
Amplitude Data centers around tracking plans - structured schemas that define events, event properties, user properties, and group properties for analytics implementation. These tracking plans serve as the single source of truth for data taxonomy, supporting branching workflows for collaborative development and version control. The platform enforces schema validation through configurable violation handling, allowing organizations to either mark unexpected data as such or reject it entirely at ingestion time.

**Ampli Developer Toolkit**
The Ampli CLI generates type-safe, auto-generated tracking libraries from tracking plans, providing developers with compile-time validation, autocompletion, and runtime checks. This code generation approach ensures instrumentation accuracy and reduces implementation errors by enforcing event names and property values defined in the tracking plan.

**Data Ingestion and Sources**
The platform supports multiple data ingestion methods including client-side SDKs (Browser, Android, iOS, Unity, Flutter, React Native), server-side SDKs (Node.js, Go, Python, Java), and third-party integrations. Data flows through the HTTP V2 API, Batch Event Upload API, and Evaluation API endpoints, with specialized plugins for platforms like Shopify and WordPress that provide pre-configured tracking implementations.

## Advanced Data Management Features

**Transformations and Data Enhancement**
Amplitude Data provides retroactive data modification capabilities through transformations that operate at query time without affecting raw data. Organizations can merge events and properties, rename property values, hide sensitive data, and create derived properties using formula-based calculations with functions like REGEXEXTRACT, CONCAT, and DATE_TIME_FORMATTER.

**Custom Events and Property Management**
The platform enables creation of custom events that combine multiple existing events with OR clauses, lookup tables for enriching properties via CSV imports, and property groups for bulk management across multiple events. Currency conversion capabilities support multi-currency revenue analysis with automatic exchange rate application.

**Data Quality and Observability**
Real-time data observability through the Observe feature automatically monitors event streams, validates against tracking plans, and categorizes events as Valid, Invalid, Unexpected, or Out of Date. The AI Data Assistant provides automated suggestions for tracking plan improvements based on event volume and query patterns.

## Key Nomenclature and Definitions

**Events and Properties**
- **Events**: User actions or occurrences tracked in the system (e.g., "Button Clicked", "Page Viewed")
- **Event Properties**: Attributes specific to individual event occurrences (e.g., button_name, page_url)
- **User Properties**: Persistent attributes of users that update over time (e.g., subscription_status, device_type)
- **Group Properties**: Attributes of account-level entities when using the Accounts add-on

**Data Classification and Access Control**
- **Active vs Inactive Events**: Status affecting user metrics calculations and dashboard visibility
- **Planned vs Unexpected Events**: Schema compliance categorization for data governance
- **PII/Sensitive Data Classification**: Enterprise Data Access Control for managing sensitive information access
- **Derived Properties**: Computed properties created retroactively using formulas applied to existing data

**Workflow Management**
- **Branches**: Isolated environments for tracking plan development with merge capabilities
- **Sources**: SDK or integration configurations within tracking plans
- **Environments**: Legacy feature mapping development/production to specific Analytics projects

## Product Ecosystem Integration

Amplitude Data integrates deeply with the broader Amplitude ecosystem, serving as the foundational layer for analytics implementation across Amplitude Analytics, Amplitude Experiment, and third-party platforms. The platform connects to data warehouses (Snowflake, Databricks, Amazon S3) through Mirror Sync strategies supporting Change Data Capture (CDC) and Change Data Feed (CDF) for bidirectional data synchronization.

**Destination Event Streaming** enables real-time behavioral data forwarding to downstream marketing, sales, and infrastructure tools with sub-60-second p95 latency targets. **Cohort Synchronization** supports one-time, scheduled, and real-time syncing of user segments to advertising platforms and marketing automation tools.

**Data Warehouse Integration** includes Amplitude Profiles for joining customer profile data with behavioral data using incremental modeling, and comprehensive export capabilities maintaining data lineage and transformation history.

## API Endpoints and Technical Implementation

**Core APIs**
- `HTTP V2 API`: Primary endpoint for client-side event ingestion
- `Batch Event Upload API`: Server-side bulk data import with 500K events per device ID daily limits
- `Taxonomy API`: CRUD operations for categories, event types, and properties
- `User Privacy API`: GDPR/CCPA compliance for user data deletion

**CLI Commands**
- `ampli pull`: Downloads latest tracking plan and generates code
- `ampli status --is-merged`: Validates branch merge status for CI/CD integration

**Configuration Options**
Key SDK configuration parameters include `batchEvents`, `sessionTimeout`, `eventUploadThreshold`, `cssSelectorAllowlist` for Autocapture, and `trackingOptions` for controlling automatic property collection.

The platform supports advanced features like Visual Labeling for no-code event creation, Time-to-Live (TTL) data retention policies, and comprehensive filtering mechanisms including drop filters (query-time) and block filters (ingestion-time) for data quality management.