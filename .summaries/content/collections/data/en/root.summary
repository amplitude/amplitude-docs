Amplitude Data is a comprehensive data governance and catalog management platform that provides end-to-end tools for planning, implementing, and managing analytics instrumentation across organizations. The platform serves as the central hub for defining data taxonomy, creating tracking plans, and ensuring data quality throughout the analytics lifecycle.

## Core Product Architecture

Amplitude Data operates as a governance layer that sits between data collection and analytics, providing both planning and management capabilities. The platform integrates deeply with Amplitude Analytics projects and supports cross-project analysis through portfolios that can combine up to 5 source projects with schema ranking for conflict resolution.

The system follows a Git-like branching model where users create feature branches from main, make tracking plan modifications, publish changes to create versioned tracking plans, and merge back to main after team review approvals. This workflow enables collaborative development with stakeholder reviews and environment-specific testing.

## Key Features and Capabilities

**Data Planning and Instrumentation:**
- Tracking plan creation with events, event properties, user properties, and group properties
- Ampli Developer Tools integration for generating type-safe, autogenerated tracking libraries
- Visual Labeling for no-code event creation through DOM element selection
- Autocapture functionality for automatic user interaction tracking (clicks, page views, sessions, form interactions)
- Schema validation with configurable violation handling (mark as unexpected or reject data)

**Data Management and Transformation:**
- Derived Properties for creating retroactive event and user properties using formulas and functions
- Lookup Tables for enriching properties by mapping CSV data sources to ingested properties
- Transformations for retroactive data structure modifications at query time
- Custom Events creation combining multiple existing events with OR clauses
- Currency conversion with OOTB derived properties and CURRENCY_CONVERT function

**Data Quality and Observability:**
- Observe feature for automatic event stream validation against tracking plans
- AI Data Assistant for generating prioritized tracking plan improvement suggestions
- Drop filters for query-time data filtering and block filters for ingestion-time blocking
- Data Access Control (DAC) for managing sensitive data access through property classification
- Chrome Extension (Amplitude Instrumentation Explorer) for real-time SDK debugging

**Integration and Streaming:**
- Destination event streaming for real-time behavioral data forwarding to downstream tools
- Cohort synchronization with third-party destinations (one-time, scheduled, real-time)
- Source and destination catalogs for third-party integrations
- Profiles feature for joining warehouse customer data with behavioral data using Change Data Capture
- WordPress and Shopify plugins for simplified SDK integration

## Key Nomenclature and Definitions

**Events and Properties:**
- **Events**: User actions or occurrences tracked in the system (e.g., "Button Clicked", "Page Viewed")
- **Event Properties**: Attributes specific to individual events at the time they occurred
- **User Properties**: Attributes of individual users that persist across sessions (device type, location, User ID)
- **Group Properties**: Properties associated with account-level entities (requires Accounts add-on)

**Data Structure:**
- **Tracking Plan**: The schema definition containing all events, properties, and their specifications
- **Sources**: SDK or integration configurations within tracking plans
- **Property Groups**: Collections of event properties that can be bulk-applied to multiple events
- **Branches**: Feature development environments for tracking plan modifications

**Data Processing:**
- **Autocapture**: Automatic capture of user interactions without manual instrumentation
- **Ampli**: Code generation tool creating type-safe SDK wrappers from tracking plans
- **Schema Violations**: Data that doesn't match the defined tracking plan structure
- **Transformations**: Retroactive modifications to event data structure at query time

**Access and Permissions:**
- **Roles**: Admin, Manager, Member, Viewer with different permission levels
- **Data Access Control (DAC)**: Enterprise feature for managing access to sensitive data
- **Property Classification**: Categorizing properties as PII, revenue, or sensitive data

## Product Ecosystem Integration

Amplitude Data serves as the foundational layer for the broader Amplitude ecosystem, integrating with:

**Amplitude Analytics**: Direct project mapping and cross-project portfolio analysis
**Amplitude Experiment**: SDK integration for A/B testing with tracking plan validation
**Third-party Platforms**: Extensive integration catalog including Segment, mParticle, Tealium, Shopify, WordPress
**Data Warehouses**: Snowflake, Databricks, Amazon S3 for data import/export and Change Data Capture
**Development Tools**: Git-like workflow integration, Jira integration for issue tracking

## API Endpoints and Technical Implementation

**Core APIs:**
- **Taxonomy API**: CRUD operations for categories, event types, event properties, and user properties
- **HTTP V2 API**: Client-side event ingestion
- **Batch Event Upload API**: Server-side and historical data import with 500K events per device ID daily limit
- **User Privacy API**: Data deletion for GDPR/CCPA compliance

**SDK Integration:**
- **Client-side SDKs**: Browser, Android, iOS, React Native, Unity, Flutter
- **Server-side SDKs**: Node.js, Go, Python, Java
- **Ampli CLI Commands**: `ampli pull`, `ampli status --is-merged` for tracking plan synchronization

**Configuration Options:**
- **batchEvents**, **sessionTimeout**, **minTimeBetweenSessions** for SDK behavior
- **trackingOptions** for controlling automatic data collection
- **cssSelectorAllowlist**, **actionClickAllowlist** for Autocapture filtering

**Data Ingestion Limits:**
- 100 batches/second, 1000 events/second for Batch API
- 100MB/1M rows limit for Lookup Tables
- P95 latency target of 60 seconds for event streaming

The platform supports both real-time and batch processing workflows, with comprehensive monitoring, retry mechanisms, and data quality controls throughout the ingestion and processing pipeline.