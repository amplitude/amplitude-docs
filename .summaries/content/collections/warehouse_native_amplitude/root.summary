# Warehouse-native Amplitude (WNA)

## Product Overview

Warehouse-native Amplitude (WNA) is an analytics solution that enables users to analyze data directly from their Snowflake data warehouse without requiring data ingestion into Amplitude's systems. This approach eliminates data transfer delays for time-sensitive analytics while leveraging Amplitude's powerful analytical capabilities. WNA establishes a read-only connection to the customer's Snowflake instance, allowing users to perform custom analyses through Amplitude's familiar interface.

Key features of WNA include:
- Direct connection to Snowflake data warehouse
- Custom data modeling capabilities
- Support for various data types (events, user properties, group properties)
- Flexible modeling approaches (single event per table or multiple events in one table)
- Bulk model management through YAML configuration
- DBT integration for streamlined data modeling
- Performance optimization through clustering keys and partitioning

WNA is particularly valuable for organizations that:
- Need to analyze time-sensitive data without transfer delays
- Want to leverage existing data warehouse investments
- Require custom analytics models on warehouse data
- Need to maintain data governance within their existing warehouse infrastructure

## Product Relationships and Features

WNA supports several data model components that work together to provide comprehensive analytics:

1. **Event Data**: Core behavioral data representing user actions within applications or systems
2. **User Properties**: Attributes that describe users and their characteristics
3. **Group Properties**: Attributes describing organizational groups (companies, teams, etc.)
4. **Fact Tables**: Primary tables containing event data with metrics and measurements
5. **Dimension Tables**: Supporting tables with descriptive attributes for contextual analysis

The product offers multiple modeling approaches to accommodate different data structures:

1. **Table Selection**: Direct mapping to existing Snowflake tables
2. **SQL Query**: Custom SQL queries for data transformation and preparation
3. **Star Schema**: A fact table connected directly to dimension tables for efficient querying
4. **Snowflake Schema**: A fact table connected to normalized dimension tables for complex relationships

Integration methods include:
- **Direct Snowflake Connection**: Using read-only credentials for secure access
- **YAML Configuration**: Bulk model management through configuration files
- **DBT Integration**: Leveraging DBT manifest files with Amplitude-specific annotations

## Key Nomenclature and Definitions

WNA uses specific terminology to describe different data model types:

- **EVENT**: Represents user actions or behaviors (clicks, page views, purchases)
- **EVENT_PROPERTIES**: Additional attributes describing events (purchase amount, item category)
- **CURRENT_USER_PROPERTIES**: Latest state of user attributes (current subscription plan, current location)
- **HISTORICAL_USER_PROPERTIES**: Time-series record of user attribute changes (subscription plan changes over time)
- **CURRENT_GROUP_PROPERTIES**: Latest state of group attributes (company size, industry)
- **HISTORICAL_GROUP_PROPERTIES**: Time-series record of group attribute changes (company growth stages)

Technical concepts important for WNA implementation include:

- **Clustering Keys**: Columns used to physically organize data for query optimization
- **Composite Clustering Keys**: Multiple columns used together for clustering
- **LINEAR() Function**: Snowflake function used with clustering keys for performance
- **Identity Spaces**: Namespaces for user identification across different data sources
- **Partitioning**: Division of data into segments for improved query performance
- **Cardinality**: Measure of uniqueness in a data column (affects clustering efficiency)

Configuration elements include:
- **TableConfig**: Configuration for direct table mapping
- **SQLConfig**: Configuration for SQL-based model creation
- **FieldMapping**: Mapping between source and destination fields
- **RequiredFields**: Mandatory fields for specific model types
- **amplitude_meta**: DBT annotation for Amplitude-specific metadata

## Product Ecosystem Integration

Warehouse-native Amplitude represents an alternative approach to data sourcing within the broader Amplitude ecosystem. While traditional Amplitude implementations require event data ingestion into Amplitude's systems, WNA maintains data within the customer's Snowflake environment.

WNA uses the same analytical interface and capabilities as standard Amplitude, providing the same types of insights and visualizations. The primary difference is in data storage location and access method.

Some limitations of WNA compared to standard Amplitude include:
- Certain advanced features may not be supported
- Performance depends on Snowflake optimization
- Formula constraints may apply to certain analyses

## Technical Implementation

### Snowflake Connection Requirements
WNA requires:
- Read-only credentials for Amplitude access to Snowflake
- Properly configured clustering keys for performance optimization
- Appropriate schema design (star or snowflake)

### Model Creation Methods

#### Direct Table Selection
```
SELECT * FROM your_snowflake_table
```

#### SQL Query Approach
Custom SQL queries can transform data before analysis.

#### Bulk Model Management (YAML)
```yaml
models:
  - name: "event_model_name"
    type: "EVENT"
    config:
      type: "TABLE"
      table: "your_event_table"
    fieldMapping:
      event_time: "timestamp_column"
      user_id: "user_identifier_column"
      event_type: "event_name_column"
```

#### DBT Integration
```sql
-- In your DBT model
{{ config(
    materialized='table',
    amplitude_meta={
        "data_models": [{
            "type": "EVENT",
            "special_columns": {
                "event_time": "timestamp_column",
                "user_id": "user_identifier_column"
            }
        }]
    }
) }}
```

### Performance Optimization
For optimal WNA performance:
- Use appropriate clustering keys based on query patterns
- Implement partitioning strategies for large datasets
- Consider cardinality when selecting clustering keys
- Choose between star and snowflake schema based on data complexity