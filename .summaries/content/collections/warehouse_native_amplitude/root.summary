# Warehouse-native Amplitude (WNA)

## Product Overview

Warehouse-native Amplitude (WNA) is a solution that enables organizations to perform analytics on data residing in their Snowflake data warehouse without requiring data ingestion into Amplitude's systems. This approach allows users to leverage Amplitude's analytical capabilities while maintaining their data within their own Snowflake environment. WNA executes queries directly against the customer's data warehouse using read-only credentials, providing faster access to time-sensitive datasets and eliminating the need for data duplication.

Key features of Warehouse-native Amplitude include:
- Direct connection to Snowflake data warehouses
- Custom data models that map to existing warehouse data structures
- Amplitude's analytics tools applied to warehouse data
- Bulk model management for creating and updating multiple data models simultaneously
- DBT integration to leverage existing DBT workflows for defining Amplitude data models
- Performance optimization through clustering keys and partitioning strategies

WNA is particularly valuable for organizations that need to analyze sensitive data that cannot leave their environment, access time-sensitive data without waiting for ingestion processes, leverage existing data warehouse investments while using Amplitude's analytics, or maintain a single source of truth in their data warehouse.

## Product Relationships and Architecture

Warehouse-native Amplitude operates as a specialized deployment model within the Amplitude ecosystem with the following components:

1. **Data Source**: The customer's Snowflake data warehouse serves as the primary data repository.
2. **Connection Layer**: Read-only credentials provide secure access to the warehouse.
3. **Data Models**: Mappings define how warehouse data should be interpreted by Amplitude.
4. **Query Execution**: Queries are executed directly against the warehouse.
5. **Analytics Interface**: The standard Amplitude UI is used for analysis.

WNA supports three primary data model types:
- **Event data**: Fact tables containing user actions and behaviors
- **User properties**: Dimensional tables with user attributes
- **Group properties**: Dimensional tables with group/account attributes

Data models can be created through multiple methods:
- Direct table selection from Snowflake
- Custom SQL queries
- Bulk model management via YAML configuration
- DBT integration using manifest files and annotations

## Key Nomenclature and Definitions

1. **Warehouse-native Data Model**: A configuration that maps warehouse data structures to Amplitude's expected format.
2. **Fact Table**: Primary event data table containing user actions and timestamps.
3. **Dimensional Tables**: Supporting tables containing attributes about users or groups.
4. **Clustering Keys**: Columns used to physically organize data in Snowflake for query optimization.
5. **Identity Spaces**: Mechanisms for identifying and connecting users across different data sources.
6. **Star Schema**: Data modeling approach with a central fact table connected to dimension tables.
7. **Snowflake Schema**: Extended star schema where dimension tables are normalized into multiple related tables.
8. **LINEAR() Function**: Snowflake function used for optimizing clustering keys.
9. **Model Types**:
   - EVENT: Captures user actions
   - EVENT_PROPERTIES: Properties associated with events
   - USER_PROPERTIES: Attributes of users (current or historical)
   - GROUP_PROPERTIES: Attributes of groups/accounts (current or historical)

## Integration with Broader Ecosystem

Warehouse-native Amplitude primarily integrates with:

1. **Snowflake**: The primary supported data warehouse platform.
2. **DBT (Data Build Tool)**: Integration allows defining Amplitude models within DBT workflows.
3. **Amplitude Analytics**: WNA models appear in standard Amplitude interfaces, though with some feature limitations.

WNA has some feature constraints compared to standard Amplitude:
- Certain formulas and calculations are unsupported
- Performance depends on warehouse configuration and optimization
- Some advanced Amplitude features may be unavailable

## Technical Implementation Details

### Snowflake Connection Requirements
- Read-only credentials for Amplitude to access the warehouse
- Properly configured clustering keys for performance

### Data Model Creation Methods

1. **Direct Table Selection**:
   - Select Snowflake tables and map fields to Amplitude's expected structure

2. **SQL Query**:
   - Write custom SQL to define the data model
   - Must include required fields like user_id and event_time

3. **Bulk Model Management via YAML**:
   ```yaml
   models:
     - name: "model_name"
       type: "EVENT"
       source:
         type: "SQL" or "TABLE"
         value: "SQL query" or "table_name"
       requiredFields:
         user_id: "column_name"
         event_time: "column_name"
       fieldMapping:
         # field mappings
   ```

4. **DBT Integration**:
   - Annotate DBT models with `amplitude_meta` configurations
   - Example:
   ```yaml
   amplitude_meta:
     data_models:
       - type: "EVENT"
         special_columns:
           user_id: "user_id_column"
           event_time: "timestamp_column"
   ```

### Performance Optimization Recommendations

1. **Clustering Keys**:
   - Use LINEAR() function for optimal key selection
   - Consider cardinality when selecting keys
   - Composite clustering keys should be ordered from highest to lowest cardinality

2. **Schema Design**:
   - Star schema for simpler queries and better performance
   - Snowflake schema for more complex data relationships

3. **Partitioning Strategies**:
   - Partition by date ranges for time-series data
   - Balance partition size for optimal query performance