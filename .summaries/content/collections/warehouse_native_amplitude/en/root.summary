# Warehouse-native Amplitude (WNA)

## Product Overview

Warehouse-native Amplitude (WNA) is a solution that allows users to analyze data directly from their Snowflake data warehouse without requiring data ingestion into Amplitude's systems. This approach enables organizations to leverage Amplitude's analytical capabilities while keeping their data in their own Snowflake environment. WNA provides faster access to time-sensitive datasets and allows for custom analyses using data models built directly from warehouse data.

### Key Features
- Direct connection to Snowflake data warehouse
- Custom data model creation from warehouse data
- Support for various data types (events, user properties, group properties)
- Bulk model management capabilities
- DBT integration for streamlined data modeling
- Performance optimization through clustering keys and partitioning

### Primary Use Cases
- Analyzing sensitive data that cannot leave the data warehouse
- Accessing time-sensitive datasets without data transfer delays
- Creating custom analytics models from existing warehouse data
- Maintaining a single source of truth in the data warehouse

## Product Relationships and Architecture

Warehouse-native Amplitude consists of several interconnected components:

1. **Data Connection Layer**: Establishes a read-only connection to the Snowflake data warehouse
2. **Data Modeling Layer**: Allows creation of fact and dimensional tables through direct table selection or SQL queries
3. **Analysis Layer**: Provides Amplitude's analytical capabilities on top of the warehouse data
4. **Management Tools**: Includes bulk model management and DBT integration for efficient model creation and updates

The system operates by mapping warehouse data structures to Amplitude's required formats through data models. These models define how raw warehouse data should be interpreted as events, user properties, or group properties within Amplitude's analytical framework.

## Key Nomenclature and Definitions

### Data Model Types
- **Event Model**: Represents user actions or system events with associated properties
- **User Properties Model**: Contains attributes about users (can be current or historical)
- **Group Properties Model**: Contains attributes about groups (can be current or historical)
- **Event Properties Model**: Contains properties associated with specific events

### Data Structures
- **Fact Table**: Primary table containing event data with metrics and foreign keys to dimension tables
- **Dimension Tables**: Supporting tables with descriptive attributes related to fact table records
- **Star Schema**: Data modeling approach with a central fact table connected to dimension tables
- **Snowflake Schema**: Extended star schema where dimension tables are normalized into multiple related tables

### Technical Concepts
- **Clustering Keys**: Columns used to physically organize data in Snowflake for optimized query performance
- **Identity Spaces**: Frameworks for identifying and connecting users across different data sources
- **Partitioning**: Division of data into segments based on specific columns to improve query performance
- **LINEAR() Function**: Used in clustering key optimization to improve data retrieval efficiency

## Integration with Product Ecosystem

Warehouse-native Amplitude integrates with:

1. **Snowflake Data Warehouse**: The primary data source that houses all customer data
2. **DBT (Data Build Tool)**: Integration allows for defining Amplitude models within DBT workflows
3. **Amplitude Analytics Platform**: Provides the analytical capabilities and user interface for data exploration

WNA fits into the broader Amplitude ecosystem as an alternative data source option. While traditional Amplitude implementations ingest event data directly, WNA allows organizations to keep their data in Snowflake while still leveraging Amplitude's analytical capabilities.

## Technical Implementation Details

### Data Model Creation Methods
1. **Direct Table Selection**: Mapping existing Snowflake tables to Amplitude data models
2. **SQL Query**: Using custom SQL to define the data structure for Amplitude models
3. **Bulk Model Management**: Using YAML configuration files to create/update multiple models simultaneously
4. **DBT Integration**: Annotating DBT models with `amplitude_meta` to automatically create Amplitude models

### Required Fields and Mapping
For event models, required fields include:
- Event ID
- Event time
- User ID
- Event type

For user/group property models, required fields include:
- User/Group ID
- Property values
- (For historical models) Timestamp information

### Bulk Model Management Configuration
The YAML configuration structure for bulk model management includes:
```yaml
dataModels:
  - name: "model_name"
    type: "EVENT|EVENT_PROPERTIES|CURRENT_USER_PROPERTIES|etc."
    tableConfig:
      schema: "schema_name"
      table: "table_name"
    # OR
    sqlConfig:
      query: "SELECT * FROM..."
    requiredFields:
      # field mappings
    fieldMapping:
      # additional field mappings
```

### DBT Integration Annotation
DBT models can be annotated with:
```yaml
{{ config(
    amplitude_meta={
        "data_models": [{
            "type": "EVENT",
            "special_columns": {
                "event_time": "timestamp_column",
                "user_id": "user_id_column",
                "event_type": "event_type_column"
            }
        }]
    }
) }}
```

## Best Practices

1. **Clustering Key Optimization**:
   - Choose columns with high cardinality and frequent use in filters
   - Consider using composite clustering keys for complex queries
   - Utilize the LINEAR() function for range-based queries

2. **Schema Design**:
   - Use star schema for simpler queries and better performance
   - Consider snowflake schema for more complex data relationships
   - Balance normalization with query performance needs

3. **Partitioning Strategies**:
   - Partition data based on query patterns
   - Consider time-based partitioning for event data
   - Optimize partition sizes for balanced performance

4. **Performance Considerations**:
   - Limit the number of joins in models
   - Pre-aggregate data where possible
   - Use appropriate data types to minimize storage and improve query speed