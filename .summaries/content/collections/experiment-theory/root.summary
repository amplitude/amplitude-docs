# Amplitude Experiment Statistical Analysis Framework

Amplitude Experiment is a comprehensive A/B testing and Multi-Armed Bandit experimentation platform that provides rigorous statistical analysis capabilities for data-driven decision making. The platform centers around Welch's T-test methodology and emphasizes proper experimental design, statistical significance testing, and risk assessment throughout the experiment lifecycle.

## Product Architecture and Feature Relationships

The statistical analysis framework operates through three integrated phases that form a cohesive experimentation workflow:

**Experiment Configuration Layer**: The Settings tab serves as the primary entry point, housing the Goals panel where users define critical statistical parameters. This includes configuring recommendation metrics (primary success indicators) and guardrail metrics (safety measures), along with setting MDE values and sample size requirements that drive the entire experimental approach.

**Statistical Processing Engine**: At the core lies Welch's T-test implementation, which handles both binary conversion metrics and continuous value metrics without assuming equal variances between groups. The engine supports configurable hypothesis testing approaches through two-sided testing (bidirectional change detection) and one-sided testing (directional change detection with reduced sample requirements).

**Real-time Analysis and Monitoring**: The platform provides continuous experiment tracking through the Cumulative Exposure graph and integrated duration estimator, allowing teams to monitor progress toward statistical significance. The Analysis Settings Stats Preferences enable organizational customization of statistical approaches and significance thresholds.

## Core Nomenclature and Statistical Concepts

**MDE (Minimum Detectable Effect)**: The smallest meaningful change in a metric that warrants detection with statistical confidence. This parameter directly determines required sample sizes and experiment duration, serving as the foundation for experimental power calculations.

**Welch's T-test**: The primary statistical hypothesis testing method that accommodates unequal variances between experimental groups, making it robust for diverse metric types and sample distributions.

**Sample Size Threshold**: The calculated minimum number of users required per variant to achieve statistical significance, derived from MDE specifications, baseline conversion rates, and configured confidence levels.

**Control Mean**: The baseline performance metric from the control group that serves as the reference point for calculating relative improvements and determining statistical significance.

**Statistical Risk Framework**: 
- **False Positive Rate**: The probability of incorrectly detecting an effect when none exists
- **False Negative Rate**: The probability of missing a real effect that actually exists

**Testing Directionality**:
- **Two-sided Testing**: Detects changes in either positive or negative directions
- **One-sided Testing**: Focuses on detecting changes in a predetermined direction with reduced sample size requirements

## Ecosystem Integration and Methodology

The statistical framework integrates with broader experimentation practices through several methodological components:

**Central Limit Theorem Application**: The platform leverages fundamental statistical theory to ensure reliable results across varying metric types and sample sizes, incorporating built-in safeguards for minimum sample requirements to maintain statistical validity.

**Sequential vs. Fixed Testing Approaches**: While Welch's T-test represents the primary fixed-sample methodology, the platform acknowledges alternative sequential testing approaches for organizations requiring different statistical frameworks.

**Business Risk Integration**: The system balances statistical rigor with business practicality by enabling customization of significance thresholds and effect size requirements based on the potential impact and risk tolerance of experimental changes.

**Resource Planning Integration**: Tools like the T-test duration estimator and Samples Per Variant Needed calculations integrate experiment planning with resource allocation, enabling teams to forecast timelines and capacity requirements before experiment launch.

## Configuration Interfaces and Implementation

**Primary Configuration Pathway**: 
Settings tab → Goals panel → Analysis Settings Stats Preferences

**Core Configuration Parameters**:
- Statistical test methodology selection (two-sided/one-sided)
- Sample size threshold specifications per variant
- MDE value definitions across different metric types
- Organizational significance level preferences

**Monitoring and Analysis Interfaces**:
- **Cumulative Exposure Graph**: Real-time progress tracking toward statistical significance
- **Sample Size Target Indicators**: Visual progress indicators for sample collection
- **Duration Estimator**: Predictive timeline calculations for experiment completion

The platform enforces a design-first methodology, requiring teams to establish MDE parameters, statistical approaches, and risk tolerance during the configuration phase. This ensures that experimental results achieve both statistical validity and business relevance, with the statistical framework providing the mathematical foundation for reliable experimentation at scale.