Amplitude is a comprehensive digital analytics and experimentation platform that provides organizations with advanced tools for product analytics, A/B testing, feature management, and customer data orchestration. The platform combines behavioral analytics, statistical experimentation, and data governance capabilities to enable data-driven product development and optimization.

## Core Product Architecture and Relationships

The Amplitude ecosystem operates through four primary product pillars that work together to provide end-to-end analytics and experimentation capabilities:

**Amplitude Analytics** serves as the foundational behavioral analytics platform, offering sophisticated chart types including Event Segmentation, Funnel Analysis, Retention Analysis, User Sessions, Journeys, Stickiness, Lifecycle, Personas, Compass, Impact Analysis, Revenue LTV, Engagement Matrix, and Data Tables. These charts enable comprehensive user behavior analysis through cohort analysis, behavioral targeting, and advanced segmentation capabilities.

**Amplitude Experiment** provides A/B testing and feature flagging infrastructure with advanced statistical methods including mixture sequential probability ratio test (mSPRT) for early experiment termination without predetermined sample sizes. The platform supports both remote evaluation for server-side experiments and local evaluation for client-side implementations, utilizing deterministic assignment through murmur3_x86_32 hash algorithms.

**Amplitude Data** functions as the central data governance and orchestration layer, managing tracking plans, generating type-safe code through Ampli CLI tools, monitoring data quality, and enabling retroactive data transformations. The platform operates using Git-like branch workflows for schema management and provides comprehensive source/destination catalogs for data integration.

**Supporting Infrastructure** includes Session Replay for user experience analysis, Guides and Surveys for in-product engagement, CDP (Customer Data Platform) capabilities, and administrative tools for account management, billing, single sign-on, and organizational governance.

## Key Nomenclature and Definitions

**Monthly Tracked Users (MTU)**: The fundamental billing unit representing unique users tracked within calendar month boundaries, serving as the primary pricing metric with sophisticated user identity resolution and synthetic MTU generation for high-volume scenarios.

**Events and Properties**: Events represent discrete user actions, while Event Properties provide contextual attributes captured at action time. User Properties maintain persistent user state across sessions, and Group Properties enable B2B account-level analytics.

**Tracking Plans**: Comprehensive schema documentation defining complete event taxonomies and property specifications, managed through branch-based workflows with collaborative review processes.

**Assignment vs Exposure Events**: Assignment Events are triggered by variant assignment API calls, while Exposure Events indicate actual feature interaction, enabling sophisticated experiment analysis and user engagement measurement.

**Flag Dependencies**: Evaluation order relationships enabling complex experimental designs including prerequisites, mutual exclusion groups, and holdout configurations.

**Behavioral Cohorts**: Dynamic user segments computed based on historical behavior patterns, updated hourly and integrated across the entire platform for targeting and analysis.

## Technical Implementation and API Endpoints

The platform provides extensive SDK support across multiple programming languages and platforms:

**Analytics SDKs**: Browser, iOS, Android, React Native, Flutter, Unity, Unreal, Node.js, Python, Java, and Go implementations with comprehensive event tracking, user identification, and behavioral analysis capabilities.

**Experiment SDKs**: Support both remote and local evaluation modes with core methods including `fetch()`, `evaluate()`, and `variant()` for assignment and exposure tracking.

**Ampli CLI Tools**: Developer command-line interface for schema management and code generation with commands like `ampli pull` for downloading tracking plans and `ampli status --is-merged` for CI/CD integration.

**Core API Infrastructure**:
- HTTP V2 API for real-time event ingestion
- Batch Event Upload API supporting 500K events per device ID daily
- Taxonomy API for programmatic schema management
- User Privacy API for GDPR/CCPA compliance
- Analytics API and Experiment APIs for data access and configuration

## Broader Product Ecosystem Integration

Amplitude integrates extensively with external platforms through comprehensive source and destination catalogs. The source catalog enables data ingestion from platforms like Segment, mParticle, and Tealium, while the destination catalog supports real-time event streaming to marketing automation, attribution, and infrastructure tools with p95 latency targets of 60 seconds.

The platform supports advanced data infrastructure integration including cloud storage systems (Amazon S3, Google Cloud Storage), data warehouses (Snowflake, Databricks), and streaming platforms through bidirectional data synchronization. Data Mutability features support INSERT, UPDATE, DELETE operations through Mirror Sync strategies.

**Performance Infrastructure**: Built on globally distributed architecture leveraging Fastly CDN for edge caching and AWS services including Application Load Balancer, RDS, and DynamoDB across multiple geographic regions (us-west-2, eu-central-1) with sub-100ms latency for cached requests.

**Enterprise Features**: Include Data Access Control (DAC) for granular permissions, Single Sign-On integration, comprehensive billing and usage analytics, PII management capabilities, and warehouse-native deployment options for organizations requiring data residency control.

The platform serves diverse use cases from product analytics and growth optimization to enterprise-scale experimentation and data governance, providing organizations with comprehensive tools for understanding user behavior, testing product changes, and maintaining data quality across complex technical architectures.